<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jyothi Vedurada | Somak Aditya</title>
    <link>https://adityasomak.github.io/authors/jyothi-vedurada/</link>
      <atom:link href="https://adityasomak.github.io/authors/jyothi-vedurada/index.xml" rel="self" type="application/rss+xml" />
    <description>Jyothi Vedurada</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 05 Sep 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://adityasomak.github.io/img/icon-192.png</url>
      <title>Jyothi Vedurada</title>
      <link>https://adityasomak.github.io/authors/jyothi-vedurada/</link>
    </image>
    
    <item>
      <title>SYNC: A Structurally guided Hard Negative Curricula for Efficient Neural Code Search</title>
      <link>https://adityasomak.github.io/publication/sync/</link>
      <pubDate>Tue, 05 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/sync/</guid>
      <description>&lt;p&gt;In neural code search, a Transformers-based pre-trained language model (such as CodeBERT) is used to embed both the query (NL) and the code snippet (PL) into a joint representation space; which is used to retrieve the relevant PLs satisfying the query. These models often make mistakes such as retrieving snippets with incorrect data types, and incorrect method names or signatures. The generalization ability beyond training data is also limited (as the code retrieval datasets vary in the ways NL-PL pairs are collected). In this work, we propose a novel contrastive learning technique (SYNC) that enables efficient finetuning of code LMs with soft and hard negatives, where the hard negatives are constructed using a set of structure-aware AST-based perturbations; targeted towards possible syntactic and semantic variations.
Our method achieves significant improvements in retrieval performance for three code LMs (CodeBERT, GraphCodeBERT, UniXCoder) over four Python code retrieval datasets.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
