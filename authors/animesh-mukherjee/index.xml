<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Animesh Mukherjee | Somak Aditya</title>
    <link>https://adityasomak.github.io/authors/animesh-mukherjee/</link>
      <atom:link href="https://adityasomak.github.io/authors/animesh-mukherjee/index.xml" rel="self" type="application/rss+xml" />
    <description>Animesh Mukherjee</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 08 Oct 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://adityasomak.github.io/img/icon-192.png</url>
      <title>Animesh Mukherjee</title>
      <link>https://adityasomak.github.io/authors/animesh-mukherjee/</link>
    </image>
    
    <item>
      <title>AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models</title>
      <link>https://adityasomak.github.io/publication/aura/</link>
      <pubDate>Wed, 08 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/aura/</guid>
      <description>&lt;p&gt;Present day LLMs face the challenge of managing affordance-based safety risks—situations where outputs inadvertently facilitate harmful actions due to overlooked logical implications. Traditional safety solutions, such as scalar outcome-based reward models, parameter tuning, or heuristic decoding strategies, lack the granularity and proactive nature needed to reliably detect and intervene during subtle yet crucial reasoning steps. Addressing this fundamental gap, we introduce AURA, an innovative, multi-layered framework centered around Process Reward Models (PRMs), providing comprehensive, step level evaluations across logical coherence and safety-awareness. Our framework seamlessly combines introspective self-critique, fine-grained PRM assessments, and adaptive safety-aware decoding to dynamically and proactively guide models toward safer reasoning trajectories. Empirical evidence clearly demonstrates that this approach significantly surpasses existing methods, significantly improving the logical integrity and affordance-sensitive safety of model outputs. This research represents a pivotal step toward safer, more responsible, and contextually aware AI, setting a new benchmark for alignment-sensitive applications.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Text2Afford: Probing Object Affordance Prediction abilities of Language Models solely from Text</title>
      <link>https://adityasomak.github.io/publication/text2afford/</link>
      <pubDate>Fri, 20 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/text2afford/</guid>
      <description>&lt;p&gt;We investigate the knowledge of object affordances in pre-trained language models
(LMs) and pre-trained Vision-Language models (VLMs). A growing body of literature
shows that PTLMs fail inconsistently and nonintuitively, demonstrating a lack of reasoning
and grounding. To take a first step toward quantifying the effect of grounding (or lack thereof),
we curate a novel and comprehensive dataset of object affordances – TEXT2AFFORD, characterized by 15 affordance classes. Unlike affordance datasets collected in vision and language domains, we annotate in-the-wild sentences with objects and affordances. Experimental results reveal that PTLMs exhibit limited reasoning abilities when it comes to uncommon object affordances. We also observe that pretrained VLMs do not necessarily capture object affordances effectively. Through few-shot fine-tuning, we demonstrate improvement in affordance knowledge in PTLMs and VLMs.
Our research contributes a novel dataset for language grounding tasks, and presents insights
into LM capabilities, advancing the understanding of object affordances.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
