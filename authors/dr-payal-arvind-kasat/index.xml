<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dr Payal Arvind Kasat | Somak Aditya</title>
    <link>https://adityasomak.github.io/authors/dr-payal-arvind-kasat/</link>
      <atom:link href="https://adityasomak.github.io/authors/dr-payal-arvind-kasat/index.xml" rel="self" type="application/rss+xml" />
    <description>Dr Payal Arvind Kasat</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 20 Sep 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://adityasomak.github.io/img/icon-192.png</url>
      <title>Dr Payal Arvind Kasat</title>
      <link>https://adityasomak.github.io/authors/dr-payal-arvind-kasat/</link>
    </image>
    
    <item>
      <title>ERVQA: A Dataset to Benchmark the Readiness of Large Vision Language Models in Hospital Environments</title>
      <link>https://adityasomak.github.io/publication/ervqa/</link>
      <pubDate>Fri, 20 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/ervqa/</guid>
      <description>&lt;p&gt;The global shortage of healthcare workers has demanded the development of smart healthcare assistants, which can help monitor and alert healthcare workers when necessary. We examine the healthcare knowledge of existing Large Vision Language Models (LVLMs) via the Visual Question Answering (VQA) task in hospital settings through expert annotated open-ended questions. We introduce the Emergency Room Visual Question Answering (ERVQA) dataset, consisting of 790 &lt;image, question, answer&gt; triplets covering diverse emergency room scenarios, a seminal benchmark for LVLMs. By developing a detailed error taxonomy and analyzing answer trends, we reveal the nuanced nature of the task. We benchmark state-of-the-art open-source and closed LVLMs using traditional and adapted VQA metrics: Entailment Score and CLIPScore Confidence. Analyzing errors across models, we infer trends based on properties like decoder type, model size, and in-context examples. Our findings suggest the ERVQA dataset presents a highly complex task, highlighting the need for specialized, domain-specific solutions.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
