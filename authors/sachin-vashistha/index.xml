<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sachin Vashistha | Somak Aditya</title>
    <link>https://adityasomak.github.io/authors/sachin-vashistha/</link>
      <atom:link href="https://adityasomak.github.io/authors/sachin-vashistha/index.xml" rel="self" type="application/rss+xml" />
    <description>Sachin Vashistha</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 08 Nov 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://adityasomak.github.io/img/icon-192.png</url>
      <title>Sachin Vashistha</title>
      <link>https://adityasomak.github.io/authors/sachin-vashistha/</link>
    </image>
    
    <item>
      <title>PRAGWORLD: A Benchmark Evaluating LLMs&#39; Local World Model under Minimal Linguistic Alterations and Conversational Dynamics</title>
      <link>https://adityasomak.github.io/publication/pragworld/</link>
      <pubDate>Sat, 08 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/pragworld/</guid>
      <description>&lt;p&gt;Real-world conversations are rich with pragmatic elements, such as entity mentions, references, and implicatures. Understanding such nuances is a requirement for successful natural communication, and often requires building a local world model which encodes such elements and captures the dynamics of their evolving states. However, it is not well-understood whether language models (LMs) construct or maintain a robust implicit representation of conversations. In this work, we evaluate the ability of LMs to encode and update their internal world model in dyadic conversations and test their malleability under linguistic alterations. To facilitate this, we apply seven minimal linguistic alterations to conversations sourced from popular datasets and construct two benchmarks comprising yes-no questions. We evaluate a wide range of open and closed source LMs and observe that they struggle to maintain robust accuracy. Our analysis unveils that LMs struggle to memorize crucial details, such as tracking entities under linguistic alterations to conversations. We then propose a dual-perspective interpretability framework which identifies transformer layers that are useful or harmful and highlights linguistic alterations most influenced by harmful layers, typically due to encoding spurious signals or relying on shortcuts. Inspired by these insights, we propose two layer-regularization based fine-tuning strategies that suppress the effect of the harmful layers.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
