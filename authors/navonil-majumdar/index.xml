<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Navonil Majumdar | Somak Aditya</title>
    <link>https://adityasomak.github.io/authors/navonil-majumdar/</link>
      <atom:link href="https://adityasomak.github.io/authors/navonil-majumdar/index.xml" rel="self" type="application/rss+xml" />
    <description>Navonil Majumdar</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 03 May 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://adityasomak.github.io/img/icon-192.png</url>
      <title>Navonil Majumdar</title>
      <link>https://adityasomak.github.io/authors/navonil-majumdar/</link>
    </image>
    
    <item>
      <title>A Robust Information-Masking Approach for Domain Counterfactual Generation</title>
      <link>https://adityasomak.github.io/publication/counterfactualda/</link>
      <pubDate>Wed, 03 May 2023 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/counterfactualda/</guid>
      <description>&lt;p&gt;Domain shift is a big challenge in NLP, thus, many approaches resort to learning domain-invariant features to mitigate the inference phase domain shift. Such methods, however, fail to leverage the domain-specific nuances relevant to the task at hand. To avoid such drawbacks, domain counterfactual generation aims to transform a text from the source domain to a given target domain. However, due to the limited availability of data, such frequency-based methods often miss and lead to some valid and spurious domain-token associations. Hence, we employ a three-step domain obfuscation approach that involves frequency and attention norm-based masking, to mask domain-specific cues, and unmasking to regain the domain generic context. Our experiments empirically show that the counterfactual samples sourced from our masked text lead to improved domain transfer on 10 out of 12 domain sentiment classification settings, with an average of 2% accuracy improvement over the state-of-the-art for unsupervised domain adaptation (UDA). Further our model outperforms the state-of-the-art by achieving 1.4% average accuracy improvement in the adversarial domain adaptation (ADA) setting. Moreover, our model also shows its domain adaptation efficacy on a large multi-domain intent classification dataset where it attains state-of-the-art results.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
