<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Somak Aditya</title>
    <link>https://adityasomak.github.io/</link>
      <atom:link href="https://adityasomak.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Somak Aditya</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 14 Mar 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://adityasomak.github.io/img/icon-192.png</url>
      <title>Somak Aditya</title>
      <link>https://adityasomak.github.io/</link>
    </image>
    
    <item>
      <title>MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning</title>
      <link>https://adityasomak.github.io/publication/mathsensei/</link>
      <pubDate>Thu, 14 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/mathsensei/</guid>
      <description>&lt;p&gt;Tool-augmented Large Language Models (TALM) are known to enhance the skillset of
large language models (LLM), thereby, leading to their improved reasoning abilities across
many tasks. While, TALMs have been successfully employed in different question-answering
benchmarks, their efficacy on complex mathematical reasoning benchmarks, and, the potential complimentary benefits offered by tools for knowledge retrieval and mathematical equation solving, are open research questions. In this work, we present MATHSENSEI, a toolaugmented large language model for mathematical reasoning. Augmented with tools for knowledge retrieval (Bing Web Search), program execution (Python), and symbolic equation solving (Wolfram-Alpha), we study the
complimentary benefits of these tools through evaluations on mathematical reasoning datasets.
We perform exhaustive ablations on MATH, a popular dataset for evaluating mathematical reasoning on diverse mathematical disciplines. We also conduct experiments involving well-known tool planners to study the impact of tool sequencing on the model performance. MATHSENSEI achieves 13.5% better accuracy over gpt-3.5-turbo with chain-ofthought on the MATH dataset. We further observe that TALMs are not as effective for simpler math word problems (in GSM-8k), and
the benefit increases as the complexity and required knowledge increases (progressively
over AQuA, MMLU-Math, and higher level complex questions in MATH). The code and data are available at &lt;a href=&#34;https://github.com/Debrup61/MathSensei&#34;&gt;MathSensei-Github&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tricking LLMs into Disobedience: Understanding, Analyzing, and Preventing Jailbreaks</title>
      <link>https://adityasomak.github.io/publication/promptinj/</link>
      <pubDate>Tue, 20 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/promptinj/</guid>
      <description>&lt;p&gt;Recent explorations with commercial Large Language Models (LLMs) have shown that non-expert users can jailbreak LLMs by simply manipulating the prompts; resulting in degenerate output behavior, privacy and security breaches, offensive outputs, and violations of content regulator policies. Limited formal studies have been carried out to formalize and analyze these attacks and their mitigations. We bridge this gap by proposing a formalism and a taxonomy of known (and possible) jailbreaks. We perform a survey of existing jailbreak methods and their effectiveness on open-source and commercial LLMs (such as GPT 3.5, OPT, BLOOM, and FLAN-T5-xxl). We further propose a limited set of prompt guards and discuss their effectiveness against known attack types.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Code Prompting Elicits Conditional Reasoning Abilities in Text&#43; Code LLMs</title>
      <link>https://adityasomak.github.io/publication/codeprompting/</link>
      <pubDate>Sun, 18 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/codeprompting/</guid>
      <description>&lt;p&gt;Reasoning is a fundamental component for achieving language understanding. Among the multiple types of reasoning, conditional reasoning, the ability to draw different conclusions depending on some condition, has been understudied in large language models (LLMs). Recent prompting methods, such as chain of thought, have significantly improved LLMs on reasoning tasks. Nevertheless, there is still little understanding of what triggers reasoning abilities in LLMs. We hypothesize that code prompts can trigger conditional reasoning in LLMs trained on text and code. We propose a chain of prompts that transforms a natural language problem into code and prompts the LLM with the generated code. Our experiments find that code prompts exhibit a performance boost between 2.6 and 7.7 points on GPT 3.5 across multiple datasets requiring conditional reasoning. We then conduct experiments to discover how code prompts elicit conditional reasoning abilities and through which features. We observe that prompts need to contain natural language text accompanied by high-quality code that closely represents the semantics of the instance text. Furthermore, we show that code prompts are more efficient, requiring fewer demonstrations, and that they trigger superior state tracking of variables or key entities.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>STUCK IN THE QUICKSAND OF NUMERACY, FAR FROM AGI SUMMIT: EVALUATING LLMS&#39; MATHEMATICAL COMPETENCY THROUGH ONTOLOGY-GUIDED PERTURBATIONS</title>
      <link>https://adityasomak.github.io/publication/more24/</link>
      <pubDate>Wed, 17 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/more24/</guid>
      <description>&lt;p&gt;Recent advancements in Large Language Models (LLMs) have showcased striking results on existing logical reasoning benchmarks, with some models even surpassing human performance. However, the true depth of their competencies and robustness, in mathematical reasoning tasks, remains an open question. In response, we develop (i) an ontology of perturbations of maths questions, (ii) a semi-automatic method of perturbation, and (iii) a dataset of perturbed maths questions
to probe the limits of LLM capabilities in mathematical-reasoning tasks.
These controlled perturbations span across multiple fine dimensions of the structural and representational aspects of maths questions. Using GPT-4, we generated the &lt;span style=&#34;font-variant-caps: small-caps&#34;&gt;More&lt;/span&gt; dataset by perturbing randomly selected five seed questions from GSM8K. This process was guided by our ontology and involved a thorough automatic and manual filtering process, yielding a set of 216 maths problems.
We conducted comprehensive evaluation of both closed-source and open-source LLMs on &lt;span style=&#34;font-variant-caps: small-caps&#34;&gt;More&lt;/span&gt;. The results show a significant performance drop across all the models against the perturbed questions. This strongly suggests that current LLMs lack robust mathematical skills and a deep understanding of reasoning. This research not only identifies multiple gaps in the capabilities of current models, but also highlights multiple potential directions for future development. Our dataset will be made publicly available at &lt;a href=&#34;https://huggingface.co/datasets/declare-lab/GSM8k_MORE&#34;&gt;huggingface library&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CS60092 Information Retrieval (Spring 2024)</title>
      <link>https://adityasomak.github.io/courses/irspring24/</link>
      <pubDate>Wed, 20 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/courses/irspring24/</guid>
      <description>&lt;blockquote&gt;
&lt;table id=&#34;nobd&#34; cellspacing=&#34;0&#34; cellpadding=&#34;2&#34; border=&#34;0&#34;&gt;
&lt;tbody&gt;&lt;tr&gt;&lt;td id=&#34;nobd&#34; align=&#34;left&#34; valign=&#34;top&#34;&gt;&lt;b&gt;Instructor&lt;/b&gt;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; align=&#34;left&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; align=&#34;left&#34;&gt;&lt;a href=&#34;http://cse.iitkgp.ac.in/~saditya/&#34;&gt;Somak Aditya&lt;/a&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&lt;b&gt;Timing (Tentative)&lt;/b&gt;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt; MON (12:00-12:55) , TUE (10:00-11:55)
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&lt;b&gt;First Class (Tentative)&lt;/b&gt;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&lt;span style=&#34;color:red&#34;&gt; January 2nd, 2023 &lt;/span&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&lt;b&gt;Venue&lt;/b&gt;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt; NC242, Nalanda Classroom
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&lt;b&gt;Teaching Assistants&lt;/b&gt;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;
    Sachin Vashishtha (sachinvashistha6916@gmail.com) &lt;br/&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/blockquote&gt;

&lt;!--
&lt;b&gt;Office Hours&lt;/b&gt; &lt;p&gt;&lt;/p&gt;
Friday - 18:10 - 19:10 (CSE-308)
--&gt;

&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Announcements&lt;/h3&gt;
&lt;ul style=&#34;list-style-type: square;&#34;&gt;



&lt;li&gt;  &lt;span style=&#39;color: red&#39;&gt;Note: for students, who are not able to register, kindly wait. I will not be able to answer your emails individually. &lt;/span&gt; From time to time, I will approve based on CGPA and other criteria (such as total class strength cap etc.). &lt;/li&gt;

&lt;!-- &lt;li&gt; Every &lt;b&gt;registered&lt;/b&gt; student should join the Google mailing list &lt;b&gt;ir-2023-spring@googlegroups.com&lt;/b&gt;. All urgent announcements would be made through the group. &lt;span style=&#34;color:red&#34;&gt;This group is meant only for registered (and approved) students. Kindly mention your roll number and the fact that you have registered in ERP.&lt;/span&gt;&lt;/li&gt; --&gt;

&lt;li&gt; IR 2023-24 Spring will be a fully &lt;u&gt;research-oriented course&lt;/u&gt;. &lt;/li&gt;

&lt;li&gt; First introductory class will be on &lt;b&gt;January 2 (Tuesday), at 10:00 am&lt;/b&gt;.&lt;/li&gt; 

&lt;li&gt; The course requires an understanding of the foundation of algorithms and data structures, probability and statistics, and knowledge of the basics of Natural Language Processing, and Machine Learning. This will be a research-oriented course that would require students to understand several CS research papers. There will be a term project that needs to be done using Python/Java. It is advisable to take this course only if you have the necessary background.
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;
Weightage: Class-Performance/Quiz (8-10%), Mid-Sem and End-Sem (60%), Term Project (30-32%).
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;&lt;h3&gt; Topics Covered &lt;/h3&gt;
&lt;table class=&#34;table&#34;&gt;
  &lt;tr class=&#39;weekbegin active&#39;&gt;
    &lt;th width=&#34;9%&#34;&gt;Week&lt;/th&gt;
    &lt;th width=&#34;9%&#34;&gt;Date&lt;/th&gt;
    &lt;!-- &lt;th width=&#34;12%&#34;&gt;Event&lt;/th&gt; --&gt;
    &lt;th width=&#34;41%&#34;&gt;Description &amp;amp; materials&lt;/th&gt;
    &lt;th width=&#34;41%&#34;&gt;Readings &amp;amp; other resources&lt;/th&gt;
  &lt;/tr&gt;&lt;/p&gt;

&lt;p&gt;&lt;tr class=&#39;weekbegin&#39;&gt;
    &lt;td&gt;Week 1&lt;/td&gt;
    &lt;td&gt;Mon. 09/01 &lt;br/&gt; &lt;/td&gt;
    &lt;!-- &lt;td&gt;Lecture (Pandu)&lt;/td&gt; --&gt;
    &lt;td&gt;&lt;b&gt;Introduction to the course&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
      &lt;ul&gt;
    &lt;!-- &lt;li&gt; Videos: &lt;a href=&#34;complementary_video_slides/05-01-07-IR-SemistructuredData.pptx&#34;&gt;&#34;Semistructured Data&#34;&lt;/a&gt; &lt;/li&gt; --&gt;
    &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp24/Lec1.pdf&#34;&gt;PDF&lt;/a&gt;
    &lt;/li&gt;&lt;br /&gt;
    &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;http://nlp.stanford.edu/IR-book/pdf/01bool.pdf&#34;&gt;IIR chapter 1&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;MG section 3.2&lt;/li&gt;
        &lt;li&gt;MIR section 8.2&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;http://www.rhymezone.com/shakespeare/&#34;&gt;Shakespeare plays&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;&lt;/p&gt;

&lt;p&gt;&lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;Tue &lt;sup&gt;10&lt;/sup&gt;&amp;frasl;&lt;sub&gt;01&lt;/sub&gt; &lt;/td&gt;
    &lt;!-- &lt;td&gt;Lecture (Chris)&lt;/td&gt; --&gt;
    &lt;td&gt;&lt;b&gt;Boolean Retrieval&lt;/b&gt;: Dictionary and postings lists, boolean querying
    &lt;!--(&lt;a href=&#34;https://github.com/manning/MergeAlgorithms&#34;&gt;starter code&lt;/a&gt;)--&gt;
    &lt;br/&gt;&lt;br/&gt;
    &lt;ul&gt;
    &lt;!-- &lt;li&gt; Videos: &lt;a href=&#34;complementary_video_slides/05-01-07-IR-SemistructuredData.pptx&#34;&gt;&#34;Semistructured Data&#34;&lt;/a&gt; &lt;/li&gt; --&gt;
    &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp23/Lec2.pdf&#34;&gt;PDF&lt;/a&gt;
    &lt;/li&gt;
    &lt;/td&gt;
    &lt;td&gt;
    &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;http://nlp.stanford.edu/IR-book/pdf/02voc.pdf&#34;&gt;IIR chapter 2&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;tr class=&#39;weekbegin&#39;&gt;
    &lt;td&gt;Week 2&lt;/td&gt;
    &lt;td&gt;Mon 15/ 01 &lt;br/&gt; Tue &lt;sup&gt;16&lt;/sup&gt;&amp;frasl;&lt;sub&gt;01&lt;/sub&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;The term vocabulary &amp;amp; postings lists&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
         &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec3.pdf&#34;&gt;PDF&lt;/a&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/IR-book/pdf/02voc.pdf&#34;&gt;IIR chapter 2&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;http://www.sims.berkeley.edu/~hearst/irbook/porter.html&#34;&gt;Porter&amp;rsquo;s stemmer (MIR)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;http://www.tartarus.org/~martin/PorterStemmer/&#34;&gt;Porter stemming algorithm (Official)&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
   &lt;tr class=&#34;weekbegin&#34;&gt;
    &lt;td&gt;Week 3&lt;/td&gt;
    &lt;td&gt;Mon. 22 / 01&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Skip Pointers, Phrase Queries and Positional Indexing&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
         &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp24/Lec4.pdf&#34;&gt;PDF&lt;/a&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;&lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/IR-book/pdf/02voc.pdf&#34;&gt;IIR chapter 2&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.524&#34;&gt;A skip list cookbook (Pugh 1990)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;http://portal.acm.org/citation.cfm?id=1028102&#34;&gt;Fast phrase querying with combined indexes (Williams, Zobel, Bahle 2004)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;http://portal.acm.org/citation.cfm?id=564415&#34;&gt;Efficient phrase querying with an auxiliary index (Bahle, Williams, Zobel 2002)&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
   &lt;tr class=&#39;warning&#39;&gt;
    &lt;td&gt;Week 4&lt;/td&gt;
    &lt;td&gt;Tues 23 / 01 &amp;amp; Mon 29 / 01&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Scoring, term weighting &amp;amp; the vector space model&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
         &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp24/Lec5.pdf&#34;&gt;PDF&lt;/a&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/IR-book/pdf/06vect.pdf&#34;&gt;IIR chapter 6&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;https://www.cs.utah.edu/~jeffp/teaching/cs5140-S15/cs5140/L4-Jaccard+nGram.pdf&#34;&gt;Jaccard Similarity and n-Grams document&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Cosine_similarity#Soft_cosine_measure&#34;&gt;Soft Cosine Measure&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;https://redirect.cs.umbc.edu/~nicholas/676/papers/p21-singhal.pdf&#34;&gt;Pivoted Document Length Norm&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr class=&#39;weekbegin&#39;&gt;
    &lt;td&gt;Week 5&lt;/td&gt;
    &lt;td&gt;Tue. 30/ 01 &amp;amp; Mon. 05 / 02&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Dictionaries and Tolerant Retieval&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
         &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec6.pdf&#34;&gt;PDF&lt;/a&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/IR-book/pdf/03dict.pdf&#34;&gt;IIR chapter 3&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
    &lt;tr class=&#34;warning&#34;&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;Tues. 06 / 02&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Evaluation&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
         &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp24/Lec7.pdf&#34;&gt;PDF&lt;/a&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
    &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/IR-book/pdf/08eval.pdf&#34;&gt;IIR chapter 8&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/what-is-average-precision-in-object-detection-localization-algorithms-and-how-to-calculate-it-3f330efe697b&#34;&gt;Average Precision&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr class=&#39;weekbegin&#39;&gt;
    &lt;td&gt;Week 6&lt;/td&gt;
    &lt;td&gt;Mon. &lt;sup&gt;12&lt;/sup&gt;&amp;frasl;&lt;sub&gt;02&lt;/sub&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Index Constructions&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
         &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec8.pdf&#34;&gt;PDF&lt;/a&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/IR-book/pdf/04const.pdf&#34;&gt;IIR chapter 4&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr class=&#39;warning&#39;&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;Mon. &lt;sup&gt;13&lt;/sup&gt;&amp;frasl;&lt;sub&gt;02&lt;/sub&gt; &lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Tutorial 1&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;To be Updated&lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/IR-book/exercises.html&#34;&gt;IIR Exercises (From IIR book and Other Problem Sets)&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr class=&#39;weekbegin&#39;&gt;
    &lt;td&gt;Week 7&lt;/td&gt;
    &lt;td&gt;Tue. 27/ 02&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Index Compression&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
         &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec9.pdf&#34;&gt;PDF&lt;/a&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/IR-book/pdf/05comp.pdf&#34;&gt;IIR chapter 5&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
   &lt;tr class=&#34;weekbegin&#34;&gt;
    &lt;td&gt;Week 8&lt;/td&gt;
    &lt;td&gt;Mon. 4 / 03 &amp;amp; Tues. 5 / 03&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Relevance Feedback &amp;amp; Query Expansion&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
         &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp24/Lec10.pdf&#34;&gt;PDF&lt;/a&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
    &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/IR-book/pdf/09expand.pdf&#34;&gt;IIR chapter 9&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;https://sigir.org/files/museum/pub-08/XXIII-1.pdf&#34;&gt;Rocchios&amp;rsquo;s Feedback Paper (SIGIR 1965)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;https://www.cs.cornell.edu/people/tj/publications/joachims_97a.pdf&#34;&gt;Probabilistic Analysis of the Rocchio (Joachim &amp;lsquo;97)&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr class=&#34;warning&#34;&gt;
    &lt;td&gt;Week 9&lt;/td&gt;
    &lt;td&gt;Mon. 11 / 03&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Probabilistic IR&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
         &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec11.pdf&#34;&gt;PDF&lt;/a&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
    &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/IR-book/pdf/11prob.pdf&#34;&gt;IIR chapter 11&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;https://seeing-theory.brown.edu/compound-probability/index.html&#34;&gt;Basics of Probability Theory&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;&lt;/p&gt;

&lt;p&gt;&lt;h3&gt;Tentative Topics &lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt; Boolean retrieval &lt;/li&gt;
&lt;li&gt; The term vocabulary &amp;amp; postings lists &lt;/li&gt;
&lt;li&gt; Skip Pointers, Phrase Queries and Positional Indexing&lt;/li&gt;
&lt;li&gt; Scoring, term weighting &amp;amp; the vector space model&lt;/li&gt;
&lt;li&gt; Dictionaries and Tolerant Retrieval&lt;/li&gt;
&lt;li&gt; Evaluation in information retrieval&lt;/li&gt;
&lt;li&gt; Index Construction and Compression&lt;/li&gt;
&lt;li&gt; Relevance feedback &amp;amp; query expansion&lt;/li&gt;
&lt;li&gt; Probabilistic information retrieval&lt;/li&gt;
&lt;li&gt; &lt;b&gt;Language models for information retrieval (Probabilistic, RNN, Transformers)&lt;/b&gt;&lt;/li&gt;
&lt;li&gt; &lt;b&gt;Retrieval-Augmented Generation (RAG) and Large Language Models&lt;/b&gt;&lt;/li&gt;
&lt;li&gt; Link analysis &amp;ndash; HITS, PageRank&lt;/li&gt;
&lt;li&gt; Word Vectors&lt;/li&gt;
&lt;li&gt; Learning to Rank&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;

&lt;p&gt;&lt;h3&gt; Pre-requisites for the course &lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt; Data structures and Algorithms &lt;/li&gt;
&lt;li&gt; Probability and Statistics &lt;/li&gt;
&lt;li&gt; Basics of Machine Learning &lt;/li&gt;
&lt;li&gt; Basics of Natural Language Processing (Some might be covered during the course) &lt;/li&gt;
&lt;li&gt; Basics of Graph algorithms (Some might be covered during the course) &lt;/li&gt;
&lt;li&gt; Programming in Python/Java &lt;/b&gt; &lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;

&lt;p&gt;&lt;h3&gt; Reference Literature, Useful Tools and Software Resources &lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt; &lt;a href=&#34;https://nlp.stanford.edu/IR-book/information-retrieval-book.html&#34;&gt;Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schütze. &lt;em&gt;Introduction to information retrieval&lt;/em&gt;, Cambridge: Cambridge university press, 2008.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt; Research Papers shared in class. &lt;/li&gt;
  &lt;li&gt; Pranav Rajpurkar&amp;rsquo;s &lt;a href=&#34;https://docs.google.com/document/d/1uvAbEhbgS_M-uDMTzmOWRlYxqCkogKRXdbKYYT98ooc/edit#&#34;&gt;Harvard CS197 AI Research Experiences&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&#34;color:red&#34;&gt; Every test should be attempted individually by each student. Plagiarism in any form will be severely penalized.&lt;/span&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models</title>
      <link>https://adityasomak.github.io/publication/logiglue/</link>
      <pubDate>Tue, 07 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/logiglue/</guid>
      <description>&lt;p&gt;Logical reasoning is fundamental for humans yet presents a substantial challenge in the domain of Artificial Intelligence. Initially, researchers used Knowledge Representation and Reasoning (KR) systems that did not scale and required non-trivial manual effort. Recently, the emergence of large language models (LLMs) has demonstrated the ability to overcome various limitations of formal Knowledge Representation (KR) systems. Consequently, there’s a growing interest in using LLMs for logical reasoning via natural language.&lt;/p&gt;

&lt;p&gt;This work strives to understand the proficiency of LLMs in logical reasoning by offering a brief review of the latest progress in this area; with a focus on the logical reasoning datasets, tasks, and the methods adopted to utilize LLMs for reasoning. To offer a thorough analysis, we’ve compiled a benchmark titled LogiGLUE. This includes 24 varied datasets encompassing deductive, abductive, and inductive reasoning. We have standardized these datasets into Seq2Seq tasks to facilitate straightforward training and evaluation for future research. Utilizing LogiGLUE as a foundation, we have trained an instruction fine-tuned language model, resulting in LogiT5. We study single-task training, multi-task training, and a &amp;ldquo;chain-of-thought&amp;rdquo; knowledge distillation finetuning technique to assess the model’s performance across the different logical reasoning categories. By this comprehensive process, we aim to shed light on the capabilities and potential pathways for enhancing logical reasoning proficiency in LLMs, paving the way for more advanced and nuanced developments in this
critical field.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Prover: Generating Intermediate Steps for NLI with Commonsense Knowledge Retrieval and Next-Step Prediction</title>
      <link>https://adityasomak.github.io/publication/multihop/</link>
      <pubDate>Tue, 05 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/multihop/</guid>
      <description>&lt;p&gt;The Natural Language Inference (NLI) task often requires reasoning over multiple steps to reach the conclusion. While the necessity of generating such intermediate steps (instead of a summary explanation) has gained popular support, it is unclear how to generate such steps without complete end-to-end supervision and how such generated steps can be further utilized. In this work, we train and enhance a sequence-to-sequence next-step prediction model with external commonsense knowledge and search to generate intermediate steps with limited next-step supervision. We show the correctness of such generated steps through human verification, on MNLI and MED datasets (and discuss the limitations through qualitative examples). We show that such generated steps can help improve end-to-end NLI task performance using simple data augmentation strategies. Using a CheckList dataset for NLI, we also explore the effect of augmentation on specific reasoning types.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SYNC: A Structurally guided Hard Negative Curricula for Efficient Neural Code Search</title>
      <link>https://adityasomak.github.io/publication/sync/</link>
      <pubDate>Tue, 05 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/sync/</guid>
      <description>&lt;p&gt;In neural code search, a Transformers-based pre-trained language model (such as CodeBERT) is used to embed both the query (NL) and the code snippet (PL) into a joint representation space; which is used to retrieve the relevant PLs satisfying the query. These models often make mistakes such as retrieving snippets with incorrect data types, and incorrect method names or signatures. The generalization ability beyond training data is also limited (as the code retrieval datasets vary in the ways NL-PL pairs are collected). In this work, we propose a novel contrastive learning technique (SYNC) that enables efficient finetuning of code LMs with soft and hard negatives, where the hard negatives are constructed using a set of structure-aware AST-based perturbations; targeted towards possible syntactic and semantic variations.
Our method achieves significant improvements in retrieval performance for three code LMs (CodeBERT, GraphCodeBERT, UniXCoder) over four Python code retrieval datasets.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NeuroSymbolic Reasoning</title>
      <link>https://adityasomak.github.io/project/neurosymbolic/</link>
      <pubDate>Thu, 31 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/project/neurosymbolic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>LoNLI: An Extensible Framework for Testing Diverse Logical Reasoning Capabilities for NLI</title>
      <link>https://adityasomak.github.io/publication/lonli/</link>
      <pubDate>Mon, 07 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/lonli/</guid>
      <description>&lt;p&gt;Natural Language Inference (NLI) is considered a representative task to test natural language understanding (NLU).  In this work, we propose an extensible framework to collectively yet categorically test diverse Logical reasoning capabilities required for NLI (and by extension, NLU). Motivated by behavioral testing, we create a semi-synthetic large test-bench (363 templates, 363k examples) and an associated framework that offers following utilities: 1) individually test and analyze reasoning capabilities along 17 reasoning dimensions (including pragmatic reasoning), 2) design experiments to study cross-capability information content (leave one out or bring one in); and 3) the synthetic nature enable us to control for artifacts and biases. We extend a publicly available framework of automated test case instantiation from free-form natural language templates (CheckList), and a well-defined taxonomy of capabilities to cover a wide range of increasingly harder test cases while varying the complexity of natural language. Through our analysis of state-of-the-art NLI systems, we observe that our benchmark is indeed hard (and non-trivial even with training on additional resources). Some capabilities stand out as harder. Further fine-grained analysis and fine-tuning experiments reveal more insights about these capabilities and the models &amp;ndash; supporting and extending previous observations; thus showing the utility of the proposed testbench.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CS60050 Machine Learning (Autumn 2023)</title>
      <link>https://adityasomak.github.io/courses/mlaut23/</link>
      <pubDate>Thu, 27 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/courses/mlaut23/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Robust Information-Masking Approach for Domain Counterfactual Generation</title>
      <link>https://adityasomak.github.io/publication/counterfactualda/</link>
      <pubDate>Wed, 03 May 2023 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/counterfactualda/</guid>
      <description>&lt;p&gt;Domain shift is a big challenge in NLP, thus, many approaches resort to learning domain-invariant features to mitigate the inference phase domain shift. Such methods, however, fail to leverage the domain-specific nuances relevant to the task at hand. To avoid such drawbacks, domain counterfactual generation aims to transform a text from the source domain to a given target domain. However, due to the limited availability of data, such frequency-based methods often miss and lead to some valid and spurious domain-token associations. Hence, we employ a three-step domain obfuscation approach that involves frequency and attention norm-based masking, to mask domain-specific cues, and unmasking to regain the domain generic context. Our experiments empirically show that the counterfactual samples sourced from our masked text lead to improved domain transfer on 10 out of 12 domain sentiment classification settings, with an average of 2% accuracy improvement over the state-of-the-art for unsupervised domain adaptation (UDA). Further our model outperforms the state-of-the-art by achieving 1.4% average accuracy improvement in the adversarial domain adaptation (ADA) setting. Moreover, our model also shows its domain adaptation efficacy on a large multi-domain intent classification dataset where it attains state-of-the-art results.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CS60092 Information Retrieval (Spring 2023)</title>
      <link>https://adityasomak.github.io/courses/irspring23/</link>
      <pubDate>Tue, 27 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/courses/irspring23/</guid>
      <description>&lt;blockquote&gt;
&lt;table id=&#34;nobd&#34; cellspacing=&#34;0&#34; cellpadding=&#34;2&#34; border=&#34;0&#34;&gt;
&lt;tbody&gt;&lt;tr&gt;&lt;td id=&#34;nobd&#34; align=&#34;left&#34; valign=&#34;top&#34;&gt;&lt;b&gt;Instructor&lt;/b&gt;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; align=&#34;left&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; align=&#34;left&#34;&gt;&lt;a href=&#34;http://cse.iitkgp.ac.in/~saditya/&#34;&gt;Somak Aditya&lt;/a&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&lt;b&gt;Timing (Tentative)&lt;/b&gt;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt; MON (12:00-12:55) , TUE (10:00-11:55)
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&lt;b&gt;First Class&lt;/b&gt;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&lt;span style=&#34;color:red&#34;&gt; January 3rd, 2023 &lt;/span&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&lt;b&gt;Venue&lt;/b&gt;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt; NC443, Nalanda Classroom
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&lt;b&gt;Teaching Assistants&lt;/b&gt;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;
    Sachin Vashishtha (sachinvashistha6916@gmail.com) &lt;br/&gt;
    Bishal Santra (bsantraigi@gmail.com)&lt;br/&gt;
    Karde Vivek Manikrao (vivek10.karde@gmail.com) &lt;br/&gt;
    Deepak Chaudhary (DEEPAKCHAUDHARY4311@kgpian.iitkgp.ac.in) &lt;br/&gt;
    &lt;!-- Abhilash Nandy &lt;br/&gt;
    Ankan Mullick &lt;br/&gt;
    Neeraj Saini &lt;br/&gt;
    Ravi Pratap Singh &lt;br/&gt;
    Vaibhav Saxena &lt;br/&gt; --&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/blockquote&gt;

&lt;!--
&lt;b&gt;Office Hours&lt;/b&gt; &lt;p&gt;&lt;/p&gt;
Friday - 18:10 - 19:10 (CSE-308)
--&gt;

&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Announcements&lt;/h3&gt;
&lt;ul style=&#34;list-style-type: square;&#34;&gt;



&lt;!--&lt;li&gt; &lt;b&gt;[Mar 17]&lt;/b&gt; Class Test 2 will be on Mar 3rd week. Syllabus will include whatever is covered after Vector Space and Scoring. &lt;/li&gt; 

&lt;li&gt; &lt;b&gt;[Jan 27]&lt;/b&gt; Class Test 1 will be on Feb 3rd week. Syllabus will include whatever is covered upto Feb 1st week. Please register in the CSE Moodle. Enrollment key is shared on Teams &lt;/li&gt; 

&lt;li&gt; &lt;b&gt;[Jan 27]&lt;/b&gt; Project Choice submission deadline: &lt;b&gt; Jan 28th 11:59 PM IST &lt;/b&gt;. &lt;/li&gt; --&gt;

&lt;li&gt; Recording of GUEST LECTURE BY &lt;a href=&#34;https://anikem.github.io/&#34;&gt;Dr. Anirudh Kembhavi&lt;/a&gt;, Director of Computer Vision at &lt;a href=&#34;https://allenai.org/&#34;&gt;Allen Institute for AI (AI2)&lt;/a&gt; is available &lt;a href=&#34;http://shorturl.at/gizQZ&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt; INVITED GUEST LECTURE BY &lt;a href=&#34;https://anikem.github.io/&#34;&gt;Dr. Anirudh Kembhavi&lt;/a&gt;, Director of Computer Vision at &lt;a href=&#34;https://allenai.org/&#34;&gt;Allen Institute for AI (AI2)&lt;/a&gt; on MARCH 13, MONDAY confirmed!!&lt;/li&gt;
&lt;li&gt;  &lt;span style=&#39;color: red&#39;&gt;Note: for students, who are not able to register, kindly wait. I will not be able to answer your emails individually. &lt;/span&gt; From time to time, I will approve based on CGPA and other criteria (such as total class strength cap etc.). &lt;/li&gt;

&lt;li&gt; Every &lt;b&gt;registered&lt;/b&gt; student should join the Google mailing list &lt;b&gt;ir-2023-spring@googlegroups.com&lt;/b&gt;. All urgent announcements would be made through the group. &lt;span style=&#34;color:red&#34;&gt;This group is meant only for registered (and approved) students. Kindly mention your roll number and the fact that you have registered in ERP.&lt;/span&gt;&lt;/li&gt;

&lt;li&gt; IR 2022-23 Spring will be a fully &lt;u&gt;research-oriented course&lt;/u&gt;. &lt;/li&gt;

 &lt;li&gt; First introductory class will be on &lt;b&gt;January 3 (Tuesday), at 10:00 am&lt;/b&gt;.&lt;/li&gt; 

&lt;li&gt; The course requires an understanding of the foundation of algorithms and data structures, probability and statistics, and knowledge of the basics of Natural Language Processing, and Machine Learning. This will be a research-oriented course that would require students to understand several CS research papers. There will be a term project that needs to be done using Python/Java. It is advisable to take this course only if you have the necessary background.
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;
Weightage: Class-Performance/Viva (8%), Mid-Sem and End-Sem (60%), Term Project (32%).
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;&lt;h3&gt; Topics Covered &lt;/h3&gt;
&lt;table class=&#34;table&#34;&gt;
  &lt;tr class=&#39;weekbegin active&#39;&gt;
    &lt;th width=&#34;9%&#34;&gt;Week&lt;/th&gt;
    &lt;th width=&#34;9%&#34;&gt;Date&lt;/th&gt;
    &lt;!-- &lt;th width=&#34;12%&#34;&gt;Event&lt;/th&gt; --&gt;
    &lt;th width=&#34;41%&#34;&gt;Description &amp;amp; materials&lt;/th&gt;
    &lt;th width=&#34;41%&#34;&gt;Readings &amp;amp; other resources&lt;/th&gt;
  &lt;/tr&gt;&lt;/p&gt;

&lt;p&gt;&lt;tr class=&#39;weekbegin&#39;&gt;
    &lt;td&gt;Week 1&lt;/td&gt;
    &lt;td&gt;Mon. 09/01&lt;/td&gt;
    &lt;!-- &lt;td&gt;Lecture (Pandu)&lt;/td&gt; --&gt;
    &lt;td&gt;&lt;b&gt;Introduction to the course&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
      &lt;ul&gt;
    &lt;!-- &lt;li&gt; Videos: &lt;a href=&#34;complementary_video_slides/05-01-07-IR-SemistructuredData.pptx&#34;&gt;&#34;Semistructured Data&#34;&lt;/a&gt; &lt;/li&gt; --&gt;
    &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp23/Lec1.pdf&#34;&gt;PDF&lt;/a&gt;
    &lt;/li&gt;&lt;br /&gt;
    &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;http://nlp.stanford.edu/IR-book/pdf/01bool.pdf&#34;&gt;IIR chapter 1&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;MG section 3.2&lt;/li&gt;
        &lt;li&gt;MIR section 8.2&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;http://www.rhymezone.com/shakespeare/&#34;&gt;Shakespeare plays&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;&lt;/p&gt;

&lt;p&gt;&lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;Tues. 10 / 01&lt;/td&gt;
    &lt;!-- &lt;td&gt;Lecture (Chris)&lt;/td&gt; --&gt;
    &lt;td&gt;&lt;b&gt;Boolean Retrieval&lt;/b&gt;: Dictionary and postings lists, boolean querying
    &lt;!--(&lt;a href=&#34;https://github.com/manning/MergeAlgorithms&#34;&gt;starter code&lt;/a&gt;)--&gt;
    &lt;br/&gt;&lt;br/&gt;
    &lt;ul&gt;
    &lt;!-- &lt;li&gt; Videos: &lt;a href=&#34;complementary_video_slides/05-01-07-IR-SemistructuredData.pptx&#34;&gt;&#34;Semistructured Data&#34;&lt;/a&gt; &lt;/li&gt; --&gt;
    &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp23/Lec2.pdf&#34;&gt;PDF&lt;/a&gt;
    &lt;/li&gt;
    &lt;/td&gt;
    &lt;td&gt;
    &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;http://nlp.stanford.edu/IR-book/pdf/02voc.pdf&#34;&gt;IIR chapter 2&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr class=&#39;weekbegin&#39;&gt;
    &lt;td&gt;Week 2&lt;/td&gt;
    &lt;td&gt;Mon. 16/ 01&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;The term vocabulary &amp;amp; postings lists&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
         &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec3.pdf&#34;&gt;PDF&lt;/a&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/IR-book/pdf/02voc.pdf&#34;&gt;IIR chapter 2&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;http://www.sims.berkeley.edu/~hearst/irbook/porter.html&#34;&gt;Porter&amp;rsquo;s stemmer (MIR)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;http://www.tartarus.org/~martin/PorterStemmer/&#34;&gt;Porter stemming algorithm (Official)&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
   &lt;tr class=&#34;warning&#34;&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;Tues. 17 / 01&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Skip Pointers, Phrase Queries and Positional Indexing&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
         &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec4.pdf&#34;&gt;PDF&lt;/a&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;&lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/IR-book/pdf/02voc.pdf&#34;&gt;IIR chapter 2&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.524&#34;&gt;A skip list cookbook (Pugh 1990)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;http://portal.acm.org/citation.cfm?id=1028102&#34;&gt;Fast phrase querying with combined indexes (Williams, Zobel, Bahle 2004)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;http://portal.acm.org/citation.cfm?id=564415&#34;&gt;Efficient phrase querying with an auxiliary index (Bahle, Williams, Zobel 2002)&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr class=&#39;weekbegin&#39;&gt;
    &lt;td&gt;Week 3&lt;/td&gt;
    &lt;td&gt;Mon. 23/ 01 &amp;amp; Tues 24/ 01&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Scoring, term weighting &amp;amp; the vector space model&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
         &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec5.pdf&#34;&gt;PDF&lt;/a&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/IR-book/pdf/06vect.pdf&#34;&gt;IIR chapter 6&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;https://www.cs.utah.edu/~jeffp/teaching/cs5140-S15/cs5140/L4-Jaccard+nGram.pdf&#34;&gt;Jaccard Similarity and n-Grams document&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Cosine_similarity#Soft_cosine_measure&#34;&gt;Soft Cosine Measure&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;https://redirect.cs.umbc.edu/~nicholas/676/papers/p21-singhal.pdf&#34;&gt;Pivoted Document Length Norm&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr class=&#39;weekbegin&#39;&gt;
    &lt;td&gt;Week 4&lt;/td&gt;
    &lt;td&gt;Mon. 30/ 01&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Dictionaries and Tolerant Retieval&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
         &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec6.pdf&#34;&gt;PDF&lt;/a&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/IR-book/pdf/03dict.pdf&#34;&gt;IIR chapter 3&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr class=&#34;warning&#34;&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;Tues. 31 / 01&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Evaluation&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
         &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec7.pdf&#34;&gt;PDF&lt;/a&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
    &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/IR-book/pdf/08eval.pdf&#34;&gt;IIR chapter 8&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr class=&#39;weekbegin&#39;&gt;
    &lt;td&gt;Week 5&lt;/td&gt;
    &lt;td&gt;Mon. &lt;sup&gt;6&lt;/sup&gt;&amp;frasl;&lt;sub&gt;02&lt;/sub&gt; &amp;amp; &lt;sup&gt;7&lt;/sup&gt;&amp;frasl;&lt;sub&gt;02&lt;/sub&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Index Constructions&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
         &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec8.pdf&#34;&gt;PDF&lt;/a&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/IR-book/pdf/04const.pdf&#34;&gt;IIR chapter 4&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr class=&#39;weekbegin&#39;&gt;
    &lt;td&gt;Week 6&lt;/td&gt;
    &lt;td&gt;Mon. &lt;sup&gt;13&lt;/sup&gt;&amp;frasl;&lt;sub&gt;02&lt;/sub&gt; &lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Tutorial 1&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;To be Updated&lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/IR-book/exercises.html&#34;&gt;IIR Exercises (From IIR book and Other Problem Sets)&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr class=&#39;weekbegin&#39;&gt;
    &lt;td&gt;Week 7&lt;/td&gt;
    &lt;td&gt;Mon. 27/ 02&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Index Compression&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
         &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec9.pdf&#34;&gt;PDF&lt;/a&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/IR-book/pdf/05comp.pdf&#34;&gt;IIR chapter 5&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr class=&#34;warning&#34;&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;Tues. 28 / 02&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Relevance Feedback &amp;amp; Query Expansion&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
         &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec10.pdf&#34;&gt;PDF&lt;/a&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
    &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/IR-book/pdf/09expand.pdf&#34;&gt;IIR chapter 9&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;https://sigir.org/files/museum/pub-08/XXIII-1.pdf&#34;&gt;Rocchios&amp;rsquo;s Feedback Paper (SIGIR 1965)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;https://www.cs.cornell.edu/people/tj/publications/joachims_97a.pdf&#34;&gt;Probabilistic Analysis of the Rocchio (Joachim &amp;lsquo;97)&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
   &lt;tr class=&#39;weekbegin&#39;&gt;
    &lt;td&gt;Week 8&lt;/td&gt;
    &lt;td&gt;Mon. 7/ 03&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Relevance Feedback &amp;amp; Query Expansion&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
         &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec10.pdf&#34;&gt;PDF&lt;/a&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
      See above.
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr class=&#34;warning&#34;&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;Tues. 8 / 03&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Probabilistic IR&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
         &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec11.pdf&#34;&gt;PDF&lt;/a&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
    &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/IR-book/pdf/11prob.pdf&#34;&gt;IIR chapter 11&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;https://seeing-theory.brown.edu/compound-probability/index.html&#34;&gt;Basics of Probability Theory&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr class=&#39;weekbegin&#39;&gt;
    &lt;td&gt;Week 9&lt;/td&gt;
    &lt;td&gt;Mon. &lt;sup&gt;13&lt;/sup&gt;&amp;frasl;&lt;sub&gt;03&lt;/sub&gt; &amp;amp; &lt;sup&gt;14&lt;/sup&gt;&amp;frasl;&lt;sub&gt;03&lt;/sub&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Language Models for IR&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
         &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp23/Lec12.pdf&#34;&gt;PDF&lt;/a&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;https://web.stanford.edu/class/cs224n/&#34;&gt;Stanford Course CS224n&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr class=&#39;weekbegin&#39;&gt;
    &lt;td&gt;Week 10&lt;/td&gt;
    &lt;td&gt;Mon. 20/ 03&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Link Analysis: HITS&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
         &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp23/Lec14HITS.pdf&#34;&gt;PDF&lt;/a&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
    &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/IR-book/pdf/21link.pdf&#34;&gt;IIR chapter 21&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=fNk_zzaMoSs&#34;&gt;Vector Spaces Video Series&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr class=&#34;warning&#34;&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;Tues. 21 / 03&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Link Analysis: PageRank&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
         &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp23/Lec14PageRank.pdf&#34;&gt;PDF&lt;/a&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
    &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/IR-book/pdf/21link.pdf&#34;&gt;IIR chapter 21&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr class=&#39;weekbegin&#39;&gt;
    &lt;td&gt;Week 11&lt;/td&gt;
    &lt;td&gt;Mon. 27/ 03&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Web Crawling&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
         &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec14.pdf&#34;&gt;PDF&lt;/a&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
    &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/IR-book/pdf/20crawl.pdf&#34;&gt;IIR chapter 20&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr class=&#34;warning&#34;&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;Tues. 28 / 03&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Word2Vec (Part I and II)&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
         &lt;li&gt;Slides:
             &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/w2vp1.pdf&#34;&gt;Part-1 PDF&lt;/a&gt;,
              &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/w2vp2.pdf&#34;&gt;Part-2 PDF&lt;/a&gt;
         &lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
    &lt;td&gt;
    &lt;ul&gt;
        &lt;li&gt; &lt;a href=&#34;https://arxiv.org/pdf/1301.3781.pdf&#34;&gt;Efficient Estimation of Word Representations in Vector Space (original word2vec paper)&lt;a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf&#34;&gt;Distributed Representations of Words and Phrases and their Compositionality (negative sampling paper)&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;&lt;/p&gt;

&lt;p&gt;&lt;h3&gt;Tentative Topics &lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt; Boolean retrieval &lt;/li&gt;
&lt;li&gt; The term vocabulary &amp;amp; postings lists &lt;/li&gt;
&lt;li&gt; Skip Pointers, Phrase Queries and Positional Indexing&lt;/li&gt;
&lt;li&gt; Scoring, term weighting &amp;amp; the vector space model&lt;/li&gt;
&lt;li&gt; Dictionaries and Tolerant Retrieval&lt;/li&gt;
&lt;li&gt; Evaluation in information retrieval&lt;/li&gt;
&lt;li&gt; Index Construction and Compression&lt;/li&gt;
&lt;li&gt; Relevance feedback &amp;amp; query expansion&lt;/li&gt;
&lt;li&gt; Probabilistic information retrieval&lt;/li&gt;
&lt;li&gt; Language models for information retrieval (Probabilistic, RNN, Transformers)&lt;/li&gt;
&lt;li&gt; Link analysis &amp;ndash; HITS, PageRank&lt;/li&gt;
&lt;li&gt; Word Vectors&lt;/li&gt;
&lt;li&gt; &lt;s&gt; Summarization &lt;/s&gt;&lt;/li&gt;
&lt;li&gt; Learning to Rank&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;

&lt;p&gt;&lt;h3&gt; Pre-requisites for the course &lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt; Data structures and Algorithms &lt;/li&gt;
&lt;li&gt; Probability and Statistics &lt;/li&gt;
&lt;li&gt; Basics of Machine Learning &lt;/li&gt;
&lt;li&gt; Basics of Natural Language Processing (Some might be covered during the course) &lt;/li&gt;
&lt;li&gt; Basics of Graph algorithms (Some might be covered during the course) &lt;/li&gt;
&lt;li&gt; Programming in Python/Java &lt;/b&gt; &lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;

&lt;p&gt;&lt;h3&gt; Reference Literature, Useful Tools and Software Resources &lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt; &lt;a href=&#34;https://nlp.stanford.edu/IR-book/information-retrieval-book.html&#34;&gt;Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schütze. &lt;em&gt;Introduction to information retrieval&lt;/em&gt;, Cambridge: Cambridge university press, 2008.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt; Research Papers shared in class. &lt;/li&gt;
  &lt;li&gt; Pranav Rajpurkar&amp;rsquo;s &lt;a href=&#34;https://docs.google.com/document/d/1uvAbEhbgS_M-uDMTzmOWRlYxqCkogKRXdbKYYT98ooc/edit#&#34;&gt;Harvard CS197 AI Research Experiences&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&#34;color:red&#34;&gt; Every test should be attempted individually by each student. Plagiarism in any form will be severely penalized.&lt;/span&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multilingual CheckList: Generation and Evaluation</title>
      <link>https://adityasomak.github.io/publication/amcg/</link>
      <pubDate>Tue, 22 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/amcg/</guid>
      <description>&lt;p&gt;The recently proposed CheckList (Riberio et al,. 2020) approach to evaluation of NLP systems has revealed high failure rates for basic capabilities for multiple state-of-the-art and commercial models. However, the CheckList creation process is manual which creates a bottleneck towards creation of multilingual CheckLists catering 100s of languages. In this work, we explore multiple approaches to generate and evaluate the quality of Multilingual CheckList. We device an algorithm &amp;ndash; Automated Multilingual Checklist Generation (AMCG) for automatically transferring a CheckList from a source to a target language that relies on a reasonable machine translation system. We then compare the CheckList generated by AMCG with CheckLists generated with different levels of human intervention. Through in-depth crosslingual experiments between English and Hindi, and broad multilingual experiments spanning 11 languages, we show that the automatic approach can provide accurate estimates of failure rates of a model across capabilities, as would a human-verified CheckList, and better than CheckLists generated by humans from scratch.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Vector Space Interpolation for Query Expansion</title>
      <link>https://adityasomak.github.io/publication/interpolation/</link>
      <pubDate>Tue, 22 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/interpolation/</guid>
      <description>&lt;p&gt;Topic-sensitive query set expansion is an important area of research that aims to improve search results for information retrieval. It is particularly crucial for queries related to sensitive and emerging topics. In this work, we describe a method for query set expansion about emerging topics using vector space interpolation. We use a transformer model called OPTIMUS, which is suitable for vector space manipulation due to its variational autoencoder nature. One of our proposed methods – Dirichlet interpolation shows promising results for query expansion. Our methods effectively generate new queries about the sensitive topic by incorporating set-level diversity, which is not captured by traditional sentence-level augmentation methods such as paraphrasing or back-translation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LITMUS Predictor: An AI Assistant for Building Reliable, High-Performing and Fair Multilingual NLP Systems</title>
      <link>https://adityasomak.github.io/publication/litmus/</link>
      <pubDate>Thu, 03 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/litmus/</guid>
      <description>&lt;p&gt;Pre-trained multilingual language models are gaining popularity due to their cross-lingual zero-shot transfer ability, but these models do not perform equally well in all languages. Evaluating task-specific performance of a model in a large number of languages is often a challenge due to lack of labeled data, as is targeting improvements in low performing languages through few-shot learning. We present a tool - LITMUS Predictor - that can make reliable performance projections for a fine-tuned task-specific model in a set of languages without test and training data, and help strategize data labeling efforts to optimize performance and fairness objectives.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CS60092 Information Retrieval (Spring 2022)</title>
      <link>https://adityasomak.github.io/courses/irspring22/</link>
      <pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/courses/irspring22/</guid>
      <description>&lt;blockquote&gt;
&lt;table id=&#34;nobd&#34; cellspacing=&#34;0&#34; cellpadding=&#34;2&#34; border=&#34;0&#34;&gt;
&lt;tbody&gt;&lt;tr&gt;&lt;td id=&#34;nobd&#34; align=&#34;left&#34; valign=&#34;top&#34;&gt;&lt;b&gt;Instructor&lt;/b&gt;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; align=&#34;left&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; align=&#34;left&#34;&gt;&lt;a href=&#34;http://cse.iitkgp.ac.in/~saditya/&#34;&gt;Somak Aditya&lt;/a&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&lt;b&gt;Timing&lt;/b&gt;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;Slot A3: MON (08:00-08:55) , MON (09:00-09:55) , TUE(12:00-12:55)
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&lt;b&gt;First Class&lt;/b&gt;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&lt;span style=&#34;color:red&#34;&gt;January 10th MON (08:00-08:55) , MON (09:00-09:55)&lt;/span&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&lt;b&gt;Venue&lt;/b&gt;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;Online Teams Channel &lt;b&gt;Information-Retrieval-2022Spring&lt;/b&gt;, Key &lt;b&gt;g7tgjqc&lt;/b&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&lt;b&gt;Teaching Assistants&lt;/b&gt;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;&lt;td id=&#34;nobd&#34; valign=&#34;top&#34; align=&#34;left&#34;&gt;
    Abhilash Nandy &lt;br/&gt;
    Ankan Mullick &lt;br/&gt;
    Neeraj Saini &lt;br/&gt;
    Ravi Pratap Singh &lt;br/&gt;
    Vaibhav Saxena &lt;br/&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/blockquote&gt;

&lt;!--
&lt;b&gt;Office Hours&lt;/b&gt; &lt;p&gt;&lt;/p&gt;
Friday - 18:10 - 19:10 (CSE-308)
--&gt;

&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Announcements&lt;/h3&gt;
&lt;ul style=&#34;list-style-type: square;&#34;&gt;

&lt;!-- &lt;li&gt; Every registered student should create an account on the Moodle system of CSE department. This system will be used for submission and grading of class tests and project. If you do not have an account already on the CSE department Moodle, create a new account for yourself following the procedure stated on the same webpage. Login to the system, and follow the link &#34;Autumn Semester (2021-22)&#34;. Choose the course &#34;CS60092_2021-22 Information Retrieval&#34;. Join this course as &#34;Student&#34;; use Student Enrolment Key: CSTU60092.
&lt;/li&gt; --&gt;
&lt;li&gt; &lt;b&gt;[Mar 28]&lt;/b&gt;  &lt;a href=&#34;https://www.microsoft.com/en-us/research/people/monojitc/&#34;&gt;Dr. Monojit Choudhury&lt;/a&gt;, Principal Data and Applied Scientist at &lt;a href=&#34;https://turing.microsoft.com/&#34;&gt;Turing India (Microsoft)&lt;/a&gt; presented a Guest Lecture on &#34;&lt;em&gt;Computing and Representing the Meanings of Words: From Wittgenstein to GPT-3 and beyond&lt;/em&gt;&#34;. The recording is available &lt;a href=&#34;https://drive.google.com/file/d/1KW4WDp2H9BKfyZv7JFGT5gXH3yUFM_fU/view?usp=sharing&#34;&gt;here (Google Drive 309.4 MB MPEG4)&lt;/a&gt;.&lt;/li&gt;

&lt;li&gt; &lt;b&gt;[Mar 17]&lt;/b&gt; Class Test 2 will be on Mar 3rd week. Syllabus will include whatever is covered after Vector Space and Scoring. &lt;/li&gt; 

&lt;li&gt; &lt;b&gt;[Jan 27]&lt;/b&gt; Class Test 1 will be on Feb 3rd week. Syllabus will include whatever is covered upto Feb 1st week. Please register in the CSE Moodle. Enrollment key is shared on Teams &lt;/li&gt; 

&lt;li&gt; &lt;b&gt;[Jan 27]&lt;/b&gt; Project Choice submission deadline: &lt;b&gt; Jan 28th 11:59 PM IST &lt;/b&gt;. &lt;/li&gt;

&lt;li&gt;  Note: for students, who are not able to register, kindly wait till 7th when the window closes. I will not be able to answer your emails individually. I will first approve based on CGPA and other criteria (such as total class strength cap etc.), once the window closes. &lt;/li&gt;

&lt;li&gt; Every &lt;b&gt;registered&lt;/b&gt; student should join the Google mailing list &lt;b&gt;ir-2022-spring@googlegroups.com&lt;/b&gt;. All urgent announcements would be made through the group. &lt;span style=&#34;color:red&#34;&gt;This group is meant only for registered (and approved) students. Kindly mention your roll number and the fact that you have registered in ERP.&lt;/span&gt;&lt;/li&gt;

&lt;li&gt; IR 2011-22 Spring will be a fully &lt;u&gt;online research-oriented course&lt;/u&gt;. &lt;/li&gt;

&lt;li&gt; First class on &lt;b&gt;January 10 (Monday), at 8:00 am (deferred from 4th due to ERP registration issues)&lt;/b&gt;. Join the class &lt;b&gt;Information-Retrieval-2022Spring&lt;/b&gt; on MS Teams (IITKGP domain; Code: g7tgjqc).
&lt;/li&gt;

&lt;li&gt; The course requires an understanding of the foundation of algorithms and data structures, probability and statistics, and knowledge of the basics of Natural Language Processing, and Machine Learning. This will be a research-oriented course that would require students to understand several CS research papers. There will be a term project that needs to be done using Python/Java. It is advisable to take this course only if you have the necessary background.
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;
Tentative Weightage: Three online proctored tests (60%), Term Project (40 %).
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;&lt;h3&gt; Pre-requisites for the course &lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt; Data structures and algorithms &lt;/li&gt;
&lt;li&gt; Probability and Statistics &lt;/li&gt;
&lt;li&gt; Basics of Machine Learning &lt;/li&gt;
&lt;li&gt; Basics of Natural Language Processing (Some might be covered during the course) &lt;/li&gt;
&lt;li&gt; Basics of Graph algorithms (Some might be covered during the course) &lt;/li&gt;
&lt;li&gt; Programming in Python/Java &lt;/b&gt; &lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;

&lt;p&gt;&lt;h3&gt;Lecture Slides&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt; Boolean retrieval - &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec2.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; The term vocabulary &amp;amp; postings lists - &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec3.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; Skip Pointers, Phrase Queries and Positional Indexing - &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec4.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; Scoring, term weighting &amp;amp; the vector space model - &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec5.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; Dictionaries and Tolerant Retrieval - &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec6.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; Evaluation in information retrieval - &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec7.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; Index Construction and Compression - &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec8.pdf&#34;&gt;PDF (Part 1)&lt;/a&gt; &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec9.pdf&#34;&gt;PDF (Part 2)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; Relevance feedback &amp;amp; query expansion - &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec10.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; Probabilistic information retrieval - &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec11.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; Language models for information retrieval - &lt;a href=&#34;https://adityasomak.github.io/files/IRSp22/Lec12.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; Link analysis &amp;ndash; HITS, PageRank&lt;/li&gt;
&lt;li&gt; Word Vectors&lt;/li&gt;
&lt;li&gt; Summarization&lt;/li&gt;
&lt;li&gt; Learning to Rank&lt;/li&gt;
&lt;li&gt; Neural IR&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;

&lt;p&gt;&lt;h3&gt; Text and Reference Literature &lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt; &lt;a href=&#34;https://nlp.stanford.edu/IR-book/information-retrieval-book.html&#34;&gt;Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schütze. &lt;em&gt;Introduction to information retrieval&lt;/em&gt;, Cambridge: Cambridge university press, 2008.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt; Research Papers shared in class. &lt;/li&gt;
&lt;/ol&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&#34;color:red&#34;&gt; Every test should be attempted individually by each student. Plagiarism in any form will be severely penalized.&lt;/span&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Teaching/Mentoring Experience</title>
      <link>https://adityasomak.github.io/courses/teachingexp/</link>
      <pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/courses/teachingexp/</guid>
      <description>&lt;p&gt;&lt;h3&gt; Teaching &lt;/h3&gt;
&lt;hr style=&#34;float: center&#34;&gt;
List of Courses Taught (at IIT Kharagpur)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Spring 2022, Information Retreival (&lt;a href=&#34;courses/irspring22/&#34;&gt;Website&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Spring 2022, Deep Learning (for PGDBA, with Prof. Sudeshna Sarkar)&lt;/li&gt;
&lt;li&gt;Spring 2022, Programming and Data Structures Lab&lt;/li&gt;
&lt;li&gt;Autumn 2022, Programmaing and Data Structures Theory (&lt;a href=&#34;https://sites.google.com/view/cs10003-fall-2022/home&#34;&gt;Website&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Autumn 2022, Programming and Data Structures Lab&lt;/li&gt;
&lt;li&gt;Spring 2023, Information Retreival (&lt;a href=&#34;courses/irspring23/&#34;&gt;Website&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Spring 2023, Programming and Data Structures Lab&lt;/li&gt;
&lt;li&gt;Autumn 2023, Machine Learning (&lt;a href=&#34;https://sites.google.com/view/cs60050-fall-2023/home&#34;&gt;Website&lt;/a&gt;, with Prof. Sudeshna Sarkar)&lt;/li&gt;
&lt;li&gt;Autumn 2023, Programming and Data Structures Lab&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Teaching Assistant Duties:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CSE-576 Natural Language Processing, Fall 2015 and Fall 2016

&lt;ul&gt;
&lt;li&gt;Responsibilities included: Creating Homework assignments, proposing class projects, and mentoring
multiple groups, delivering advanced topic lectures.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;CSE-471 Introduction To Artificial Intelligence, Spring 2016&lt;/li&gt;
&lt;li&gt;CSE-310 Data Structures and Algorithms, Spring 2015&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;h3&gt; Mentoring (Post Ph.D) &lt;/h3&gt;
&lt;hr style=&#34;float: center&#34;&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;(2023-Present) Swarup Padhi: LLM and Numeric, &lt;em&gt;Pursuing BTech Thesis&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;(2023-Present) &lt;a href=&#34;https://adityasomak.github.io/authors/daivik/&#34; target=&#34;_blank&#34;&gt;Daivik Agarwal&lt;/a&gt;: 3D Spatial Reasoning, &lt;em&gt;Pursuing BTech Thesis&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;(2023-Present) &lt;a href=&#34;https://adityasomak.github.io/authors/shreyasj/&#34; target=&#34;_blank&#34;&gt;Shreyas Jena&lt;/a&gt;: Counterfactual Video QA, &lt;em&gt;Pursuing BTech Thesis&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;(2023-Present) &lt;a href=&#34;https://adityasomak.github.io/authors/adityab/&#34; target=&#34;_blank&#34;&gt;Aditya Soni&lt;/a&gt;: LLM and Bacjdoor attacks, &lt;em&gt;Pursuing BTech Thesis&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;(2023-Present) &lt;a href=&#34;https://adityasomak.github.io/authors/debrupd/&#34; target=&#34;_blank&#34;&gt;Debrup Das&lt;/a&gt;: LLM and Mathematical Reasoning, &lt;em&gt;Pursuing MTech Thesis&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;(2022-Present) &lt;a href=&#34;https://adityasomak.github.io/authors/sachin/&#34; target=&#34;_blank&#34;&gt;Sachin Vashishtha&lt;/a&gt;: NeuroSymbolic NLP, Jailbreaks and LLM, LLM and Reasoning, &lt;em&gt;Pursuing PhD, PMRF Fellow&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;(2022) &lt;a href=&#34;https://adityasomak.github.io/authors/vivekk/&#34; target=&#34;_blank&#34;&gt;Vivek Karde&lt;/a&gt;: Analyzing Effects of Singlehop Entailments on CrossLingual NLI, &lt;em&gt;MTech Degree Awarded&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;(2021-23) &lt;a href=&#34;https://atharva-naik.github.io/&#34; target=&#34;_blank&#34;&gt;Atharva Naik&lt;/a&gt;: Code Search/Retrieval , Jailbreak and LLMs, &lt;em&gt;Presently MS Student CMU&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;(2021) &lt;a href=&#34;https://ishan00.github.io/&#34; target=&#34;_blank&#34;&gt;Ishan Tarunesh&lt;/a&gt;: CheckList NLI&lt;/li&gt;
&lt;li&gt;(2021) &lt;a href=&#34;https://scholar.google.com/citations?user=KACcWC4AAAAJ&amp;amp;hl=en&#34; target=&#34;_blank&#34;&gt;Karthikeyan K&lt;/a&gt;: Multi-lingual extension of TaxiNLI (&lt;a href=&#34;https://github.com/microsoft/TaxiXNLI&#34; target=&#34;_blank&#34;&gt;TaxiXNLI&lt;/a&gt;; Primary Mentor: Monojit Choudhury; at Duke U.)&lt;/li&gt;
&lt;li&gt;(2021) Vishesh Agarwal: Polynomial Simplification (Primary Mentor: Navin Goyal)&lt;/li&gt;
&lt;li&gt;(2020-21) &lt;a href=&#34;https://aalok-sathe.gitlab.io/&#34; target=&#34;_blank&#34;&gt;Aalok Sathe&lt;/a&gt;: TaxiNLI and TaxiXNLI (at MIT BCS)&lt;/li&gt;
&lt;li&gt;(2020) &lt;a href=&#34;https://pratikmjoshi.github.io/&#34; target=&#34;_blank&#34;&gt;Pratik Joshi&lt;/a&gt;: TaxiNLI: Taking a ride up the NLU Hill. (at CMU LTI)&lt;/li&gt;
&lt;li&gt;(2019) Pranil Joshi (IIT-B), Abhinav Mishra (IIT-G), Bhavy Khatri (IIT-K): Knowledge-sharing between Cross-domain Agents

&lt;ul&gt;
&lt;li&gt;Mentored with Kushal Chawla (USC), Sharmila Nangi Reddy&lt;/li&gt;
&lt;li&gt;(Feb 2020) Patent filed &lt;a href=&#34;https://patents.google.com/patent/US20210264111A1/en&#34; target=&#34;_blank&#34;&gt;USPTO&lt;/a&gt;. Status &lt;em&gt;pending&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;h3&gt; Mentoring (During Ph.D) &lt;/h3&gt;
&lt;hr style=&#34;float: center&#34;&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/saharudra/&#34; target=&#34;_blank&#34;&gt;Rudra Saha&lt;/a&gt; (2017 &amp;ndash; 2018), Graduated, MS Student, Computer Science, ASU, Currently Researcher@Verisk

&lt;ul&gt;
&lt;li&gt;Spatial Knowledge Distillation to aid Visual Reasoning&lt;/li&gt;
&lt;li&gt;Thesis: &lt;a href=&#34;https://repository.asu.edu/items/51644&#34; target=&#34;_blank&#34;&gt;Multimodal Representation Learning for Visual Reasoning and Text-to-Image Translation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/divyanshubandil/&#34; target=&#34;_blank&#34;&gt;Divyanshu Bandil&lt;/a&gt;(2016 &amp;ndash; 2017), Graduated MS Student, Computer Engineering.

&lt;ul&gt;
&lt;li&gt;Visual Question Categorization. Visual Question Answering in Dynamic Environments&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/trideeprath/&#34; target=&#34;_blank&#34;&gt;Trideep Rath&lt;/a&gt; (2015 &amp;ndash; 2017), Graduated, MS, Computer Science, ASU. Currently Data Scientist, Kayak

&lt;ul&gt;
&lt;li&gt;Thesis: &lt;a href=&#34;https://repository.asu.edu/attachments/186590/content/Rath_asu_0010N_17130.pdf&#34; target=&#34;_blank&#34;&gt;Word and Relation Embedding for Sentence Representation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing the Effects of Reasoning Types on Cross-Lingual Transfer Performance</title>
      <link>https://adityasomak.github.io/publication/crosslingual/</link>
      <pubDate>Wed, 22 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/crosslingual/</guid>
      <description>&lt;p&gt;Multilingual language models achieve impressive zero-shot accuracies in many languages in complex tasks such as Natural Language Inference (NLI). Examples in NLI (and equivalent complex tasks) often pertain to various types of sub-tasks, requiring different kinds of reasoning. Certain types of reasoning have proven to be more difficult to learn in a monolingual context, and in the crosslingual context, similar observations may shed light on zero-shot transfer efficiency and few-shot sample selection. Hence, to investigate the effects of types of reasoning on transfer performance, we propose a category-annotated multilingual NLI dataset.  We discuss the challenges to scale monolingual annotations to multiple languages. We statistically observe interesting effects that the confluence of reasoning types and language similarities have on transfer performance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Predicting joint intent-slot structure</title>
      <link>https://adityasomak.github.io/publication/intentslotpatent/</link>
      <pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/intentslotpatent/</guid>
      <description>&lt;p&gt;Systems and methods for natural language processing (NLP) are described. The systems may be trained by identifying training data including clean data and noisy data; predicting annotation information using an artificial neural network (ANN); computing a loss value for the annotation information using a weighted loss function that applies a first weight to the clean data and at least one second weight to the noisy data; and updating the ANN based on the loss value. The noisy data may be obtained by identifying a set of unannotated sentences in a target domain, delexicalizing the set of unannotated sentences, finding similar sentences in a source domain, filling at least one arbitrary value in the similar delexicalized sentences, generating annotation information for the similar delexicalized sentences using an annotation model for the source domain, and applying a heuristic mapping to produce annotation information for the sentences in the target domain&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Trusting RoBERTa over BERT: Insights from CheckListing the Natural Language Inference Task</title>
      <link>https://adityasomak.github.io/publication/checklist/</link>
      <pubDate>Thu, 22 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/checklist/</guid>
      <description>&lt;p&gt;The recent state-of-the-art natural language understanding (NLU) systems often behave unpredictably, failing on simpler reasoning examples. Despite this, there has been limited focus on quantifying progress towards systems with more predictable behavior. We think that reasoning capability-wise behavioral summary (proposed in Ribeiro et al. (2020)) is a step towards bridging this gap. We create a CHECKLIST test-suite (184K examples) for the Natural Language Inference (NLI) task, a representative NLU task. We benchmark state-of-the-art NLI systems on this test-suite, which reveals fine-grained insights into the reasoning abilities of BERT and RoBERTa. Our analysis further reveals inconsistencies of the models on examples derived from the same template or distinct templates but pertaining to same reasoning capability, indicating that generalizing the models’ behavior through observations made on a CheckList is non-trivial. Through an user-study, we find that users were able to utilize behavioral information to generalize much better for examples predicted from RoBERTa, compared to that of BERT.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing the Nuances of Transformers&#39; Polynomial Simplification Abilities</title>
      <link>https://adityasomak.github.io/publication/polysimp/</link>
      <pubDate>Fri, 07 May 2021 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/polysimp/</guid>
      <description>&lt;p&gt;Symbolic Mathematical tasks such as integration often require multiple welldefined steps and understanding of sub-tasks to reach a solution. To understand Transformers’ abilities in such tasks in a fine-grained manner, we deviate from traditional end-to-end settings, and explore a step-wise polynomial simplification task. Polynomials can be written in a simple normal form as a sum of monomials which are ordered in a lexicographic order. For a polynomial which is not necessarily in this normal form, a sequence of simplification steps is applied to reach the fully simplified (i.e., in the normal form) polynomial. We propose a synthetic Polynomial dataset generation algorithm that generates polynomials with unique proof steps. Through varying coefficient configurations, input representation, proof granularity, and extensive hyper-parameter tuning, we observe that Transformers consistently struggle with numeric multiplication. We explore two ways to mitigate this: Curriculum Learning and a Symbolic Calculator approach (where the numeric operations are offloaded to a calculator). Both approaches provide significant gains over the vanilla Transformers-based baseline.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Creating a knowledge graph based on text-based knowledge corpora</title>
      <link>https://adityasomak.github.io/publication/makrstaraipatent/</link>
      <pubDate>Thu, 22 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/makrstaraipatent/</guid>
      <description>&lt;p&gt;In some embodiments, a knowledge graph generation system extracts noun-phrases from sentences of a knowledge corpora and determines the relations between the noun-phrases based on a relation classifier that is configured to predict a relation between a pair of entities without restricting the entities to a set of named entities. The knowledge graph generation system further generates a sub-graph for each of the sentences based on the noun-phrases and the determined relations. Nodes or entities of the sub-graph represent the non-phrases in the sentence and edges represent the relations between the noun-phrases connected by the respective edges. The knowledge graph generation system merges the sub-graphs to generate the knowledge graph for the knowledge corpora.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TaxiNLI: Taking a Ride up the NLU Hill</title>
      <link>https://adityasomak.github.io/publication/taxinli/</link>
      <pubDate>Sat, 18 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/taxinli/</guid>
      <description>&lt;p&gt;Pre-trained Transformer-based neural architectures have consistently achieved state-of-the-art performance in the Natural Language Inference (NLI) task. Since NLI examples encompass a variety of linguistic, logical, and reasoning phenomena, it remains unclear as to which specific concepts are learnt by the trained systems and where they can achieve strong generalization. To investigate this question, we propose a taxonomic hierarchy of categories that are relevant for the NLI task. We introduce TAXINLI, a new dataset, that has 10k examples from the MNLI dataset (Williams et al., 2018) with these taxonomic labels. Through various experiments on TAXINLI, we observe that whereas for certain taxonomic categories SOTA neural models have achieved near perfect accuracies - a large jump over the previous models - some categories still remain difficult. Our work adds to the growing body of literature that shows the gaps in the current NLI systems and datasets through a systematic presentation and analysis of reasoning categories.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mathematical Reasoning</title>
      <link>https://adityasomak.github.io/project/symbolicmath/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/project/symbolicmath/</guid>
      <description>&lt;p&gt;&lt;h3&gt; Symbolic Mathematics &lt;/h3&gt;
In a recent work (&lt;a href=&#34;https://mathai-iclr.github.io/papers/papers/MATHAI_6_paper.pdf&#34; target=&#34;_blank&#34;&gt;PolySimp ICLR 2021 MathAI Workshop&lt;/a&gt;) with Navin Goyal and Vishesh Agarwal, we explored Transformers&amp;rsquo; abilities to perform multiple-step reasoning in well-defined purely symbolic tasks such as step-wise polynomial simplification.  Polynomials can be written in a simple normal form as a sum of monomials which are ordered in a lexicographic order. For a polynomial which is not necessarily in this normal form, a sequence of simplification steps is applied to reach the fully simplified (i.e., in the normal form) polynomial. We propose a synthetic Polynomial dataset generation algorithm that generates polynomials with unique proof steps.&lt;/p&gt;

&lt;p&gt;Through varying coefficient configurations, input representation, proof granularity, and extensive hyper-parameter tuning, we observe that Transformers consistently struggle with numeric multiplication. We explore two ways to mitigate this: Curriculum Learning and a Symbolic Calculator approach (where the numeric operations are offloaded to a calculator). Both approaches provide significant gains over the vanilla Transformers-based baseline.&lt;/p&gt;

&lt;p&gt;&lt;h3&gt; Mathematical Word Problems &lt;/h3&gt;
We have further moved on to both simple (graduate school level arithmetic) and harder mathematical problems.&lt;/p&gt;

&lt;p&gt;With colleagues in SUTD (Pengfei Hong, Deepanway Ghoshal, Navonil Majumdar, Prof. Soujanya Poria) and Univ. of Michigan (Prof. Rada Mihalcea), we investigate robustness of LLMs&amp;rsquo; mathematical understanding abilities. While LLMs showcase striking results on existing math word problems, the true depth of their competencies and robustness, in mathematical reasoning tasks, remains an open question. In response, we develop (i) an ontology of perturbations of maths questions, (ii) a semi-automatic method of perturbation, and (iii) a dataset of perturbed maths questions to probe the limits of LLM capabilities in mathematical-reasoning tasks. Through careful perturbations of a simple dataset (GSM8k), we create a variant named MORE. We conducted comprehensive evaluation of both closed-source and open-source LLMs on &lt;span style=&#34;font-variant-caps: small-caps&#34;&gt;More&lt;/span&gt;. The results show a significant performance drop across all the models against the perturbed questions. This strongly suggests that current LLMs lack robust mathematical skills and a deep understanding of reasoning. The work is currently published as an &lt;a href=&#34;https://arxiv.org/pdf/2401.09395.pdf&#34; target=&#34;_blank&#34;&gt;ArXiv Preprint&lt;/a&gt;.
&lt;!-- &lt;div&gt;
&lt;div id=&#34;References&#34; align=&#34;left&#34; style=&#34;width: 100%; overflow-y: hidden;&#34; class=&#34;wcustomhtml&#34;&gt;&lt;h3 style=&#34;margin-bottom:0px;&#34;&gt;References&lt;/h3&gt;
&lt;hr style=&#34;float: center&#34;&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
Vishesh Agarwal, Somak Aditya, Navin Goyal. Analyzing the Nuances of Transformers&#39; Polynomial Simplification Abilities. ICLR 2021 MathAI Workshop. 
&lt;/li&gt;
&lt;/ul&gt; --&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Uncovering Relations for Marketing Knowledge Representation</title>
      <link>https://adityasomak.github.io/publication/makrstarai/</link>
      <pubDate>Fri, 07 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/makrstarai/</guid>
      <description>&lt;p&gt;Online behaviors of consumers and marketers generate massive marketing data, which ever more sophisticated models attempt to turn into insights and aid decisions by
marketers. Yet, in making decisions human managers bring to bear marketing knowledge which reside outside of data and models. Thus, it behooves creation of an automated
marketing knowledge base that can interact with data and models. Currently, marketing knowledge is dispersed in large corpora, but no definitive knowledge base for
marketing exists. Out of the two broad aspects of marketing knowledge - representation and reasoning - this treatise focuses on the former. Specifically,
we focus on creation of marketing knowledge graph from corpora, which requires identification of entities and relations. The relation identification task is
particularly challenging in marketing, because of the non-factoid nature of much marketing knowledge, and the difficulty of forming rules that govern relations.
Specifically, we define a set of relations to capture marketing knowledge, propose a pipeline for creating the knowledge graph from text and propose a
rule-guided semi-supervised relation prediction algorithm to extract relations between marketing entities from sentences.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Integrating Knowledge and Reasoning in Image Understanding</title>
      <link>https://adityasomak.github.io/publication/integratingsurvey/</link>
      <pubDate>Wed, 09 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/integratingsurvey/</guid>
      <description>&lt;p&gt;Deep learning based data-driven approaches have been successfully applied in various image under-
standing applications ranging from object recognition, semantic segmentation to visual question an-
swering.  However, the lack of knowledge integration as well as higher-level reasoning capabilities
with  the  methods  still  pose  a  hindrance. In  this work, we present a brief survey of a few represen-
tative  reasoning  mechanisms,  knowledge  integration methods and their corresponding image under-
standing applications developed by various groups of researchers, approaching the problem from a va-
riety of angles.  Furthermore, we discuss upon key efforts on integrating external knowledge with neu-
ral networks. Taking cues from these efforts, we conclude by discussing potential pathways to im-
prove reasoning capabilities.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Knowledge and Reasoning for Image Understanding</title>
      <link>https://adityasomak.github.io/publication/thesis/</link>
      <pubDate>Wed, 09 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/thesis/</guid>
      <description>&lt;p&gt;Image Understanding is a long-established discipline in computer vision, which encompasses a body of advanced image processing techniques, that are used to locate (“where”), characterize and recognize (“what”) objects, regions, and their attributes in the image. However, the notion of “understanding” (and the goal of artificial intelligent machines) goes beyond factual recall of the recognized components and includes reasoning and thinking beyond what can be seen (or perceived). Understanding is often evaluated by asking questions of increasing difficulty. Thus, the expected functionalities of an intelligent Image Understanding system can be expressed in terms of the functionalities that are required to answer questions about an image. Answering questions about images require primarily three components: Image Understanding, question (natural language) understanding, and reasoning based on knowledge. Any question, asking beyond what can be directly seen, requires modeling of commonsense (or background/ontological/factual) knowledge and reasoning.&lt;/p&gt;

&lt;p&gt;Knowledge and reasoning have seen scarce use in image understanding applications. In this thesis, we demonstrate the utilities of incorporating background knowledge and using explicit reasoning in image understanding applications. We first present a comprehensive survey of the previous work that utilized background knowledge and reasoning in understanding images. This survey outlines the limited use of commonsense knowledge in high-level applications. We then present a set of vision and reasoning-based methods to solve several applications and show that these approaches benefit in terms of accuracy and interpretability from the explicit use of knowledge and reasoning. We propose novel knowledge representations of image, knowledge acquisition methods, and a new implementation of an efficient probabilistic logical reasoning engine that can utilize publicly available commonsense knowledge to solve applications such as visual question answering, image puzzles. Additionally, we identify the need for new datasets that explicitly require external commonsense knowledge to solve. We propose the new task of Image Riddles, which requires a combination of vision, and reasoning based on ontological knowledge; and we collect a sufficiently large dataset to serve as an ideal testbed for vision and reasoning research. Lastly, we propose end-to-end deep architectures that can combine vision, knowledge and reasoning modules together and achieve large performance boosts over state-of-the-art methods&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Spatial Knowledge Distillation to aid Visual Reasoning</title>
      <link>https://adityasomak.github.io/publication/spatialkd/</link>
      <pubDate>Wed, 09 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/spatialkd/</guid>
      <description>&lt;p&gt;For tasks involving language and vision, the current state-of-the-art methods do not leverage any additional information
that might be present to gather privileged knowledge. Instead, such an ability is expected to be learnt during the training
phase. One such task is Visual Question Answering, where large diagnostic datasets have been proposed to
test a system’s capability of reasoning and answering questions about images. In
this work, we take a step towards integrating this additional privileged information in the form of spatial knowledge to
aid in visual reasoning. We propose a framework that combines recent advances in knowledge distillation (teacherstudent
framework), relational reasoning and probabilistic logical languages to incorporate such knowledge in existing
neural networks for the surrogate task of fact-based Visual Question Answering. Specifically, for a question posed
against an image, we use a probabilistic logical language to encode the spatial knowledge and the spatial understanding
about the question in the form of a mask that is directly provided to the teacher network. The student network learns
from the ground-truth information as well as the teacher’s prediction via distillation. We also demonstrate the impact
of predicting such a mask inside the teacher’s network using attention. Empirically, we show that both the methods
improve the test accuracy over a state-of-the-art approach on a publicly available dataset.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tr^2AIL</title>
      <link>https://adityasomak.github.io/people/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/people/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Explicit Reasoning over End-to-End Neural Architectures</title>
      <link>https://adityasomak.github.io/publication/pslvqa/</link>
      <pubDate>Sat, 17 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/pslvqa/</guid>
      <description>&lt;p&gt;Many vision and language tasks require commonsense reasoning beyond data-driven image and natural language pro-
cessing. Here we adopt Visual Question Answering (VQA) as an example task, where a system is expected to answer a
question in natural language about an image. Current state-of-the-art systems attempted to solve the task using deep neural
architectures and achieved promising performance. However, the resulting systems are generally opaque and they struggle
in understanding questions for which extra knowledge is required. In this paper, we present an explicit reasoning layer on
top of a set of penultimate neural network based systems. The reasoning layer enables reasoning and answering questions
where additional knowledge is required, and at the same time provides an interpretable interface to the end users. Specif-
ically, the reasoning layer adopts a Probabilistic Soft Logic (PSL) based engine to reason over a basket of inputs: visual
relations, semantic parse of the question, and background ontological knowledge from word2vec and ConceptNet.
Experimental analysis of the answers and the key evidential predicates generated on the VQA dataset validate our approach.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reasoning in NLP</title>
      <link>https://adityasomak.github.io/project/nlp/</link>
      <pubDate>Wed, 24 Feb 2016 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/project/nlp/</guid>
      <description>&lt;p&gt;Models that claim to understand language, should also be able to demonstrate its abilities to reason across various dimensions. My present goal is to &lt;em&gt;evaluate&lt;/em&gt;, &lt;em&gt;enhance&lt;/em&gt; and &lt;em&gt;explain&lt;/em&gt; the reasoning capabilities of such systems (or language models).&lt;/p&gt;

&lt;p&gt;&lt;h4&gt; &lt;span style=&#34;color:red&#34;&gt;!!NEW!!&lt;/span&gt; Reasoning in LLMs &lt;/h4&gt;
Our group has invested significantly in advancing reasoning abilities of LLMs in a multi-hop setting. The following drafts are in progress: 1) DeSelect$^+$: Efficient Leaf Selection to Improve Entailment Tree Generation, 2) A comprehensive survey of Logical Reasoning abilities of Large Language Models alongwith a benchmark, and 3) Multi-step Logical Reasoning under Incomplete Knowledge.&lt;/p&gt;

&lt;p&gt;References
&lt;ul&gt;
&lt;li&gt; &lt;a href=&#34;https://arxiv.org/pdf/2401.10065.pdf&#34; target=&#34;_blank&#34;&gt;Code Prompting Elicits Conditional Reasoning Abilities
in Text+Code LLMs&lt;/a&gt;,  Haritz Puerto&lt;sup&gt;1&lt;/sup&gt;, Martin Tutek&lt;sup&gt;1&lt;/sup&gt;, Somak Aditya&lt;sup&gt;2&lt;/sup&gt;, Xiaodan Zhu&lt;sup&gt;1,3&lt;/sup&gt;, Iryna Gurevych&lt;sup&gt;1&lt;/sup&gt;
&lt;sup&gt;1&lt;/sup&gt;Ubiquitous Knowledge Processing Lab (UKP Lab),TU Darmstadt and Hessian Center for AI (hessian.AI)
&lt;sup&gt;2&lt;/sup&gt;IIT Kharagpur, &lt;sup&gt;2&lt;/sup&gt;Queen’s University, &lt;em&gt;ArXiv Jan 2024&lt;/em&gt; &lt;span style=&#34;color:red&#34;&gt;!!NEW!!&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt; &lt;a href=&#34;https://arxiv.org/pdf/2310.00836.pdf&#34; target=&#34;_blank&#34;&gt;Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing
Logical Reasoning Capabilities of Language Models&lt;/a&gt;, Man Luo&lt;sup&gt;1,2&lt;em&gt;&lt;/sup&gt;,  Shrinidhi Kumbhar&lt;sup&gt;1&lt;/em&gt;&lt;/sup&gt;, Ming shen,&lt;sup&gt;1&lt;/sup&gt; Mihir Parmar&lt;sup&gt;1&lt;/sup&gt;, Neeraj Varshney&lt;sup&gt;1&lt;/sup&gt;, Pratyay Banerjee&lt;sup&gt;3&lt;/sup&gt;, Somak Aditya&lt;sup&gt;4&lt;/sup&gt;, Chitta Baral&lt;sup&gt;1&lt;/sup&gt;, &lt;sup&gt;1&lt;/sup&gt;Arizona State University, &lt;sup&gt;2&lt;/sup&gt;Mayo Clinic, &lt;sup&gt;3&lt;/sup&gt;Amazon Alexa AI, &lt;sup&gt;4&lt;/sup&gt;IIT KGP &lt;em&gt;ArXiv Nov 2023&lt;/em&gt; &lt;span style=&#34;color:red&#34;&gt;!!NEW!!&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt; &lt;a href=&#34;https://arxiv.org/abs/2208.14641&#34; target=&#34;_blank&#34;&gt;Generating Intermediate Steps for NLI with Next-Step Supervision&lt;/a&gt;, &lt;em&gt;AACL-IJCNLP 2023, Main&lt;/em&gt; &lt;span style=&#34;color:red&#34;&gt;!!NEW!!&lt;/span&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;

&lt;p&gt;&lt;h3&gt; Natural Language Inference &lt;/h3&gt;
Large pre-trained language models show high performance in popular NLP benchmarks (GLUE, SuperGLUE), while failing poorly in datasets with targeted linguistic and logical phenomena. We consolidate the interesting reasoning phenomena
in Taxonomy of reasoning w.r.t the NLI task. Our first work along this line published in &lt;a href=&#34;https://github.com/microsoft/TaxiNLI&#34; target=&#34;_blank&#34;&gt;CoNLL 2020&lt;/a&gt; showed that these models (BERT, RoBERTa) may not know how to perform certain types of reasoning such as causal, numeric, spatial, temporal; but they can identify the type of reasoning required for a new example.&lt;/p&gt;

&lt;p&gt;We did a follow-up, adapting the CheckList methodology, where we create a large &lt;a href=&#34;https://github.com/microsoft/LoNLI&#34; target=&#34;_blank&#34;&gt;CheckList-NLI&lt;/a&gt; dataset to individually yet collectively test different reasoning capabilities, including pragmatic ones. Through our test-suite, we show that such a post-hoc evaluation provides a more comprehensive overview of the behavioral nature of the language models. A thorough human study with Linguistic Olympiad participants shows that behavioral summary leads to better explanation and RoBERTa&amp;rsquo;s behavior is more predictable than BERT. Currently, we are also exploring augmenting NLI datasets with verifiable proofs.&lt;/p&gt;

&lt;p&gt;Summary and Extensions:
&lt;ul&gt;
&lt;li&gt; &lt;a href=&#34;https://github.com/microsoft/TaxiNLI&#34; target=&#34;_blank&#34;&gt;TaxiNLI: Taxonomic Fragmentation of the NLI Task&lt;/a&gt;, &lt;em&gt;CoNLL 2020&lt;/em&gt;
&lt;/li&gt;
&lt;li&gt; &lt;a href=&#34;https://github.com/microsoft/TaxiXNLI&#34; target=&#34;_blank&#34;&gt;TaxiXNLI: Multi-lingual Extension of TaxiNLI&lt;/a&gt;, &lt;em&gt;EMNLP 2021 MRL Workshop&lt;/em&gt;
&lt;/li&gt;
&lt;li&gt; &lt;a href=&#34;https://github.com/microsoft/LoNLI&#34; target=&#34;_blank&#34;&gt;LoNLI: Testing Diverse Reasoning of NLI Systems&lt;/a&gt;, &lt;em&gt;LREV 2023, In Print, &lt;span style=&#34;color:red&#34;&gt;!!NEW!!&lt;/span&gt;&lt;/em&gt;
&lt;/li&gt;&lt;/p&gt;

&lt;p&gt;&lt;/ul&gt;&lt;/p&gt;

&lt;div&gt;
&lt;h4&gt; Enhancing NLI: Multi-hop, Causality and Counterfactuals &amp; Reasoning in LLMs &lt;/h4&gt;
As observed through TaxiNLI family of work, language models struggle with many important reasoning types. With Deepanway Ghoshal and Monojit choudhury, we explored a &lt;b&gt;less annotation-intensive&lt;/b&gt; way to &lt;a href=https://arxiv.org/abs/2208.14641&#34;&gt;generate intermediate steps&lt;/a&gt; for complex reasoning examples in free-form NLI datasets. We observe, not only, we can generate such multi-hop steps without end-to-end supervision; but the steps are accurate as they &lt;em&gt;can be augmented directly&lt;/em&gt; to improve NLI model&#39;s predictive ability. 


&lt;img src=&#34;https://adityasomak.github.io/project/prooftypes.png&#34; alt=&#34;img&#34;/&gt;
&lt;/div&gt;

&lt;p&gt;References
&lt;ul&gt;
&lt;li&gt; &lt;a href=&#34;https://arxiv.org/abs/2208.14641&#34; target=&#34;_blank&#34;&gt;Generating Intermediate Steps for NLI with Next-Step Supervision&lt;/a&gt;, &lt;em&gt;AACL-IJCNLP 2023, Main&lt;/em&gt; &lt;span style=&#34;color:red&#34;&gt;!!NEW!!&lt;/span&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;

&lt;hr style=&#34;width:100%;text-align:left;margin-left:0&#34;&gt;

&lt;p&gt;Previously I have been interested in mapping natural language to formal language representation and reasoning with it. My proposed solutions towards Question-Answering and Winograd Schema Challenge during my Ph.D have been motivated by the central idea of semantic parsing, followed by logical (or probabilistic logical) reasoning.&lt;/p&gt;

&lt;p&gt;&lt;h3&gt; Semantic Parsing (K-Parser) &lt;/h3&gt;
We (led by co-authors Arpit Sharma and Nguyen Vo) have explored mapping of natural language to formal representation, that enbales logical reasoning. Through several papers (&lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.717.6262&amp;amp;rep=rep1&amp;amp;type=pdf&#34; target=&#34;_blank&#34;&gt;K-Parser IJCAI-15&lt;/a&gt;, &lt;a href=&#34;https://aclanthology.org/W15-0811.pdf&#34; target=&#34;_blank&#34;&gt;K-Parser NAACL 15&lt;/a&gt;), we showed how such semantic parsing enables us to find event mentions, and (even patially but interpretably) solved Winograd Schema challenge problems.&lt;/p&gt;

&lt;!-- &lt;div&gt;
&lt;div id=&#34;References&#34; align=&#34;left&#34; style=&#34;width: 100%; overflow-y: hidden;&#34; class=&#34;wcustomhtml&#34;&gt;&lt;h3 style=&#34;margin-bottom:0px;&#34;&gt;References&lt;/h3&gt;
&lt;hr style=&#34;float: center&#34;&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
Ishan Tarunesh, Somak Aditya, Monojit Choudhury. Trusting RoBERTa over BERT: Insights from CheckListing the Natural Language Inference Task. Arxiv 2015. 
&lt;/li&gt;&lt;li&gt;
Pratik Joshi*, Somak Aditya*, Aalok Sathe*, Monojit Choudhury. TaxiNLI: Taking a Ride up the NLU Hill. CoNLL 2020.
&lt;/li&gt;&lt;li&gt;
Arpit Sharma, Somak Aditya, Vo Nguyen and Chitta Baral. Towards Addressing the Winograd Schema Challenge - Building and Using a Semantic Parser and a Knowledge Hunting Module. IJCAI 2015.
&lt;/li&gt;&lt;li&gt;
Somak Aditya, Chitta Baral, Nguyen Ha Vo, Joohyung Lee, Jieping Ye, Zaw Naung, Barry Lumpkin, Jenny Hastings, Richard Scherl, Dawn M. Sweet, Daniela Inclezan. Recognizing Social Constructs from Textual Conversation. HLT-NAACL 2015.
&lt;/li&gt;&lt;li&gt;
Arpit Sharma, Nguyen H. Vo, Somak Aditya and Chitta Baral. Identifying Various Kinds of Event Mentions in K-Parser Output The 3rd Workshop on EVENTS: Definition, Detection, Coreference, and Representation. HLT-NAACL 2015.
&lt;/li&gt;
&lt;/ul&gt; --&gt;
</description>
    </item>
    
    <item>
      <title>Vision and Reasoning</title>
      <link>https://adityasomak.github.io/project/vision/</link>
      <pubDate>Sun, 08 Nov 2015 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/project/vision/</guid>
      <description>&lt;p&gt;Many vision and language tasks require commonsense reasoning beyond data-driven image and natural language processing. Cognitive Sciences and Active Vision literature points to an explicit iterative interaction
among perception, reasoning, and memory (knowledge) modules (&lt;a href=&#34;https://www.public.asu.edu/~cbaral/papers/acs2016.pdf&#34; target=&#34;_blank&#34;&gt;DeepIU ACS 2015&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;h3&gt; Ongoing Projects &lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt; SERB DST Startup Research Grant (2021-23) ~ INR 26 Lacs
        | Topic: &amp;ldquo;Learning from Rules and Data for Image Analytics&amp;rdquo;&lt;/li&gt;
&lt;li&gt; IIT Kharagpur Faculty Startup Research Grant (2022-24) ~ INR 25 Lacs
        &lt;br/&gt; Topic: The Role of Feedback in Vision-Language enabled Embodied Agents towards Applications in Desire Management
        &lt;br/&gt; Joint PI: Prof. Pawan Goyal &lt;/li&gt;
&lt;li&gt; Counterfactual Reasoning in Videos &lt;/li&gt;
&lt;li&gt; Active Learning for 3D Video Grounding (with Dr. Maneesh Singh)&lt;/li&gt;
&lt;/ol&gt;&lt;/p&gt;

&lt;p&gt;&lt;h3&gt; Captioning &lt;/h3&gt;
In our earliest attempt (&lt;a href=&#34;https://imagesdg.wordpress.com/image-to-scene-description-graph/&#34; target=&#34;_blank&#34;&gt;CVIU 2017&lt;/a&gt;), we used a combination of image classification, reasoning with commonsense knowledge (extracted from training captions) to propose a Scene Description Graph as an intermediate representation for a natural image. We showed the efficacy of this representation through image captioning, image retrieval tasks (and QA case studies).&lt;/p&gt;

&lt;p&gt;&lt;h3&gt; Visual QA, Image Puzzles and Visual Reasoning &lt;/h3&gt;
We have proposed instantiations of this abstract architecture to solve image puzzles, VQA and visual reasoning tasks such as CLEVR. In our &lt;a href=&#34;https://visionandreasoning.wordpress.com&#34; target=&#34;_blank&#34;&gt;AAAI 2018 VQA&lt;/a&gt;, and &lt;a href=&#34;https://imageriddle.wordpress.com/imageriddle/&#34; target=&#34;_blank&#34;&gt;UAI 2018 Puzzles&lt;/a&gt; work, we have proposed an explicit probabilistic soft logic layer on top of a neural architecture that helps integrate commonsense knowledge and induces post-hoc interpretability.&lt;/p&gt;

&lt;p&gt;Later on, for an end-to-end (differentiable) integration of spatial knowledge, we explore a combination of knowledge distillation, probabilistic logic, and relational network in our &lt;a href=&#34;https://www.public.asu.edu/~cbaral/papers/2019-wacv.pdf&#34; target=&#34;_blank&#34;&gt;WACV 2019 CLEVR&lt;/a&gt;.&lt;/p&gt;

&lt;!-- &lt;div&gt;
&lt;div id=&#34;References&#34; align=&#34;left&#34; style=&#34;width: 100%; overflow-y: hidden;&#34; class=&#34;wcustomhtml&#34;&gt;&lt;h3 style=&#34;margin-bottom:0px;&#34;&gt;References&lt;/h3&gt;
&lt;hr style=&#34;float: center&#34;&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt; 
&lt;li&gt; 
Somak Aditya, Yezhou Yang, Chitta Baral. Integrating Knowledge and Reasoning in Image Understanding. IJCAI 2019. 
&lt;/li&gt;
&lt;li&gt;
Somak Aditya, Rudra Saha, Yezhou Yang and Chitta Baral. Spatial Knowledge Distillation to aid Visual Reasoning. WACV 2019. 
&lt;/li&gt;
&lt;li&gt;
Somak Aditya, Yezhou Yang, Chitta Baral. Explicit Reasoning over End-to-End Neural Architectures for Visual Question Answering. AAAI 2018.
&lt;/li&gt;
&lt;li&gt;
Somak Aditya, Yezhou Yang, Chitta Baral and Yiannis Aloimonos. Combining Knowledge and Reasoning through Probabilistic Soft Logic for Image Puzzle Solving. UAI 2018.
&lt;/li&gt;
&lt;li&gt;
Somak Aditya, Yezhou Yang, Chitta Baral, Yiannis Aloimonos and Cornelia Fermuller. Image Understanding using Vision and Reasoning through Scene Description Graph. Computer Vision and Image Understanding Journal. (Accepted December 2017)
&lt;/li&gt;
&lt;li&gt;
Somak Aditya, Yezhou Yang, Chitta Baral and Yiannis Aloimonos. Answering Image Riddles using Vision and Reasoning through Probabilistic Soft Logic.. Arxiv version. 2016. Website with additional information on this work.
&lt;/li&gt;
&lt;li&gt;
Somak Aditya, Chitta Baral, Yezhou Yang, Yiannnis Aloimonos and Cornelia Fermuller. DeepIU: An architecture for image understanding. Advances in Cognitive Systems. 2016.
&lt;/li&gt;
&lt;li&gt;
Somak Aditya, Yezhou Yang, Chitta Baral, Cornelia Fermuller, Yiannis Aloimonos. From Images to Sentences through Scene Description Graphs using Commonsense Reasoning and Knowledge. Arxiv version.
&lt;/li&gt;
&lt;li&gt;
Somak Aditya, Yiannis Aloimonos, Chitta Baral, Cornelia Fermuller and Yezhou Yang. Visual common-sense for scene understanding using perception, semantic parsing and reasoning. Common-sense 2015, AAAI 2015 Spring Symposium. (Appenidix with code.)
&lt;/li&gt;
&lt;/ul&gt; --&gt;
</description>
    </item>
    
    <item>
      <title>Recognizing social constructs from textual conversation</title>
      <link>https://adityasomak.github.io/publication/social/</link>
      <pubDate>Sat, 25 Jul 2015 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/social/</guid>
      <description>&lt;p&gt;In this paper we present our work on recognizing high level social constructs such as Leadership and Status from textual conversation using an approach that makes use of the background knowledge about social hierarchy and integrates statistical methods and symbolic logic based methods. We use a stratified approach in which we first detect lower level language constructs such as politeness, command and agreement that help us to infer intermediate constructs such as deference, closeness and authority that are observed between the parties engaged in conversation. These intermediate constructs in turn are used to determine the social constructs Leadership and Status. We have implemented this system successfully in both English and Korean languages and achieved considerable accuracy.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Towards Addressing the Winograd Schema Challenge-Building and Using a Semantic Parser and a Knowledge Hunting Module.</title>
      <link>https://adityasomak.github.io/publication/kparser/</link>
      <pubDate>Sat, 25 Jul 2015 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/kparser/</guid>
      <description>&lt;p&gt;Concerned about the Turing test’s ability to correctly
evaluate if a system exhibits human-like intelligence,
the Winograd Schema Challenge (WSC)
has been proposed as an alternative. A Winograd
Schema consists of a sentence and a question. The
answers to the questions are intuitive for humans
but are designed to be difficult for machines, as
they require various forms of commonsense knowledge
about the sentence. In this paper we demonstrate
our progress towards addressing the WSC.
We present an approach that identifies the knowledge
needed to answer a challenge question, hunts
down that knowledge from text repositories, and
then reasons with them to come up with the answer.
In the process we develop a semantic parser
(&lt;a href=&#34;www.kparser.org&#34; target=&#34;_blank&#34;&gt;www.kparser.org&lt;/a&gt;). We show that our approach
works well with respect to a subset of Winograd
schemas.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Identifying various kinds of event mentions in k-parser output</title>
      <link>https://adityasomak.github.io/publication/identifying/</link>
      <pubDate>Thu, 25 Jun 2015 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/identifying/</guid>
      <description>&lt;p&gt;In this paper we show how our semantic parser (Knowledge Parser or K-Parser) identifies various kinds of event mentions in the input text. The types include recursive (complex) and non recursive event mentions. K-Parser outputs each event mention in form of an acyclic graph with root nodes as the verbs that drive those events. The children nodes of the verbs represent the entities participating in the events, and their conceptual classes. The on-line demo of the system is available at &lt;a href=&#34;http://kparser.org&#34; target=&#34;_blank&#34;&gt;http://kparser.org&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Visual common-sense for scene understanding using perception, semantic parsing and reasoning.</title>
      <link>https://adityasomak.github.io/publication/common-sense/</link>
      <pubDate>Thu, 12 Mar 2015 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/common-sense/</guid>
      <description>&lt;p&gt;In this paper we explore the use of visual commonsense
knowledge and other kinds of knowledge (such as
domain knowledge, background knowledge, linguistic
knowledge) for scene understanding. In particular, we
combine visual processing with techniques from natural
language understanding (especially semantic parsing),
common-sense reasoning and knowledge representation
and reasoning to improve visual perception to reason
about finer aspects of activities&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
