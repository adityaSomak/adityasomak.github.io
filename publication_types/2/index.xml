<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2 | Somak Aditya</title>
    <link>https://adityasomak.github.io/publication_types/2/</link>
      <atom:link href="https://adityasomak.github.io/publication_types/2/index.xml" rel="self" type="application/rss+xml" />
    <description>2</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 07 Aug 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://adityasomak.github.io/img/icon-192.png</url>
      <title>2</title>
      <link>https://adityasomak.github.io/publication_types/2/</link>
    </image>
    
    <item>
      <title>LoNLI: An Extensible Framework for Testing Diverse Logical Reasoning Capabilities for NLI</title>
      <link>https://adityasomak.github.io/publication/lonli/</link>
      <pubDate>Mon, 07 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/lonli/</guid>
      <description>&lt;p&gt;Natural Language Inference (NLI) is considered a representative task to test natural language understanding (NLU).  In this work, we propose an extensible framework to collectively yet categorically test diverse Logical reasoning capabilities required for NLI (and by extension, NLU). Motivated by behavioral testing, we create a semi-synthetic large test-bench (363 templates, 363k examples) and an associated framework that offers following utilities: 1) individually test and analyze reasoning capabilities along 17 reasoning dimensions (including pragmatic reasoning), 2) design experiments to study cross-capability information content (leave one out or bring one in); and 3) the synthetic nature enable us to control for artifacts and biases. We extend a publicly available framework of automated test case instantiation from free-form natural language templates (CheckList), and a well-defined taxonomy of capabilities to cover a wide range of increasingly harder test cases while varying the complexity of natural language. Through our analysis of state-of-the-art NLI systems, we observe that our benchmark is indeed hard (and non-trivial even with training on additional resources). Some capabilities stand out as harder. Further fine-grained analysis and fine-tuning experiments reveal more insights about these capabilities and the models &amp;ndash; supporting and extending previous observations; thus showing the utility of the proposed testbench.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Image Understanding using Vision and Reasoning through Scene Description Graph</title>
      <link>https://adityasomak.github.io/publication/sdg_cviu/</link>
      <pubDate>Mon, 18 Dec 2017 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/sdg_cviu/</guid>
      <description>&lt;p&gt;Two of the fundamental tasks in image understanding using text are caption generation and visual question answering
[4, 72]. This work presents an intermediate knowledge structure that can be used for both tasks to obtain increased interpretability.
We call this knowledge structure Scene Description Graph (SDG), as it is a directed labeled graph, representing objects, actions,
regions, as well as their attributes, along with inferred concepts and semantic (from KM-Ontology [12]), ontological (i.e. superclass, hasProperty),
and spatial relations. Thereby a general architecture is proposed in which a system can represent both the content and underlying concepts of an
image using an SDG. The architecture is implemented using generic visual recognition techniques and commonsense reasoning to extract graphs from
images. The utility of the generated SDGs is demonstrated in the applications of image captioning, image retrieval, and through examples in
visual question answering. The experiments in this work show that the extracted graphs capture syntactic and semantic content of images
with reasonable accuracy.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
