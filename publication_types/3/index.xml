<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>3 | Somak Aditya</title>
    <link>https://adityasomak.github.io/publication_types/3/</link>
      <atom:link href="https://adityasomak.github.io/publication_types/3/index.xml" rel="self" type="application/rss+xml" />
    <description>3</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 17 Jan 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://adityasomak.github.io/img/icon-192.png</url>
      <title>3</title>
      <link>https://adityasomak.github.io/publication_types/3/</link>
    </image>
    
    <item>
      <title>STUCK IN THE QUICKSAND OF NUMERACY, FAR FROM AGI SUMMIT: EVALUATING LLMS&#39; MATHEMATICAL COMPETENCY THROUGH ONTOLOGY-GUIDED PERTURBATIONS</title>
      <link>https://adityasomak.github.io/publication/more24/</link>
      <pubDate>Wed, 17 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/more24/</guid>
      <description>&lt;p&gt;Recent advancements in Large Language Models (LLMs) have showcased striking results on existing logical reasoning benchmarks, with some models even surpassing human performance. However, the true depth of their competencies and robustness, in mathematical reasoning tasks, remains an open question. In response, we develop (i) an ontology of perturbations of maths questions, (ii) a semi-automatic method of perturbation, and (iii) a dataset of perturbed maths questions
to probe the limits of LLM capabilities in mathematical-reasoning tasks.
These controlled perturbations span across multiple fine dimensions of the structural and representational aspects of maths questions. Using GPT-4, we generated the &lt;span style=&#34;font-variant-caps: small-caps&#34;&gt;More&lt;/span&gt; dataset by perturbing randomly selected five seed questions from GSM8K. This process was guided by our ontology and involved a thorough automatic and manual filtering process, yielding a set of 216 maths problems.
We conducted comprehensive evaluation of both closed-source and open-source LLMs on &lt;span style=&#34;font-variant-caps: small-caps&#34;&gt;More&lt;/span&gt;. The results show a significant performance drop across all the models against the perturbed questions. This strongly suggests that current LLMs lack robust mathematical skills and a deep understanding of reasoning. This research not only identifies multiple gaps in the capabilities of current models, but also highlights multiple potential directions for future development. Our dataset will be made publicly available at &lt;a href=&#34;https://huggingface.co/datasets/declare-lab/GSM8k_MORE&#34;&gt;huggingface library&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Trusting RoBERTa over BERT: Insights from CheckListing the Natural Language Inference Task</title>
      <link>https://adityasomak.github.io/publication/checklist/</link>
      <pubDate>Thu, 22 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/checklist/</guid>
      <description>&lt;p&gt;The recent state-of-the-art natural language understanding (NLU) systems often behave unpredictably, failing on simpler reasoning examples. Despite this, there has been limited focus on quantifying progress towards systems with more predictable behavior. We think that reasoning capability-wise behavioral summary (proposed in Ribeiro et al. (2020)) is a step towards bridging this gap. We create a CHECKLIST test-suite (184K examples) for the Natural Language Inference (NLI) task, a representative NLU task. We benchmark state-of-the-art NLI systems on this test-suite, which reveals fine-grained insights into the reasoning abilities of BERT and RoBERTa. Our analysis further reveals inconsistencies of the models on examples derived from the same template or distinct templates but pertaining to same reasoning capability, indicating that generalizing the modelsâ€™ behavior through observations made on a CheckList is non-trivial. Through an user-study, we find that users were able to utilize behavioral information to generalize much better for examples predicted from RoBERTa, compared to that of BERT.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
