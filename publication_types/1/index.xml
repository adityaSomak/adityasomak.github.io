<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>1 | Somak Aditya</title>
    <link>https://adityasomak.github.io/publication_types/1/</link>
      <atom:link href="https://adityasomak.github.io/publication_types/1/index.xml" rel="self" type="application/rss+xml" />
    <description>1</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 18 Jul 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://adityasomak.github.io/img/icon-192.png</url>
      <title>1</title>
      <link>https://adityasomak.github.io/publication_types/1/</link>
    </image>
    
    <item>
      <title>Analyzing the Nuances of Transformers&#39; Polynomial Simplification Abilities</title>
      <link>https://adityasomak.github.io/publication/polysimp/</link>
      <pubDate>Sat, 18 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/polysimp/</guid>
      <description>&lt;p&gt;Symbolic Mathematical tasks such as integration often require multiple welldefined steps and understanding of sub-tasks to reach a solution. To understand Transformers’ abilities in such tasks in a fine-grained manner, we deviate from traditional end-to-end settings, and explore a step-wise polynomial simplification task. Polynomials can be written in a simple normal form as a sum of monomials which are ordered in a lexicographic order. For a polynomial which is not necessarily in this normal form, a sequence of simplification steps is applied to reach the fully simplified (i.e., in the normal form) polynomial. We propose a synthetic Polynomial dataset generation algorithm that generates polynomials with unique proof steps. Through varying coefficient configurations, input representation, proof granularity, and extensive hyper-parameter tuning, we observe that Transformers consistently struggle with numeric multiplication. We explore two ways to mitigate this: Curriculum Learning and a Symbolic Calculator approach (where the numeric operations are offloaded to a calculator). Both approaches provide significant gains over the vanilla Transformers-based baseline.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TaxiNLI: Taking a Ride up the NLU Hill</title>
      <link>https://adityasomak.github.io/publication/taxinli/</link>
      <pubDate>Sat, 18 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/taxinli/</guid>
      <description>&lt;p&gt;Pre-trained Transformer-based neural architectures have consistently achieved state-of-the-art performance in the Natural Language Inference (NLI) task. Since NLI examples encompass a variety of linguistic, logical, and reasoning phenomena, it remains unclear as to which specific concepts are learnt by the trained systems and where they can achieve strong generalization. To investigate this question, we propose a taxonomic hierarchy of categories that are relevant for the NLI task. We introduce TAXINLI, a new dataset, that has 10k examples from the MNLI dataset (Williams et al., 2018) with these taxonomic labels. Through various experiments on TAXINLI, we observe that whereas for certain taxonomic categories SOTA neural models have achieved near perfect accuracies - a large jump over the previous models - some categories still remain difficult. Our work adds to the growing body of literature that shows the gaps in the current NLI systems and datasets through a systematic presentation and analysis of reasoning categories.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Uncovering Relations for Marketing Knowledge Representation</title>
      <link>https://adityasomak.github.io/publication/makrstarai/</link>
      <pubDate>Fri, 07 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/makrstarai/</guid>
      <description>&lt;p&gt;Online behaviors of consumers and marketers generate massive marketing data, which ever more sophisticated models attempt to turn into insights and aid decisions by
marketers. Yet, in making decisions human managers bring to bear marketing knowledge which reside outside of data and models. Thus, it behooves creation of an automated
marketing knowledge base that can interact with data and models. Currently, marketing knowledge is dispersed in large corpora, but no definitive knowledge base for
marketing exists. Out of the two broad aspects of marketing knowledge - representation and reasoning - this treatise focuses on the former. Specifically,
we focus on creation of marketing knowledge graph from corpora, which requires identification of entities and relations. The relation identification task is
particularly challenging in marketing, because of the non-factoid nature of much marketing knowledge, and the difficulty of forming rules that govern relations.
Specifically, we define a set of relations to capture marketing knowledge, propose a pipeline for creating the knowledge graph from text and propose a
rule-guided semi-supervised relation prediction algorithm to extract relations between marketing entities from sentences.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Integrating Knowledge and Reasoning in Image Understanding</title>
      <link>https://adityasomak.github.io/publication/integratingsurvey/</link>
      <pubDate>Wed, 09 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/integratingsurvey/</guid>
      <description>&lt;p&gt;Deep learning based data-driven approaches have been successfully applied in various image under-
standing applications ranging from object recognition, semantic segmentation to visual question an-
swering.  However, the lack of knowledge integration as well as higher-level reasoning capabilities
with  the  methods  still  pose  a  hindrance. In  this work, we present a brief survey of a few represen-
tative  reasoning  mechanisms,  knowledge  integration methods and their corresponding image under-
standing applications developed by various groups of researchers, approaching the problem from a va-
riety of angles.  Furthermore, we discuss upon key efforts on integrating external knowledge with neu-
ral networks. Taking cues from these efforts, we conclude by discussing potential pathways to im-
prove reasoning capabilities.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Spatial Knowledge Distillation to aid Visual Reasoning</title>
      <link>https://adityasomak.github.io/publication/spatialkd/</link>
      <pubDate>Wed, 09 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/spatialkd/</guid>
      <description>&lt;p&gt;For tasks involving language and vision, the current state-of-the-art methods do not leverage any additional information
that might be present to gather privileged knowledge. Instead, such an ability is expected to be learnt during the training
phase. One such task is Visual Question Answering, where large diagnostic datasets have been proposed to
test a system’s capability of reasoning and answering questions about images. In
this work, we take a step towards integrating this additional privileged information in the form of spatial knowledge to
aid in visual reasoning. We propose a framework that combines recent advances in knowledge distillation (teacherstudent
framework), relational reasoning and probabilistic logical languages to incorporate such knowledge in existing
neural networks for the surrogate task of fact-based Visual Question Answering. Specifically, for a question posed
against an image, we use a probabilistic logical language to encode the spatial knowledge and the spatial understanding
about the question in the form of a mask that is directly provided to the teacher network. The student network learns
from the ground-truth information as well as the teacher’s prediction via distillation. We also demonstrate the impact
of predicting such a mask inside the teacher’s network using attention. Empirically, we show that both the methods
improve the test accuracy over a state-of-the-art approach on a publicly available dataset.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Explicit Reasoning over End-to-End Neural Architectures</title>
      <link>https://adityasomak.github.io/publication/pslvqa/</link>
      <pubDate>Sat, 17 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/pslvqa/</guid>
      <description>&lt;p&gt;Many vision and language tasks require commonsense reasoning beyond data-driven image and natural language pro-
cessing. Here we adopt Visual Question Answering (VQA) as an example task, where a system is expected to answer a
question in natural language about an image. Current state-of-the-art systems attempted to solve the task using deep neural
architectures and achieved promising performance. However, the resulting systems are generally opaque and they struggle
in understanding questions for which extra knowledge is required. In this paper, we present an explicit reasoning layer on
top of a set of penultimate neural network based systems. The reasoning layer enables reasoning and answering questions
where additional knowledge is required, and at the same time provides an interpretable interface to the end users. Specif-
ically, the reasoning layer adopts a Probabilistic Soft Logic (PSL) based engine to reason over a basket of inputs: visual
relations, semantic parse of the question, and background ontological knowledge from word2vec and ConceptNet.
Experimental analysis of the answers and the key evidential predicates generated on the VQA dataset validate our approach.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Combining Knowledge and Reasoning through Probabilistic Soft Logic for Image Puzzle Solving</title>
      <link>https://adityasomak.github.io/publication/riddles/</link>
      <pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/riddles/</guid>
      <description>&lt;p&gt;In this work, we explore a genre of puzzles (“image riddles”) which involves a set of images and a question.
Answering these puzzles require both capabilities involving visual detection (including object, activity recognition) and,
knowledge-based or commonsense reasoning. We compile a dataset of over 3k riddles where each riddle consists of 4 images and
a groundtruth answer. The annotations are validated using crowd-sourced evaluation. We also define an automatic evaluation
metric to track future progress. Our task bears similarity with the commonly known IQ tasks such as analogy solving,
sequence filling that are often used to test intelligence.&lt;/p&gt;

&lt;p&gt;We develop a Probabilistic Reasoning-based approach that utilizes probabilistic commonsense knowledge to answer these
riddles with a reasonable accuracy. We demonstrate the results of our approach using both automatic and human evaluations.
Our approach achieves some promising results ($12\%$ improvement over baseline) for these riddles and provides a strong baseline for future attempts. We make
the entire dataset and related materials publicly available to the community.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DeepIU: An Architecture for Image Understanding</title>
      <link>https://adityasomak.github.io/publication/deepiu/</link>
      <pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/deepiu/</guid>
      <description>&lt;p&gt;Image Understanding is fundamental to systems that need to extract contents and infer concepts
from images. In this paper, we develop an architecture for understanding images, through which a
system can recognize the content and the underlying concepts of an image and, reason and answer
questions about both using a visual module, a reasoning module, and a commonsense knowledge
base. In this architecture, visual data combines with background knowledge and; iterates through
visual and reasoning modules to answer questions about an image or to generate a textual description
of an image. We first provide motivations of such a Deep Image Understanding architecture
and then, we describe the necessary components it should include. We also introduce our own
preliminary implementation of this architecture and empirically show how this more generic implementation
compares with a recent end-to-end Neural approach on specific applications. We
address the knowledge-representation challenge in such an architecture by representing an image
using a directed labeled graph (called Scene Description Graph). Our implementation uses generic
visual recognition techniques and commonsense reasoning to extract such graphs from images.
Our experiments show that the extracted graphs capture the syntactic and semantic content of an
image with reasonable accuracy.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>From Images to Sentences through Scene Description Graphs using Commonsense Reasoning and Knowledge</title>
      <link>https://adityasomak.github.io/publication/sdg/</link>
      <pubDate>Sun, 01 Nov 2015 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/sdg/</guid>
      <description>&lt;p&gt;In this paper we propose the construction of linguistic
descriptions of images. This is achieved through the extraction
of scene description graphs (SDGs) from visual scenes
using an automatically constructed knowledge base. SDGs
are constructed using both vision and reasoning. Specifically,
commonsense reasoning1
is applied on (a) detections
obtained from existing perception methods on given
images, (b) a “commonsense” knowledge base constructed
using natural language processing of image annotations
and &amp;copy; lexical ontological knowledge from resources such
as WordNet. Amazon Mechanical Turk(AMT)-based evaluations
on Flickr8k, Flickr30k and MS-COCO datasets show
that in most cases, sentences auto-constructed from SDGs
obtained by our method give a more relevant and thorough
description of an image than a recent state-of-the-art image
caption based approach. Our Image-Sentence Alignment
Evaluation results are also comparable to that of the recent
state-of-the art approaches.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Recognizing social constructs from textual conversation</title>
      <link>https://adityasomak.github.io/publication/social/</link>
      <pubDate>Sat, 25 Jul 2015 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/social/</guid>
      <description>&lt;p&gt;In this paper we present our work on recognizing high level social constructs such as Leadership and Status from textual conversation using an approach that makes use of the background knowledge about social hierarchy and integrates statistical methods and symbolic logic based methods. We use a stratified approach in which we first detect lower level language constructs such as politeness, command and agreement that help us to infer intermediate constructs such as deference, closeness and authority that are observed between the parties engaged in conversation. These intermediate constructs in turn are used to determine the social constructs Leadership and Status. We have implemented this system successfully in both English and Korean languages and achieved considerable accuracy.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Towards Addressing the Winograd Schema Challenge-Building and Using a Semantic Parser and a Knowledge Hunting Module.</title>
      <link>https://adityasomak.github.io/publication/kparser/</link>
      <pubDate>Sat, 25 Jul 2015 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/kparser/</guid>
      <description>&lt;p&gt;Concerned about the Turing test’s ability to correctly
evaluate if a system exhibits human-like intelligence,
the Winograd Schema Challenge (WSC)
has been proposed as an alternative. A Winograd
Schema consists of a sentence and a question. The
answers to the questions are intuitive for humans
but are designed to be difficult for machines, as
they require various forms of commonsense knowledge
about the sentence. In this paper we demonstrate
our progress towards addressing the WSC.
We present an approach that identifies the knowledge
needed to answer a challenge question, hunts
down that knowledge from text repositories, and
then reasons with them to come up with the answer.
In the process we develop a semantic parser
(&lt;a href=&#34;www.kparser.org&#34; target=&#34;_blank&#34;&gt;www.kparser.org&lt;/a&gt;). We show that our approach
works well with respect to a subset of Winograd
schemas.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Identifying various kinds of event mentions in k-parser output</title>
      <link>https://adityasomak.github.io/publication/identifying/</link>
      <pubDate>Thu, 25 Jun 2015 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/identifying/</guid>
      <description>&lt;p&gt;In this paper we show how our semantic parser (Knowledge Parser or K-Parser) identifies various kinds of event mentions in the input text. The types include recursive (complex) and non recursive event mentions. K-Parser outputs each event mention in form of an acyclic graph with root nodes as the verbs that drive those events. The children nodes of the verbs represent the entities participating in the events, and their conceptual classes. The on-line demo of the system is available at &lt;a href=&#34;http://kparser.org&#34; target=&#34;_blank&#34;&gt;http://kparser.org&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Visual common-sense for scene understanding using perception, semantic parsing and reasoning.</title>
      <link>https://adityasomak.github.io/publication/common-sense/</link>
      <pubDate>Thu, 12 Mar 2015 00:00:00 +0000</pubDate>
      <guid>https://adityasomak.github.io/publication/common-sense/</guid>
      <description>&lt;p&gt;In this paper we explore the use of visual commonsense
knowledge and other kinds of knowledge (such as
domain knowledge, background knowledge, linguistic
knowledge) for scene understanding. In particular, we
combine visual processing with techniques from natural
language understanding (especially semantic parsing),
common-sense reasoning and knowledge representation
and reasoning to improve visual perception to reason
about finer aspects of activities&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
