[{"authors":["abhinavm"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"3d1e8668ec14f72535342422e463a623","permalink":"https://adityasomak.github.io/authors/abhinavm/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/abhinavm/","section":"authors","summary":"","tags":null,"title":"Abhinav Menon","type":"authors"},{"authors":["admin"],"categories":null,"content":"I am looking for motivated full-time Ph.D students who are interested in pursuing their thesis in the exciting juncture of classical and statistical AI (often dubbed as neuro-symbolic methods).  -- Latest News    1) NEW!!! (Oct 2022):  I am looking for a motivated IIT KGP BTech student (CS preferably), very proficient in coding to work with us in a multi-university collaboration on \"Spatio-Temporal Reasoning in Vision and Language\" with U.Utah, IIIT-H, and industrial partners.   2) I am looking for motivated full-time Ph.D students who are interested in pursuing their thesis in the exciting juncture of classical and statistical AI (often dubbed as neuro-symbolic methods).     !!! Major Update: I have joined IIT Kharagpur CSE as an Assistant Professor on November 2021. I am looking for motivated Ph.D, MTech, and BTech students. !!!  -- I am an Assistant Professor (Grade I) at IIT Kharagpur Department of CSE. I was a Postdoctoral Researcher at Microsoft Research India advised by Dr. Monojit Choudhury. Prior to joining MSR India, I spent 1.5 years in Adobe Research as a full-time Researcher. I completed by Ph.D from CIDSE, Arizona State University in June 2018, under the supervision of Prof. Chitta Baral and Prof. Yezhou Yang.\nDeployable AI systems should be able to reason with knowledge that is commonplace to humans. Thus, my research aims to enhance, evaluate, and explain different types of complex reasoning abilities of AI systems. At MSRI, I explored evaluation and enhancement of reasoning capabilities of Transformers-based language models. Apart from end-to-end reasoning, I am paralley exploring multi-hop reasoning capabilities of neural methods in both symbolic and natural language domains. During my Ph.D, I have explored enhancement of reasoning capabilities of image understanding systems. Through a combination of Deep Learning, Knowledge Representation, and Probabilistic Logical Reasoning, I demonstrated the benefits of using reasoning and knowledge in Visual Question-Answering, Captioning, image puzzle solving and visual reasoning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://adityasomak.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am looking for motivated full-time Ph.D students who are interested in pursuing their thesis in the exciting juncture of classical and statistical AI (often dubbed as neuro-symbolic methods).  -- Latest News    1) NEW!!! (Oct 2022):  I am looking for a motivated IIT KGP BTech student (CS preferably), very proficient in coding to work with us in a multi-university collaboration on \"Spatio-Temporal Reasoning in Vision and Language\"","tags":null,"title":"Somak Aditya","type":"authors"},{"authors":["atharva"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"f0156d990a33054f824dea947810629a","permalink":"https://adityasomak.github.io/authors/atharva/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/atharva/","section":"authors","summary":"","tags":null,"title":"Atharva Naik","type":"authors"},{"authors":["druhin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"5154fb3592b30a3937afc6d04af3c69c","permalink":"https://adityasomak.github.io/authors/druhin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/druhin/","section":"authors","summary":"","tags":null,"title":"Druhin Abrol","type":"authors"},{"authors":["sachin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"3cb75511e988725cd6e952804b41d6f7","permalink":"https://adityasomak.github.io/authors/sachin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/sachin/","section":"authors","summary":"","tags":null,"title":"Sachin Vashisth","type":"authors"},{"authors":["vivekk"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"bd61faf8edb613e3e777225393fb6838","permalink":"https://adityasomak.github.io/authors/vivekk/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/vivekk/","section":"authors","summary":"","tags":null,"title":"Vivek Karde","type":"authors"},{"authors":["Deepanway Ghoshal","Somak Aditya","Monojit Choudhury"],"categories":null,"content":"The Natural Language Inference (NLI) task often requires reasoning over multiple steps to reach the conclusion. While the necessity of generating such intermediate steps (instead of a summary explanation) has gained popular support, it is unclear how to generate such steps without complete end-to-end supervision and how such generated steps can be further utilized. In this work, we train a sequence-to-sequence model to generate only the next step given an NLI premise and hypothesis pair (and previous steps); then enhance it with external knowledge and symbolic search to generate intermediate steps with only next-step supervision. We show the correctness of such generated steps through automated and human verification. Furthermore, we show that such generated steps can help improve end-to-end NLI task performance using simple data augmentation strategies, across multiple public NLI datasets.\n","date":1661904000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661904000,"objectID":"b8f77ec3cd48a6240273112faac8e320","permalink":"https://adityasomak.github.io/publication/multihop/","publishdate":"2022-08-31T00:00:00Z","relpermalink":"/publication/multihop/","section":"publication","summary":"The Natural Language Inference (NLI) task often requires reasoning over multiple steps to reach the conclusion. While the necessity of generating such intermediate steps (instead of a summary explanation) has gained popular support, it is unclear how to generate such steps without complete end-to-end supervision and how such generated steps can be further utilized. In this work, we train a sequence-to-sequence model to generate only the next step given an NLI premise and hypothesis pair (and previous steps); then enhance it with external knowledge and symbolic search to generate intermediate steps with only next-step supervision.","tags":null,"title":"Generating Intermediate Steps for NLI with Next-Step Supervision","type":"publication"},{"authors":["Karthikeyan K","Shaily Bhatt","Pankaj Singh","Somak Aditya","Sandipan Dandapat","Sunayana Sitaram","Monojit Choudhury"],"categories":null,"content":"The recently proposed CheckList (Riberio et al,. 2020) approach to evaluation of NLP systems has revealed high failure rates for basic capabilities for multiple state-of-the-art and commercial models. However, the CheckList creation process is manual which creates a bottleneck towards creation of multilingual CheckLists catering 100s of languages. In this work, we explore multiple approaches to generate and evaluate the quality of Multilingual CheckList. We device an algorithm \u0026ndash; Automated Multilingual Checklist Generation (AMCG) for automatically transferring a CheckList from a source to a target language that relies on a reasonable machine translation system. We then compare the CheckList generated by AMCG with CheckLists generated with different levels of human intervention. Through in-depth crosslingual experiments between English and Hindi, and broad multilingual experiments spanning 11 languages, we show that the automatic approach can provide accurate estimates of failure rates of a model across capabilities, as would a human-verified CheckList, and better than CheckLists generated by humans from scratch.\n","date":1648080000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648080000,"objectID":"bd5b865899102cb0a00c11b2aa793181","permalink":"https://adityasomak.github.io/publication/amcg/","publishdate":"2022-03-24T00:00:00Z","relpermalink":"/publication/amcg/","section":"publication","summary":"The recently proposed CheckList (Riberio et al,. 2020) approach to evaluation of NLP systems has revealed high failure rates for basic capabilities for multiple state-of-the-art and commercial models. However, the CheckList creation process is manual which creates a bottleneck towards creation of multilingual CheckLists catering 100s of languages. In this work, we explore multiple approaches to generate and evaluate the quality of Multilingual CheckList. We device an algorithm \u0026ndash; Automated Multilingual Checklist Generation (AMCG) for automatically transferring a CheckList from a source to a target language that relies on a reasonable machine translation system.","tags":null,"title":"Multilingual CheckList: Generation and Evaluation","type":"publication"},{"authors":["Anirudh Srinivasan","Gauri Kholkar","Rahul Kejriwal","Tanuja Ganu","Sandipan Dandapat","Sunayana Sitaram","Balakrishnan Santhanam","Somak Aditya","Kalika Bali","Monojit Choudhury"],"categories":null,"content":"Pre-trained multilingual language models are gaining popularity due to their cross-lingual zero-shot transfer ability, but these models do not perform equally well in all languages. Evaluating task-specific performance of a model in a large number of languages is often a challenge due to lack of labeled data, as is targeting improvements in low performing languages through few-shot learning. We present a tool - LITMUS Predictor - that can make reliable performance projections for a fine-tuned task-specific model in a set of languages without test and training data, and help strategize data labeling efforts to optimize performance and fairness objectives.\n","date":1643846400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643846400,"objectID":"5e14c151ccda82d44735d9e6e1120d28","permalink":"https://adityasomak.github.io/publication/litmus/","publishdate":"2022-02-03T00:00:00Z","relpermalink":"/publication/litmus/","section":"publication","summary":"Pre-trained multilingual language models are gaining popularity due to their cross-lingual zero-shot transfer ability, but these models do not perform equally well in all languages. Evaluating task-specific performance of a model in a large number of languages is often a challenge due to lack of labeled data, as is targeting improvements in low performing languages through few-shot learning. We present a tool - LITMUS Predictor - that can make reliable performance projections for a fine-tuned task-specific model in a set of languages without test and training data, and help strategize data labeling efforts to optimize performance and fairness objectives.","tags":null,"title":"LITMUS Predictor: An AI Assistant for Building Reliable, High-Performing and Fair Multilingual NLP Systems","type":"publication"},{"authors":null,"categories":null,"content":" Instructor \u0026nbsp;\u0026nbsp;\u0026nbsp; Somak Aditya Timing \u0026nbsp;\u0026nbsp;\u0026nbsp; Slot A3: MON (08:00-08:55) , MON (09:00-09:55) , TUE(12:00-12:55) First Class \u0026nbsp;\u0026nbsp;\u0026nbsp; January 10th MON (08:00-08:55) , MON (09:00-09:55) Venue \u0026nbsp;\u0026nbsp;\u0026nbsp; Online Teams Channel Information-Retrieval-2022Spring, Key g7tgjqc Teaching Assistants \u0026nbsp;\u0026nbsp;\u0026nbsp; Abhilash Nandy  Ankan Mullick  Neeraj Saini  Ravi Pratap Singh  Vaibhav Saxena    Office Hours \nFriday - 18:10 - 19:10 (CSE-308) --  Announcements Every registered student should create an account on the Moodle system of CSE department. This system will be used for submission and grading of class tests and project. If you do not have an account already on the CSE department Moodle, create a new account for yourself following the procedure stated on the same webpage. Login to the system, and follow the link \"Autumn Semester (2021-22)\". Choose the course \"CS60092_2021-22 Information Retrieval\". Join this course as \"Student\"; use Student Enrolment Key: CSTU60092.  --  [Mar 28] Dr. Monojit Choudhury, Principal Data and Applied Scientist at Turing India (Microsoft) presented a Guest Lecture on \"Computing and Representing the Meanings of Words: From Wittgenstein to GPT-3 and beyond\". The recording is available here (Google Drive 309.4 MB MPEG4).  [Mar 17] Class Test 2 will be on Mar 3rd week. Syllabus will include whatever is covered after Vector Space and Scoring.   [Jan 27] Class Test 1 will be on Feb 3rd week. Syllabus will include whatever is covered upto Feb 1st week. Please register in the CSE Moodle. Enrollment key is shared on Teams   [Jan 27] Project Choice submission deadline:  Jan 28th 11:59 PM IST .   Note: for students, who are not able to register, kindly wait till 7th when the window closes. I will not be able to answer your emails individually. I will first approve based on CGPA and other criteria (such as total class strength cap etc.), once the window closes.   Every registered student should join the Google mailing list ir-2022-spring@googlegroups.com. All urgent announcements would be made through the group. This group is meant only for registered (and approved) students. Kindly mention your roll number and the fact that you have registered in ERP.  IR 2011-22 Spring will be a fully online research-oriented course.   First class on January 10 (Monday), at 8:00 am (deferred from 4th due to ERP registration issues). Join the class Information-Retrieval-2022Spring on MS Teams (IITKGP domain; Code: g7tgjqc).   The course requires an understanding of the foundation of algorithms and data structures, probability and statistics, and knowledge of the basics of Natural Language Processing, and Machine Learning. This will be a research-oriented course that would require students to understand several CS research papers. There will be a term project that needs to be done using Python/Java. It is advisable to take this course only if you have the necessary background.   \n\nTentative Weightage: Three online proctored tests (60%), Term Project (40 %). \n Pre-requisites for the course    Data structures and algorithms   Probability and Statistics   Basics of Machine Learning   Basics of Natural Language Processing (Some might be covered during the course)   Basics of Graph algorithms (Some might be covered during the course)   Programming in Python/Java   \nLecture Slides   Boolean retrieval - PDF  The term vocabulary \u0026amp; postings lists - PDF  Skip Pointers, Phrase Queries and Positional Indexing - PDF  Scoring, term weighting \u0026amp; the vector space model - PDF  Dictionaries and Tolerant Retrieval - PDF  Evaluation in information retrieval - PDF  Index Construction and Compression - PDF (Part 1) PDF (Part 2)  Relevance feedback \u0026amp; query expansion - PDF  Probabilistic information retrieval - PDF  Language models for information retrieval - PDF  Link analysis \u0026ndash; HITS, PageRank  Word Vectors  Summarization  Learning to Rank  Neural IR \n Text and Reference Literature    Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schütze. Introduction to information retrieval, Cambridge: Cambridge university press, 2008.  Research Papers shared in class.  \nEvery test should be attempted individually by each student. Plagiarism in any form will be severely penalized.\n","date":1639094400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639094400,"objectID":"f8a87beb2a977952436fb28c66fb78d6","permalink":"https://adityasomak.github.io/courses/irspring22/","publishdate":"2021-12-10T00:00:00Z","relpermalink":"/courses/irspring22/","section":"courses","summary":"Course Page for CS60092","tags":null,"title":"CS60092 Information Retrieval (Spring 2022)","type":"courses"},{"authors":null,"categories":null,"content":" Teaching  Teaching Assistant Duties:\n CSE-576 Natural Language Processing, Fall 2015 and Fall 2016  Responsibilities included: Creating Homework assignments, proposing class projects, and mentoring multiple groups, delivering advanced topic lectures.  CSE-471 Introduction To Artificial Intelligence, Spring 2016 CSE-310 Data Structures and Algorithms, Spring 2015   Mentoring (Post Ph.D)   (2021) Ishan Tarunesh: CheckList NLI (2021) Karthikeyan K: Multi-lingual extension of TaxiNLI (TaxiXNLI; Primary Mentor: Monojit Choudhury; at Duke U.) (2021) Vishesh Agarwal: Polynomial Simplification (Primary Mentor: Navin Goyal) (2020-21) Aalok Sathe: TaxiNLI and TaxiXNLI (at MIT BCS) (2020) Pratik Joshi: TaxiNLI: Taking a ride up the NLU Hill. (at CMU LTI) (2019) Pranil Joshi (IIT-B), Abhinav Mishra (IIT-G), Bhavy Khatri (IIT-K): Knowledge-sharing between Cross-domain Agents  Mentored with Kushal Chawla (USC), Sharmila Nangi Reddy (Feb 2020) Patent filed USPTO. Status pending.    Mentoring (During Ph.D)   Rudra Saha (2017 \u0026ndash; 2018), Graduated, MS Student, Computer Science, ASU, Currently Researcher@Verisk  Spatial Knowledge Distillation to aid Visual Reasoning Thesis: Multimodal Representation Learning for Visual Reasoning and Text-to-Image Translation  Divyanshu Bandil(2016 \u0026ndash; 2017), Graduated MS Student, Computer Engineering.  Visual Question Categorization. Visual Question Answering in Dynamic Environments  Trideep Rath (2015 \u0026ndash; 2017), Graduated, MS, Computer Science, ASU. Currently Data Scientist, Kayak  Thesis: Word and Relation Embedding for Sentence Representation   ","date":1639094400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639094400,"objectID":"90ea878364bda47a24c6ca1c4b711923","permalink":"https://adityasomak.github.io/courses/teachingexp/","publishdate":"2021-12-10T00:00:00Z","relpermalink":"/courses/teachingexp/","section":"courses","summary":"PhD and Post-PhD","tags":null,"title":"Teaching/Mentoring Experience","type":"courses"},{"authors":["Karthikeyan K","Aalok Sathe","Somak Aditya","Monojit Choudhury"],"categories":null,"content":"Multilingual language models achieve impressive zero-shot accuracies in many languages in complex tasks such as Natural Language Inference (NLI). Examples in NLI (and equivalent complex tasks) often pertain to various types of sub-tasks, requiring different kinds of reasoning. Certain types of reasoning have proven to be more difficult to learn in a monolingual context, and in the crosslingual context, similar observations may shed light on zero-shot transfer efficiency and few-shot sample selection. Hence, to investigate the effects of types of reasoning on transfer performance, we propose a category-annotated multilingual NLI dataset. We discuss the challenges to scale monolingual annotations to multiple languages. We statistically observe interesting effects that the confluence of reasoning types and language similarities have on transfer performance.\n","date":1632268800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632268800,"objectID":"98efac203f83e63bff09172ad2b92aba","permalink":"https://adityasomak.github.io/publication/crosslingual/","publishdate":"2021-09-22T00:00:00Z","relpermalink":"/publication/crosslingual/","section":"publication","summary":"Multilingual language models achieve impressive zero-shot accuracies in many languages in complex tasks such as Natural Language Inference (NLI). Examples in NLI (and equivalent complex tasks) often pertain to various types of sub-tasks, requiring different kinds of reasoning. Certain types of reasoning have proven to be more difficult to learn in a monolingual context, and in the crosslingual context, similar observations may shed light on zero-shot transfer efficiency and few-shot sample selection.","tags":null,"title":"Analyzing the Effects of Reasoning Types on Cross-Lingual Transfer Performance","type":"publication"},{"authors":["Somak Aditya","Sharmila Nangi Reddy","Pranil Joshi","Kushal Chawla","Bhavy Khatri","Abhinav Mishra"],"categories":null,"content":"Systems and methods for natural language processing (NLP) are described. The systems may be trained by identifying training data including clean data and noisy data; predicting annotation information using an artificial neural network (ANN); computing a loss value for the annotation information using a weighted loss function that applies a first weight to the clean data and at least one second weight to the noisy data; and updating the ANN based on the loss value. The noisy data may be obtained by identifying a set of unannotated sentences in a target domain, delexicalizing the set of unannotated sentences, finding similar sentences in a source domain, filling at least one arbitrary value in the similar delexicalized sentences, generating annotation information for the similar delexicalized sentences using an annotation model for the source domain, and applying a heuristic mapping to produce annotation information for the sentences in the target domain\n","date":1629936000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629936000,"objectID":"6d8edff74b53ec26203e37835e52c3c4","permalink":"https://adityasomak.github.io/publication/intentslotpatent/","publishdate":"2021-08-26T00:00:00Z","relpermalink":"/publication/intentslotpatent/","section":"publication","summary":"Systems and methods for natural language processing (NLP) are described. The systems may be trained by identifying training data including clean data and noisy data; predicting annotation information using an artificial neural network (ANN); computing a loss value for the annotation information using a weighted loss function that applies a first weight to the clean data and at least one second weight to the noisy data; and updating the ANN based on the loss value.","tags":null,"title":"Predicting joint intent-slot structure","type":"publication"},{"authors":["Ishan Tarunesh","Somak Aditya","Monojit Choudhury"],"categories":null,"content":"The recent state-of-the-art natural language understanding (NLU) systems often behave un\u0002predictably, failing on simpler reasoning examples. Despite this, there has been limited focus on quantifying progress towards systems with more predictable behavior. We think that reasoning capability-wise behavioral summary (proposed in Ribeiro et al. (2020)) is a step towards bridging this gap. We create a CHECKLIST test-suite (184K examples) for the Natural Language Inference (NLI) task, a representative NLU task. We benchmark state-of-the-art NLI systems on this test-suite, which reveals fine-grained insights into the reasoning abilities of BERT and RoBERTa. Our analysis further reveals inconsistencies of the models on examples derived from the same template or distinct templates but pertaining to same reasoning capability, indicating that generalizing the models’ behavior through observations made on a CheckList is non-trivial. Through an user-study, we find that users were able to utilize behavioral information to generalize much better for examples predicted from RoBERTa, compared to that of BERT.\n","date":1626912000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626912000,"objectID":"2e078eab07ae9fdebd5f073717f58fa0","permalink":"https://adityasomak.github.io/publication/checklist/","publishdate":"2021-07-22T00:00:00Z","relpermalink":"/publication/checklist/","section":"publication","summary":"The recent state-of-the-art natural language understanding (NLU) systems often behave un\u0002predictably, failing on simpler reasoning examples. Despite this, there has been limited focus on quantifying progress towards systems with more predictable behavior. We think that reasoning capability-wise behavioral summary (proposed in Ribeiro et al. (2020)) is a step towards bridging this gap. We create a CHECKLIST test-suite (184K examples) for the Natural Language Inference (NLI) task, a representative NLU task. We benchmark state-of-the-art NLI systems on this test-suite, which reveals fine-grained insights into the reasoning abilities of BERT and RoBERTa.","tags":null,"title":"Trusting RoBERTa over BERT: Insights from CheckListing the Natural Language Inference Task","type":"publication"},{"authors":["Vishesh Agarwal","Somak Aditya","Navin Goyal"],"categories":null,"content":"Symbolic Mathematical tasks such as integration often require multiple well\u0002defined steps and understanding of sub-tasks to reach a solution. To understand Transformers’ abilities in such tasks in a fine-grained manner, we deviate from traditional end-to-end settings, and explore a step-wise polynomial simplification task. Polynomials can be written in a simple normal form as a sum of monomials which are ordered in a lexicographic order. For a polynomial which is not neces\u0002sarily in this normal form, a sequence of simplification steps is applied to reach the fully simplified (i.e., in the normal form) polynomial. We propose a syn\u0002thetic Polynomial dataset generation algorithm that generates polynomials with unique proof steps. Through varying coefficient configurations, input represen\u0002tation, proof granularity, and extensive hyper-parameter tuning, we observe that Transformers consistently struggle with numeric multiplication. We explore two ways to mitigate this: Curriculum Learning and a Symbolic Calculator approach (where the numeric operations are offloaded to a calculator). Both approaches provide significant gains over the vanilla Transformers-based baseline.\n","date":1620345600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620345600,"objectID":"fc53ab9894e232763d1ca48678524cb1","permalink":"https://adityasomak.github.io/publication/polysimp/","publishdate":"2021-05-07T00:00:00Z","relpermalink":"/publication/polysimp/","section":"publication","summary":"Symbolic Mathematical tasks such as integration often require multiple well\u0002defined steps and understanding of sub-tasks to reach a solution. To understand Transformers’ abilities in such tasks in a fine-grained manner, we deviate from traditional end-to-end settings, and explore a step-wise polynomial simplification task. Polynomials can be written in a simple normal form as a sum of monomials which are ordered in a lexicographic order. For a polynomial which is not neces\u0002sarily in this normal form, a sequence of simplification steps is applied to reach the fully simplified (i.","tags":null,"title":"Analyzing the Nuances of Transformers' Polynomial Simplification Abilities","type":"publication"},{"authors":["Somak Aditya","Atanu Sinha"],"categories":null,"content":"In some embodiments, a knowledge graph generation system extracts noun-phrases from sentences of a knowledge corpora and determines the relations between the noun-phrases based on a relation classifier that is configured to predict a relation between a pair of entities without restricting the entities to a set of named entities. The knowledge graph generation system further generates a sub-graph for each of the sentences based on the noun-phrases and the determined relations. Nodes or entities of the sub-graph represent the non-phrases in the sentence and edges represent the relations between the noun-phrases connected by the respective edges. The knowledge graph generation system merges the sub-graphs to generate the knowledge graph for the knowledge corpora.\n","date":1619049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619049600,"objectID":"b9cce6adf8b30a73dd1f91ae1dd87dbe","permalink":"https://adityasomak.github.io/publication/makrstaraipatent/","publishdate":"2021-04-22T00:00:00Z","relpermalink":"/publication/makrstaraipatent/","section":"publication","summary":"In some embodiments, a knowledge graph generation system extracts noun-phrases from sentences of a knowledge corpora and determines the relations between the noun-phrases based on a relation classifier that is configured to predict a relation between a pair of entities without restricting the entities to a set of named entities. The knowledge graph generation system further generates a sub-graph for each of the sentences based on the noun-phrases and the determined relations.","tags":null,"title":"Creating a knowledge graph based on text-based knowledge corpora","type":"publication"},{"authors":["Pratik Joshi*","Somak Aditya*","Aalok Sathe*","Monojit Choudhury"],"categories":null,"content":"Pre-trained Transformer-based neural architectures have consistently achieved state-of-the-art performance in the Natural Language Inference (NLI) task. Since NLI examples encompass a variety of linguistic, logical, and reasoning phenomena, it remains unclear as to which specific concepts are learnt by the trained systems and where they can achieve strong generalization. To investigate this question, we propose a taxonomic hierarchy of categories that are relevant for the NLI task. We introduce TAXINLI, a new dataset, that has 10k examples from the MNLI dataset (Williams et al., 2018) with these taxonomic labels. Through various experiments on TAXINLI, we observe that whereas for certain taxonomic categories SOTA neural models have achieved near perfect accuracies - a large jump over the previous models - some categories still remain difficult. Our work adds to the growing body of literature that shows the gaps in the current NLI systems and datasets through a systematic presentation and analysis of reasoning categories.\n","date":1595030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595030400,"objectID":"4a0bfab0747f0ea44b61a570720b5056","permalink":"https://adityasomak.github.io/publication/taxinli/","publishdate":"2020-07-18T00:00:00Z","relpermalink":"/publication/taxinli/","section":"publication","summary":"Pre-trained Transformer-based neural architectures have consistently achieved state-of-the-art performance in the Natural Language Inference (NLI) task. Since NLI examples encompass a variety of linguistic, logical, and reasoning phenomena, it remains unclear as to which specific concepts are learnt by the trained systems and where they can achieve strong generalization. To investigate this question, we propose a taxonomic hierarchy of categories that are relevant for the NLI task. We introduce TAXINLI, a new dataset, that has 10k examples from the MNLI dataset (Williams et al.","tags":null,"title":"TaxiNLI: Taking a Ride up the NLU Hill","type":"publication"},{"authors":null,"categories":null,"content":"In a recent work (PolySimp ICLR 2021 MathAI Workshop) with Navin Goyal and Vishesh Agarwal, we explored Transformers\u0026rsquo; abilities to perform multiple-step reasoning in well-defined purely symbolic tasks such as step-wise polynomial simplification. Polynomials can be written in a simple normal form as a sum of monomials which are ordered in a lexicographic order. For a polynomial which is not neces\u0002sarily in this normal form, a sequence of simplification steps is applied to reach the fully simplified (i.e., in the normal form) polynomial. We propose a syn\u0002thetic Polynomial dataset generation algorithm that generates polynomials with unique proof steps.\nThrough varying coefficient configurations, input represen\u0002tation, proof granularity, and extensive hyper-parameter tuning, we observe that Transformers consistently struggle with numeric multiplication. We explore two ways to mitigate this: Curriculum Learning and a Symbolic Calculator approach (where the numeric operations are offloaded to a calculator). Both approaches provide significant gains over the vanilla Transformers-based baseline.\nReferences     Vishesh Agarwal, Somak Aditya, Navin Goyal. Analyzing the Nuances of Transformers' Polynomial Simplification Abilities. ICLR 2021 MathAI Workshop.   -- ","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"4ed5db40f1853361133ca6d7f2cf5c7f","permalink":"https://adityasomak.github.io/project/symbolicmath/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/project/symbolicmath/","section":"project","summary":"We explore multi-hop reasoning capabilities of Transformer/GNN methods in purely symbolic domains.","tags":["symbolic","ATP","transformers","logic","deep-learning"],"title":"Symbolic Mathematics","type":"project"},{"authors":["Somak Aditya","Atanu Sinha"],"categories":null,"content":"Online behaviors of consumers and marketers generate massive marketing data, which ever more sophisticated models attempt to turn into insights and aid decisions by marketers. Yet, in making decisions human managers bring to bear marketing knowledge which reside outside of data and models. Thus, it behooves creation of an automated marketing knowledge base that can interact with data and models. Currently, marketing knowledge is dispersed in large corpora, but no definitive knowledge base for marketing exists. Out of the two broad aspects of marketing knowledge - representation and reasoning - this treatise focuses on the former. Specifically, we focus on creation of marketing knowledge graph from corpora, which requires identification of entities and relations. The relation identification task is particularly challenging in marketing, because of the non-factoid nature of much marketing knowledge, and the difficulty of forming rules that govern relations. Specifically, we define a set of relations to capture marketing knowledge, propose a pipeline for creating the knowledge graph from text and propose a rule-guided semi-supervised relation prediction algorithm to extract relations between marketing entities from sentences.\n","date":1581033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581033600,"objectID":"477d7685e49cb6ff22a6dd106963e163","permalink":"https://adityasomak.github.io/publication/makrstarai/","publishdate":"2020-02-07T00:00:00Z","relpermalink":"/publication/makrstarai/","section":"publication","summary":"Online behaviors of consumers and marketers generate massive marketing data, which ever more sophisticated models attempt to turn into insights and aid decisions by marketers. Yet, in making decisions human managers bring to bear marketing knowledge which reside outside of data and models. Thus, it behooves creation of an automated marketing knowledge base that can interact with data and models. Currently, marketing knowledge is dispersed in large corpora, but no definitive knowledge base for marketing exists.","tags":null,"title":"Uncovering Relations for Marketing Knowledge Representation","type":"publication"},{"authors":["Somak Aditya","Yezhou Yang","Chitta Baral"],"categories":null,"content":"Deep learning based data-driven approaches have been successfully applied in various image under- standing applications ranging from object recognition, semantic segmentation to visual question an- swering. However, the lack of knowledge integration as well as higher-level reasoning capabilities with the methods still pose a hindrance. In this work, we present a brief survey of a few represen- tative reasoning mechanisms, knowledge integration methods and their corresponding image under- standing applications developed by various groups of researchers, approaching the problem from a va- riety of angles. Furthermore, we discuss upon key efforts on integrating external knowledge with neu- ral networks. Taking cues from these efforts, we conclude by discussing potential pathways to im- prove reasoning capabilities.\n","date":1546992000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546992000,"objectID":"9cb6945d9bbd24b4247da26d91ba1ebb","permalink":"https://adityasomak.github.io/publication/integratingsurvey/","publishdate":"2019-01-09T00:00:00Z","relpermalink":"/publication/integratingsurvey/","section":"publication","summary":"Deep learning based data-driven approaches have been successfully applied in various image under- standing applications ranging from object recognition, semantic segmentation to visual question an- swering. However, the lack of knowledge integration as well as higher-level reasoning capabilities with the methods still pose a hindrance. In this work, we present a brief survey of a few represen- tative reasoning mechanisms, knowledge integration methods and their corresponding image under- standing applications developed by various groups of researchers, approaching the problem from a va- riety of angles.","tags":null,"title":"Integrating Knowledge and Reasoning in Image Understanding","type":"publication"},{"authors":["Somak Aditya"],"categories":null,"content":"Image Understanding is a long-established discipline in computer vision, which encompasses a body of advanced image processing techniques, that are used to locate (“where”), characterize and recognize (“what”) objects, regions, and their attributes in the image. However, the notion of “understanding” (and the goal of artificial intelligent machines) goes beyond factual recall of the recognized components and includes reasoning and thinking beyond what can be seen (or perceived). Understanding is often evaluated by asking questions of increasing difficulty. Thus, the expected functionalities of an intelligent Image Understanding system can be expressed in terms of the functionalities that are required to answer questions about an image. Answering questions about images require primarily three components: Image Understanding, question (natural language) understanding, and reasoning based on knowledge. Any question, asking beyond what can be directly seen, requires modeling of commonsense (or background/ontological/factual) knowledge and reasoning.\nKnowledge and reasoning have seen scarce use in image understanding applications. In this thesis, we demonstrate the utilities of incorporating background knowledge and using explicit reasoning in image understanding applications. We first present a comprehensive survey of the previous work that utilized background knowledge and reasoning in understanding images. This survey outlines the limited use of commonsense knowledge in high-level applications. We then present a set of vision and reasoning-based methods to solve several applications and show that these approaches benefit in terms of accuracy and interpretability from the explicit use of knowledge and reasoning. We propose novel knowledge representations of image, knowledge acquisition methods, and a new implementation of an efficient probabilistic logical reasoning engine that can utilize publicly available commonsense knowledge to solve applications such as visual question answering, image puzzles. Additionally, we identify the need for new datasets that explicitly require external commonsense knowledge to solve. We propose the new task of Image Riddles, which requires a combination of vision, and reasoning based on ontological knowledge; and we collect a sufficiently large dataset to serve as an ideal testbed for vision and reasoning research. Lastly, we propose end-to-end deep architectures that can combine vision, knowledge and reasoning modules together and achieve large performance boosts over state-of-the-art methods\n","date":1546992000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546992000,"objectID":"0f40cf545adf79432be77d9037b7794b","permalink":"https://adityasomak.github.io/publication/thesis/","publishdate":"2019-01-09T00:00:00Z","relpermalink":"/publication/thesis/","section":"publication","summary":"Image Understanding is a long-established discipline in computer vision, which encompasses a body of advanced image processing techniques, that are used to locate (“where”), characterize and recognize (“what”) objects, regions, and their attributes in the image. However, the notion of “understanding” (and the goal of artificial intelligent machines) goes beyond factual recall of the recognized components and includes reasoning and thinking beyond what can be seen (or perceived). Understanding is often evaluated by asking questions of increasing difficulty.","tags":null,"title":"Knowledge and Reasoning for Image Understanding","type":"publication"},{"authors":["Somak Aditya","Rudra Saha","Yezhou Yang","Chitta Baral"],"categories":null,"content":"For tasks involving language and vision, the current state-of-the-art methods do not leverage any additional information that might be present to gather privileged knowledge. Instead, such an ability is expected to be learnt during the training phase. One such task is Visual Question Answering, where large diagnostic datasets have been proposed to test a system’s capability of reasoning and answering questions about images. In this work, we take a step towards integrating this additional privileged information in the form of spatial knowledge to aid in visual reasoning. We propose a framework that combines recent advances in knowledge distillation (teacherstudent framework), relational reasoning and probabilistic logical languages to incorporate such knowledge in existing neural networks for the surrogate task of fact-based Visual Question Answering. Specifically, for a question posed against an image, we use a probabilistic logical language to encode the spatial knowledge and the spatial understanding about the question in the form of a mask that is directly provided to the teacher network. The student network learns from the ground-truth information as well as the teacher’s prediction via distillation. We also demonstrate the impact of predicting such a mask inside the teacher’s network using attention. Empirically, we show that both the methods improve the test accuracy over a state-of-the-art approach on a publicly available dataset.\n","date":1546992000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546992000,"objectID":"e99e9a74490c91a23c0ddff4821cc879","permalink":"https://adityasomak.github.io/publication/spatialkd/","publishdate":"2019-01-09T00:00:00Z","relpermalink":"/publication/spatialkd/","section":"publication","summary":"For tasks involving language and vision, the current state-of-the-art methods do not leverage any additional information that might be present to gather privileged knowledge. Instead, such an ability is expected to be learnt during the training phase. One such task is Visual Question Answering, where large diagnostic datasets have been proposed to test a system’s capability of reasoning and answering questions about images. In this work, we take a step towards integrating this additional privileged information in the form of spatial knowledge to aid in visual reasoning.","tags":null,"title":"Spatial Knowledge Distillation to aid Visual Reasoning","type":"publication"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://adityasomak.github.io/people/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"Hello!","tags":null,"title":"Tr^2AIL","type":"widget_page"},{"authors":["Somak Aditya","Yezhou Yang","Chitta Baral"],"categories":null,"content":"Many vision and language tasks require commonsense reasoning beyond data-driven image and natural language pro- cessing. Here we adopt Visual Question Answering (VQA) as an example task, where a system is expected to answer a question in natural language about an image. Current state-of-the-art systems attempted to solve the task using deep neural architectures and achieved promising performance. However, the resulting systems are generally opaque and they struggle in understanding questions for which extra knowledge is required. In this paper, we present an explicit reasoning layer on top of a set of penultimate neural network based systems. The reasoning layer enables reasoning and answering questions where additional knowledge is required, and at the same time provides an interpretable interface to the end users. Specif- ically, the reasoning layer adopts a Probabilistic Soft Logic (PSL) based engine to reason over a basket of inputs: visual relations, semantic parse of the question, and background ontological knowledge from word2vec and ConceptNet. Experimental analysis of the answers and the key evidential predicates generated on the VQA dataset validate our approach.\n","date":1542412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542412800,"objectID":"5d4da9816cf40f8d97f4d3237b8ac3f6","permalink":"https://adityasomak.github.io/publication/pslvqa/","publishdate":"2018-11-17T00:00:00Z","relpermalink":"/publication/pslvqa/","section":"publication","summary":"Many vision and language tasks require commonsense reasoning beyond data-driven image and natural language pro- cessing. Here we adopt Visual Question Answering (VQA) as an example task, where a system is expected to answer a question in natural language about an image. Current state-of-the-art systems attempted to solve the task using deep neural architectures and achieved promising performance. However, the resulting systems are generally opaque and they struggle in understanding questions for which extra knowledge is required.","tags":null,"title":"Explicit Reasoning over End-to-End Neural Architectures","type":"publication"},{"authors":["Somak Aditya","Yezhou Yang","Chitta Baral","Yiannis Aloimonos"],"categories":null,"content":"In this work, we explore a genre of puzzles (“image riddles”) which involves a set of images and a question. Answering these puzzles require both capabilities involving visual detection (including object, activity recognition) and, knowledge-based or commonsense reasoning. We compile a dataset of over 3k riddles where each riddle consists of 4 images and a groundtruth answer. The annotations are validated using crowd-sourced evaluation. We also define an automatic evaluation metric to track future progress. Our task bears similarity with the commonly known IQ tasks such as analogy solving, sequence filling that are often used to test intelligence.\nWe develop a Probabilistic Reasoning-based approach that utilizes probabilistic commonsense knowledge to answer these riddles with a reasonable accuracy. We demonstrate the results of our approach using both automatic and human evaluations. Our approach achieves some promising results ($12\\%$ improvement over baseline) for these riddles and provides a strong baseline for future attempts. We make the entire dataset and related materials publicly available to the community.\n","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"ea75c382abb3162648644deb6f8f19f3","permalink":"https://adityasomak.github.io/publication/riddles/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"/publication/riddles/","section":"publication","summary":"In this work, we explore a genre of puzzles (“image riddles”) which involves a set of images and a question. Answering these puzzles require both capabilities involving visual detection (including object, activity recognition) and, knowledge-based or commonsense reasoning. We compile a dataset of over 3k riddles where each riddle consists of 4 images and a groundtruth answer. The annotations are validated using crowd-sourced evaluation. We also define an automatic evaluation metric to track future progress.","tags":null,"title":"Combining Knowledge and Reasoning through Probabilistic Soft Logic for Image Puzzle Solving","type":"publication"},{"authors":["Somak Aditya","Yezhou Yang","Chitta Baral","Cornelia Fermuller","Yiannis Aloimonos"],"categories":null,"content":"Two of the fundamental tasks in image understanding using text are caption generation and visual question answering [4, 72]. This work presents an intermediate knowledge structure that can be used for both tasks to obtain increased interpretability. We call this knowledge structure Scene Description Graph (SDG), as it is a directed labeled graph, representing objects, actions, regions, as well as their attributes, along with inferred concepts and semantic (from KM-Ontology [12]), ontological (i.e. superclass, hasProperty), and spatial relations. Thereby a general architecture is proposed in which a system can represent both the content and underlying concepts of an image using an SDG. The architecture is implemented using generic visual recognition techniques and commonsense reasoning to extract graphs from images. The utility of the generated SDGs is demonstrated in the applications of image captioning, image retrieval, and through examples in visual question answering. The experiments in this work show that the extracted graphs capture syntactic and semantic content of images with reasonable accuracy.\n","date":1513555200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513555200,"objectID":"453cfb1c1944cf52392c98f63cb46b26","permalink":"https://adityasomak.github.io/publication/sdg_cviu/","publishdate":"2017-12-18T00:00:00Z","relpermalink":"/publication/sdg_cviu/","section":"publication","summary":"Two of the fundamental tasks in image understanding using text are caption generation and visual question answering [4, 72]. This work presents an intermediate knowledge structure that can be used for both tasks to obtain increased interpretability. We call this knowledge structure Scene Description Graph (SDG), as it is a directed labeled graph, representing objects, actions, regions, as well as their attributes, along with inferred concepts and semantic (from KM-Ontology [12]), ontological (i.","tags":null,"title":"Image Understanding using Vision and Reasoning through Scene Description Graph","type":"publication"},{"authors":["Somak Aditya","Yezhou Yang","Chitta Baral","Cornelia Fermuller","Yiannis Aloimonos"],"categories":null,"content":"Image Understanding is fundamental to systems that need to extract contents and infer concepts from images. In this paper, we develop an architecture for understanding images, through which a system can recognize the content and the underlying concepts of an image and, reason and answer questions about both using a visual module, a reasoning module, and a commonsense knowledge base. In this architecture, visual data combines with background knowledge and; iterates through visual and reasoning modules to answer questions about an image or to generate a textual description of an image. We first provide motivations of such a Deep Image Understanding architecture and then, we describe the necessary components it should include. We also introduce our own preliminary implementation of this architecture and empirically show how this more generic implementation compares with a recent end-to-end Neural approach on specific applications. We address the knowledge-representation challenge in such an architecture by representing an image using a directed labeled graph (called Scene Description Graph). Our implementation uses generic visual recognition techniques and commonsense reasoning to extract such graphs from images. Our experiments show that the extracted graphs capture the syntactic and semantic content of an image with reasonable accuracy.\n","date":1464739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464739200,"objectID":"742033ae75cf97b18a93037d51976d13","permalink":"https://adityasomak.github.io/publication/deepiu/","publishdate":"2016-06-01T00:00:00Z","relpermalink":"/publication/deepiu/","section":"publication","summary":"Image Understanding is fundamental to systems that need to extract contents and infer concepts from images. In this paper, we develop an architecture for understanding images, through which a system can recognize the content and the underlying concepts of an image and, reason and answer questions about both using a visual module, a reasoning module, and a commonsense knowledge base. In this architecture, visual data combines with background knowledge and; iterates through visual and reasoning modules to answer questions about an image or to generate a textual description of an image.","tags":null,"title":"DeepIU: An Architecture for Image Understanding","type":"publication"},{"authors":null,"categories":null,"content":"Models that claim to understand language, should also be able to demonstrate its abilities to reason across various dimensions. My present goal is to evaluate, enhance and explain the reasoning capabilities of such systems (or language models).\n Natural Language Inference  Large pre-trained language models show high performance in popular NLP benchmarks (GLUE, SuperGLUE), while failing poorly in datasets with targeted linguistic and logical phenomena. We consolidate the interesting reasoning phenomena in Taxonomy of reasoning w.r.t the NLI task. Our first work along this line published in CoNLL 2020 showed that these models (BERT, RoBERTa) may not know how to perform certain types of reasoning such as causal, numeric, spatial, temporal; but they can identify the type of reasoning required for a new example.\nWe did a follow-up, adapting the CheckList methodology, where we create a large CheckList-NLI dataset to individually yet collectively test different reasoning capabilities, including pragmatic ones. Through our test-suite, we show that such a post-hoc evaluation provides a more comprehensive overview of the behavioral nature of the language models. A thorough human study with Linguistic Olympiad participants shows that behavioral summary leads to better explanation and RoBERTa\u0026rsquo;s behavior is more predictable than BERT. Currently, we are also exploring augmenting NLI datasets with verifiable proofs.\nSummary and Extensions:   TaxiNLI: Taxonomic Fragmentation of the NLI Task, CoNLL 2020   TaxiXNLI: Multi-lingual Extension of TaxiNLI, EMNLP 2021 MRL Workshop   LoNLI: Testing Diverse Reasoning of NLI Systems, Under review in LRE \n\n  Enhancing NLI: Multi-hop, Causality and Counterfactuals  As observed through TaxiNLI family of work, language models struggle with many important reasoning types. With Deepanway Ghoshal and Monojit choudhury, we explored a less annotation-intensive way to generate intermediate steps for complex reasoning examples in free-form NLI datasets. We observe, not only, we can generate such multi-hop steps without end-to-end supervision; but the steps are accurate as they can be augmented directly to improve NLI model's predictive ability.  References   Generating Intermediate Steps for NLI with Next-Step Supervision, ArXiv 2022, Under review  \nPreviously I have been interested in mapping natural language to formal language representation and reasoning with it. My proposed solutions towards Question-Answering and Winograd Schema Challenge during my Ph.D have been motivated by the central idea of semantic parsing, followed by logical (or probabilistic logical) reasoning.\n Semantic Parsing (K-Parser)  We (led by co-authors Arpit Sharma and Nguyen Vo) have explored mapping of natural language to formal representation, that enbales logical reasoning. Through several papers (K-Parser IJCAI-15, K-Parser NAACL 15), we showed how such semantic parsing enables us to find event mentions, and (even patially but interpretably) solved Winograd Schema challenge problems.\nReferences     Ishan Tarunesh, Somak Aditya, Monojit Choudhury. Trusting RoBERTa over BERT: Insights from CheckListing the Natural Language Inference Task. Arxiv 2015.  Pratik Joshi*, Somak Aditya*, Aalok Sathe*, Monojit Choudhury. TaxiNLI: Taking a Ride up the NLU Hill. CoNLL 2020.  Arpit Sharma, Somak Aditya, Vo Nguyen and Chitta Baral. Towards Addressing the Winograd Schema Challenge - Building and Using a Semantic Parser and a Knowledge Hunting Module. IJCAI 2015.  Somak Aditya, Chitta Baral, Nguyen Ha Vo, Joohyung Lee, Jieping Ye, Zaw Naung, Barry Lumpkin, Jenny Hastings, Richard Scherl, Dawn M. Sweet, Daniela Inclezan. Recognizing Social Constructs from Textual Conversation. HLT-NAACL 2015.  Arpit Sharma, Nguyen H. Vo, Somak Aditya and Chitta Baral. Identifying Various Kinds of Event Mentions in K-Parser Output The 3rd Workshop on EVENTS: Definition, Detection, Coreference, and Representation. HLT-NAACL 2015.   -- ","date":1456272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456272000,"objectID":"62fd26629cf36dbf7e5ac29b6fdbb0a0","permalink":"https://adityasomak.github.io/project/nlp/","publishdate":"2016-02-24T00:00:00Z","relpermalink":"/project/nlp/","section":"project","summary":"We investigate how to evaluate, explain and enhance neural models under the lens of reasoning.","tags":["deep-learning","Logic","Language","common-sense","Semantic Parsing","Knowledge Acqusition"],"title":"Reasoning in NLP","type":"project"},{"authors":null,"categories":null,"content":"Many vision and language tasks require commonsense reasoning beyond data-driven image and natural language processing. Cognitive Sciences and Active Vision literature points to an explicit iterative interaction among perception, reasoning, and memeory (knowledge) modules (DeepIU ACS 2015).\n Captioning  In our earliest attempt (CVIU 2017 ), we used a combination of image classification, reasoning with commonsense knowledge (extracted from training captions) to propose a Scene Description Graph as an intermediate representation for a natural image. We showed the efficacy of this representation through image captioning, image retrieval tasks (and QA case studies.)\n Visual QA, Image Puzzles and Visual Reasoning  We have proposed instantiations of this abstract architecture to solve image puzzles, VQA and visual reasoning tasks such as CLEVR. In our AAAI 2018 VQA, and UAI 2018 Puzzles work, we have proposed an explicit probabilistic soft logic layer on top of a neural architecture that helps integrate commonsense knowledge and induces post-hoc interpretability.\nLater on, for an end-to-end (differentiable) integration of spatial knowledge, we explore a combination of knowledge distillation, probabilistic logic, and relational network in our WACV 2019 CLEVR.\nReferences     Somak Aditya, Yezhou Yang, Chitta Baral. Integrating Knowledge and Reasoning in Image Understanding. IJCAI 2019.   Somak Aditya, Rudra Saha, Yezhou Yang and Chitta Baral. Spatial Knowledge Distillation to aid Visual Reasoning. WACV 2019.   Somak Aditya, Yezhou Yang, Chitta Baral. Explicit Reasoning over End-to-End Neural Architectures for Visual Question Answering. AAAI 2018.   Somak Aditya, Yezhou Yang, Chitta Baral and Yiannis Aloimonos. Combining Knowledge and Reasoning through Probabilistic Soft Logic for Image Puzzle Solving. UAI 2018.   Somak Aditya, Yezhou Yang, Chitta Baral, Yiannis Aloimonos and Cornelia Fermuller. Image Understanding using Vision and Reasoning through Scene Description Graph. Computer Vision and Image Understanding Journal. (Accepted December 2017)   Somak Aditya, Yezhou Yang, Chitta Baral and Yiannis Aloimonos. Answering Image Riddles using Vision and Reasoning through Probabilistic Soft Logic.. Arxiv version. 2016. Website with additional information on this work.   Somak Aditya, Chitta Baral, Yezhou Yang, Yiannnis Aloimonos and Cornelia Fermuller. DeepIU: An architecture for image understanding. Advances in Cognitive Systems. 2016.   Somak Aditya, Yezhou Yang, Chitta Baral, Cornelia Fermuller, Yiannis Aloimonos. From Images to Sentences through Scene Description Graphs using Commonsense Reasoning and Knowledge. Arxiv version.   Somak Aditya, Yiannis Aloimonos, Chitta Baral, Cornelia Fermuller and Yezhou Yang. Visual common-sense for scene understanding using perception, semantic parsing and reasoning. Common-sense 2015, AAAI 2015 Spring Symposium. (Appenidix with code.)   -- ","date":1446940800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1446940800,"objectID":"8be9c72c0f6eaa839cefd70503777aa2","permalink":"https://adityasomak.github.io/project/vision/","publishdate":"2015-11-08T00:00:00Z","relpermalink":"/project/vision/","section":"project","summary":"In this project, we explore how reasoning with knowledge enhance image understanding.","tags":["deep-learning","soft-logic","Vision and Language","Vision","Language"],"title":"Vision and Reasoning","type":"project"},{"authors":["Somak Aditya","Yezhou Yang","Chitta Baral","Cornelia Fermuller","Yiannis Aloimonos"],"categories":null,"content":"In this paper we propose the construction of linguistic descriptions of images. This is achieved through the extraction of scene description graphs (SDGs) from visual scenes using an automatically constructed knowledge base. SDGs are constructed using both vision and reasoning. Specifically, commonsense reasoning1 is applied on (a) detections obtained from existing perception methods on given images, (b) a “commonsense” knowledge base constructed using natural language processing of image annotations and \u0026copy; lexical ontological knowledge from resources such as WordNet. Amazon Mechanical Turk(AMT)-based evaluations on Flickr8k, Flickr30k and MS-COCO datasets show that in most cases, sentences auto-constructed from SDGs obtained by our method give a more relevant and thorough description of an image than a recent state-of-the-art image caption based approach. Our Image-Sentence Alignment Evaluation results are also comparable to that of the recent state-of-the art approaches.\n","date":1446336000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1446336000,"objectID":"aa6a548aa04668d349bf10d0c6c97f3e","permalink":"https://adityasomak.github.io/publication/sdg/","publishdate":"2015-11-01T00:00:00Z","relpermalink":"/publication/sdg/","section":"publication","summary":"In this paper we propose the construction of linguistic descriptions of images. This is achieved through the extraction of scene description graphs (SDGs) from visual scenes using an automatically constructed knowledge base. SDGs are constructed using both vision and reasoning. Specifically, commonsense reasoning1 is applied on (a) detections obtained from existing perception methods on given images, (b) a “commonsense” knowledge base constructed using natural language processing of image annotations and \u0026copy; lexical ontological knowledge from resources such as WordNet.","tags":null,"title":"From Images to Sentences through Scene Description Graphs using Commonsense Reasoning and Knowledge","type":"publication"},{"authors":["Somak Aditya","Chitta Baral","Nguyen Vo","Joohyung Lee","Jieping Ye","Zaw Naung","Barry Lumpkin","Jenny Hastings","Richard Scherl","Dawn M Sweet","Daniela Inclezan"],"categories":null,"content":"In this paper we present our work on recognizing high level social constructs such as Leadership and Status from textual conversation using an approach that makes use of the background knowledge about social hierarchy and integrates statistical methods and symbolic logic based methods. We use a stratified approach in which we first detect lower level language constructs such as politeness, command and agreement that help us to infer intermediate constructs such as deference, closeness and authority that are observed between the parties engaged in conversation. These intermediate constructs in turn are used to determine the social constructs Leadership and Status. We have implemented this system successfully in both English and Korean languages and achieved considerable accuracy.\n","date":1437782400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1437782400,"objectID":"f5d9f4a4b4ab5cd777a0be60d4ed5c13","permalink":"https://adityasomak.github.io/publication/social/","publishdate":"2015-07-25T00:00:00Z","relpermalink":"/publication/social/","section":"publication","summary":"In this paper we present our work on recognizing high level social constructs such as Leadership and Status from textual conversation using an approach that makes use of the background knowledge about social hierarchy and integrates statistical methods and symbolic logic based methods. We use a stratified approach in which we first detect lower level language constructs such as politeness, command and agreement that help us to infer intermediate constructs such as deference, closeness and authority that are observed between the parties engaged in conversation.","tags":null,"title":"Recognizing social constructs from textual conversation","type":"publication"},{"authors":["Arpit Sharma","Nguyen Ha Vo","Somak Aditya","Chitta Baral"],"categories":null,"content":"Concerned about the Turing test’s ability to correctly evaluate if a system exhibits human-like intelligence, the Winograd Schema Challenge (WSC) has been proposed as an alternative. A Winograd Schema consists of a sentence and a question. The answers to the questions are intuitive for humans but are designed to be difficult for machines, as they require various forms of commonsense knowledge about the sentence. In this paper we demonstrate our progress towards addressing the WSC. We present an approach that identifies the knowledge needed to answer a challenge question, hunts down that knowledge from text repositories, and then reasons with them to come up with the answer. In the process we develop a semantic parser (www.kparser.org). We show that our approach works well with respect to a subset of Winograd schemas.\n","date":1437782400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1437782400,"objectID":"cd516e6a02ca02a76a5b8b8e8ba4c618","permalink":"https://adityasomak.github.io/publication/kparser/","publishdate":"2015-07-25T00:00:00Z","relpermalink":"/publication/kparser/","section":"publication","summary":"Concerned about the Turing test’s ability to correctly evaluate if a system exhibits human-like intelligence, the Winograd Schema Challenge (WSC) has been proposed as an alternative. A Winograd Schema consists of a sentence and a question. The answers to the questions are intuitive for humans but are designed to be difficult for machines, as they require various forms of commonsense knowledge about the sentence. In this paper we demonstrate our progress towards addressing the WSC.","tags":null,"title":"Towards Addressing the Winograd Schema Challenge-Building and Using a Semantic Parser and a Knowledge Hunting Module.","type":"publication"},{"authors":["Arpit Sharma","Nguyen Ha Vo","Somak Aditya","Chitta Baral"],"categories":null,"content":"In this paper we show how our semantic parser (Knowledge Parser or K-Parser) identifies various kinds of event mentions in the input text. The types include recursive (complex) and non recursive event mentions. K-Parser outputs each event mention in form of an acyclic graph with root nodes as the verbs that drive those events. The children nodes of the verbs represent the entities participating in the events, and their conceptual classes. The on-line demo of the system is available at http://kparser.org\n","date":1435190400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435190400,"objectID":"f0dadba6ae27240132176399a4bf2eee","permalink":"https://adityasomak.github.io/publication/identifying/","publishdate":"2015-06-25T00:00:00Z","relpermalink":"/publication/identifying/","section":"publication","summary":"In this paper we show how our semantic parser (Knowledge Parser or K-Parser) identifies various kinds of event mentions in the input text. The types include recursive (complex) and non recursive event mentions. K-Parser outputs each event mention in form of an acyclic graph with root nodes as the verbs that drive those events. The children nodes of the verbs represent the entities participating in the events, and their conceptual classes.","tags":null,"title":"Identifying various kinds of event mentions in k-parser output","type":"publication"},{"authors":["Somak Aditya","Yezhou Yang","Chitta Baral","Cornelia Fermuller","Yiannis Aloimonos"],"categories":null,"content":"In this paper we explore the use of visual commonsense knowledge and other kinds of knowledge (such as domain knowledge, background knowledge, linguistic knowledge) for scene understanding. In particular, we combine visual processing with techniques from natural language understanding (especially semantic parsing), common-sense reasoning and knowledge representation and reasoning to improve visual perception to reason about finer aspects of activities\n","date":1426118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1426118400,"objectID":"333b12bb8c3c55efbbcc1918d700c7d9","permalink":"https://adityasomak.github.io/publication/common-sense/","publishdate":"2015-03-12T00:00:00Z","relpermalink":"/publication/common-sense/","section":"publication","summary":"In this paper we explore the use of visual commonsense knowledge and other kinds of knowledge (such as domain knowledge, background knowledge, linguistic knowledge) for scene understanding. In particular, we combine visual processing with techniques from natural language understanding (especially semantic parsing), common-sense reasoning and knowledge representation and reasoning to improve visual perception to reason about finer aspects of activities","tags":null,"title":"Visual common-sense for scene understanding using perception, semantic parsing and reasoning.","type":"publication"}]