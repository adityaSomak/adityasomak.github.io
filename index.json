[{"authors":["admin"],"categories":null,"content":"I have joined Microsoft Research India as a Postdoctoral Researcher. At MSRI, with Dr. Monojit Choudhury, I am developing models with logical reasoning capabilities required for natural language understanding (NLU). Specifically, we ask: what reasoning capabilities are required for tasks such as Natural Language Inferencing? Are neural models capable of performing such reasoning? Are there possible neuro-symbolic variants that may fill this gap? Parallel to exploring neuro-symbolic methods under the lens of NLI, I am also exploring multi-hop reasoning capabilities of neural methods in symbolic domains with Dr. Navin Goyal.\nOverall, my central goal in research has been devloping models that are both data-driven, yet enriched by knowledge. Developing such models require three central considerations: knowledge acqusition, representation and reasoning. While at MSRI, I am primarily focusing on reasoning capabilities, my former efforts had a higher concentration on the latter two. I completed my Ph.D. in Computer Science from CIDSE, Arizona State University on June 27, 2018. For my doctoral studies, I worked on Image Understanding using a combination of Deep Learning, Commonsense Reasoning and Knowledge Representation Techniques under the supervision of Prof. Chitta Baral (Past President of KR.inc, 2016-2018). I was also co-advised by Dr. Yezhou Yang (Assistant Professor, ASU). My current efforts have been published in AAAI, IJCAI, UAI, NAACL, CVIU (journal), WACV.\nI completed my Masters (ME) in Computer Science from the Indian Institute of Science, Bangalore in 2011 with a concentration in Machine Learning under the supervision of Prof. M Narasimha Murty (Dean, Faculty of Engineering, IISc). I have done my Bachelors in CS from Jadavpur University in 2009. After a brief stint at Yahoo, I joined Strand Life Sciences where I worked in the NGS team for nearly 2.5 years before joining ASU to pursue PhD.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://adityasomak.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I have joined Microsoft Research India as a Postdoctoral Researcher. At MSRI, with Dr. Monojit Choudhury, I am developing models with logical reasoning capabilities required for natural language understanding (NLU). Specifically, we ask: what reasoning capabilities are required for tasks such as Natural Language Inferencing? Are neural models capable of performing such reasoning? Are there possible neuro-symbolic variants that may fill this gap? Parallel to exploring neuro-symbolic methods under the lens of NLI, I am also exploring multi-hop reasoning capabilities of neural methods in symbolic domains with Dr.","tags":null,"title":"Somak Aditya","type":"authors"},{"authors":["Somak Aditya","Atanu Sinha"],"categories":null,"content":"Online behaviors of consumers and marketers generate massive marketing data, which ever more sophisticated models attempt to turn into insights and aid decisions by marketers. Yet, in making decisions human managers bring to bear marketing knowledge which reside outside of data and models. Thus, it behooves creation of an automated marketing knowledge base that can interact with data and models. Currently, marketing knowledge is dispersed in large corpora, but no definitive knowledge base for marketing exists. Out of the two broad aspects of marketing knowledge - representation and reasoning - this treatise focuses on the former. Specifically, we focus on creation of marketing knowledge graph from corpora, which requires identification of entities and relations. The relation identification task is particularly challenging in marketing, because of the non-factoid nature of much marketing knowledge, and the difficulty of forming rules that govern relations. Specifically, we define a set of relations to capture marketing knowledge, propose a pipeline for creating the knowledge graph from text and propose a rule-guided semi-supervised relation prediction algorithm to extract relations between marketing entities from sentences.\n","date":1581033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581033600,"objectID":"f765dd7736d8ab3cc1e3294e40f861f8","permalink":"https://adityasomak.github.io/publication/makrstarai/","publishdate":"2020-02-07T00:00:00Z","relpermalink":"/publication/makrstarai/","section":"publication","summary":"Online behaviors of consumers and marketers generate massive marketing data, which ever more sophisticated models attempt to turn into insights and aid decisions by marketers. Yet, in making decisions human managers bring to bear marketing knowledge which reside outside of data and models. Thus, it behooves creation of an automated marketing knowledge base that can interact with data and models. Currently, marketing knowledge is dispersed in large corpora, but no definitive knowledge base for marketing exists.","tags":null,"title":"Uncovering Relations for Marketing Knowledge Representation","type":"publication"},{"authors":["Somak Aditya","Yezhou Yang","Chitta Baral"],"categories":null,"content":"Deep learning based data-driven approaches have been successfully applied in various image under- standing applications ranging from object recognition, semantic segmentation to visual question an- swering. However, the lack of knowledge integration as well as higher-level reasoning capabilities with the methods still pose a hindrance. In this work, we present a brief survey of a few represen- tative reasoning mechanisms, knowledge integration methods and their corresponding image under- standing applications developed by various groups of researchers, approaching the problem from a va- riety of angles. Furthermore, we discuss upon key efforts on integrating external knowledge with neu- ral networks. Taking cues from these efforts, we conclude by discussing potential pathways to im- prove reasoning capabilities.\n","date":1546992000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546992000,"objectID":"40dd08d4c3119b2e3ede17066f053ddc","permalink":"https://adityasomak.github.io/publication/integratingsurvey/","publishdate":"2019-01-09T00:00:00Z","relpermalink":"/publication/integratingsurvey/","section":"publication","summary":"Deep learning based data-driven approaches have been successfully applied in various image under- standing applications ranging from object recognition, semantic segmentation to visual question an- swering. However, the lack of knowledge integration as well as higher-level reasoning capabilities with the methods still pose a hindrance. In this work, we present a brief survey of a few represen- tative reasoning mechanisms, knowledge integration methods and their corresponding image under- standing applications developed by various groups of researchers, approaching the problem from a va- riety of angles.","tags":null,"title":"Integrating Knowledge and Reasoning in Image Understanding","type":"publication"},{"authors":["Somak Aditya","Rudra Saha","Yezhou Yang","Chitta Baral"],"categories":null,"content":"For tasks involving language and vision, the current state-of-the-art methods do not leverage any additional information that might be present to gather privileged knowledge. Instead, such an ability is expected to be learnt during the training phase. One such task is Visual Question Answering, where large diagnostic datasets have been proposed to test a system’s capability of reasoning and answering questions about images. In this work, we take a step towards integrating this additional privileged information in the form of spatial knowledge to aid in visual reasoning. We propose a framework that combines recent advances in knowledge distillation (teacherstudent framework), relational reasoning and probabilistic logical languages to incorporate such knowledge in existing neural networks for the surrogate task of fact-based Visual Question Answering. Specifically, for a question posed against an image, we use a probabilistic logical language to encode the spatial knowledge and the spatial understanding about the question in the form of a mask that is directly provided to the teacher network. The student network learns from the ground-truth information as well as the teacher’s prediction via distillation. We also demonstrate the impact of predicting such a mask inside the teacher’s network using attention. Empirically, we show that both the methods improve the test accuracy over a state-of-the-art approach on a publicly available dataset.\n","date":1546992000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546992000,"objectID":"7caca6826b9ef22a3e176f19e1601712","permalink":"https://adityasomak.github.io/publication/spatialkd/","publishdate":"2019-01-09T00:00:00Z","relpermalink":"/publication/spatialkd/","section":"publication","summary":"For tasks involving language and vision, the current state-of-the-art methods do not leverage any additional information that might be present to gather privileged knowledge. Instead, such an ability is expected to be learnt during the training phase. One such task is Visual Question Answering, where large diagnostic datasets have been proposed to test a system’s capability of reasoning and answering questions about images. In this work, we take a step towards integrating this additional privileged information in the form of spatial knowledge to aid in visual reasoning.","tags":null,"title":"Spatial Knowledge Distillation to aid Visual Reasoning","type":"publication"},{"authors":["Somak Aditya","Yezhou Yang","Chitta Baral"],"categories":null,"content":"Many vision and language tasks require commonsense reasoning beyond data-driven image and natural language pro- cessing. Here we adopt Visual Question Answering (VQA) as an example task, where a system is expected to answer a question in natural language about an image. Current state-of-the-art systems attempted to solve the task using deep neural architectures and achieved promising performance. However, the resulting systems are generally opaque and they struggle in understanding questions for which extra knowledge is required. In this paper, we present an explicit reasoning layer on top of a set of penultimate neural network based systems. The reasoning layer enables reasoning and answering questions where additional knowledge is required, and at the same time provides an interpretable interface to the end users. Specif- ically, the reasoning layer adopts a Probabilistic Soft Logic (PSL) based engine to reason over a basket of inputs: visual relations, semantic parse of the question, and background ontological knowledge from word2vec and ConceptNet. Experimental analysis of the answers and the key evidential predicates generated on the VQA dataset validate our approach.\n","date":1542412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542412800,"objectID":"904366aa222c46088dbaa205cee1610c","permalink":"https://adityasomak.github.io/publication/pslvqa/","publishdate":"2018-11-17T00:00:00Z","relpermalink":"/publication/pslvqa/","section":"publication","summary":"Many vision and language tasks require commonsense reasoning beyond data-driven image and natural language pro- cessing. Here we adopt Visual Question Answering (VQA) as an example task, where a system is expected to answer a question in natural language about an image. Current state-of-the-art systems attempted to solve the task using deep neural architectures and achieved promising performance. However, the resulting systems are generally opaque and they struggle in understanding questions for which extra knowledge is required.","tags":null,"title":"Explicit Reasoning over End-to-End Neural Architectures","type":"publication"},{"authors":null,"categories":null,"content":"","date":1541721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541721600,"objectID":"b7105284fbc07ad49d708bc74a925e59","permalink":"https://adityasomak.github.io/project/spatialkd/","publishdate":"2018-11-09T00:00:00Z","relpermalink":"/project/spatialkd/","section":"project","summary":"In this work, we propose a way to integrate spatial commonsense knowledge to aid visual reasoning `external_link`.","tags":["deep-learning","soft-logic","Vision and Language"],"title":"Spatial Knowledge Distillation to aid Visual Reasoning","type":"project"},{"authors":null,"categories":null,"content":"","date":1541635200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541635200,"objectID":"f43f9f8ac169da41e99c53d0338e1b8d","permalink":"https://adityasomak.github.io/project/pslvqa/","publishdate":"2018-11-08T00:00:00Z","relpermalink":"/project/pslvqa/","section":"project","summary":"In this work, we propose an exlicit reasoning layer over Deep Neural Architectures to solve VQA. `external_link`.","tags":["deep-learning","soft-logic","Vision and Language"],"title":"Solving Visual QA using Reasoning and Deep Learning","type":"project"},{"authors":["Somak Aditya","Yezhou Yang","Chitta Baral","Yiannis Aloimonos"],"categories":null,"content":"In this work, we explore a genre of puzzles (“image riddles”) which involves a set of images and a question. Answering these puzzles require both capabilities involving visual detection (including object, activity recognition) and, knowledge-based or commonsense reasoning. We compile a dataset of over 3k riddles where each riddle consists of 4 images and a groundtruth answer. The annotations are validated using crowd-sourced evaluation. We also define an automatic evaluation metric to track future progress. Our task bears similarity with the commonly known IQ tasks such as analogy solving, sequence filling that are often used to test intelligence.\nWe develop a Probabilistic Reasoning-based approach that utilizes probabilistic commonsense knowledge to answer these riddles with a reasonable accuracy. We demonstrate the results of our approach using both automatic and human evaluations. Our approach achieves some promising results ($12\\%$ improvement over baseline) for these riddles and provides a strong baseline for future attempts. We make the entire dataset and related materials publicly available to the community.\n","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"ea75c382abb3162648644deb6f8f19f3","permalink":"https://adityasomak.github.io/publication/riddles/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"/publication/riddles/","section":"publication","summary":"In this work, we explore a genre of puzzles (“image riddles”) which involves a set of images and a question. Answering these puzzles require both capabilities involving visual detection (including object, activity recognition) and, knowledge-based or commonsense reasoning. We compile a dataset of over 3k riddles where each riddle consists of 4 images and a groundtruth answer. The annotations are validated using crowd-sourced evaluation. We also define an automatic evaluation metric to track future progress.","tags":null,"title":"Combining Knowledge and Reasoning through Probabilistic Soft Logic for Image Puzzle Solving","type":"publication"},{"authors":["Somak Aditya","Yezhou Yang","Chitta Baral","Cornelia Fermuller","Yiannis Aloimonos"],"categories":null,"content":"Two of the fundamental tasks in image understanding using text are caption generation and visual question answering [4, 72]. This work presents an intermediate knowledge structure that can be used for both tasks to obtain increased interpretability. We call this knowledge structure Scene Description Graph (SDG), as it is a directed labeled graph, representing objects, actions, regions, as well as their attributes, along with inferred concepts and semantic (from KM-Ontology [12]), ontological (i.e. superclass, hasProperty), and spatial relations. Thereby a general architecture is proposed in which a system can represent both the content and underlying concepts of an image using an SDG. The architecture is implemented using generic visual recognition techniques and commonsense reasoning to extract graphs from images. The utility of the generated SDGs is demonstrated in the applications of image captioning, image retrieval, and through examples in visual question answering. The experiments in this work show that the extracted graphs capture syntactic and semantic content of images with reasonable accuracy.\n","date":1513555200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513555200,"objectID":"453cfb1c1944cf52392c98f63cb46b26","permalink":"https://adityasomak.github.io/publication/sdg_cviu/","publishdate":"2017-12-18T00:00:00Z","relpermalink":"/publication/sdg_cviu/","section":"publication","summary":"Two of the fundamental tasks in image understanding using text are caption generation and visual question answering [4, 72]. This work presents an intermediate knowledge structure that can be used for both tasks to obtain increased interpretability. We call this knowledge structure Scene Description Graph (SDG), as it is a directed labeled graph, representing objects, actions, regions, as well as their attributes, along with inferred concepts and semantic (from KM-Ontology [12]), ontological (i.","tags":null,"title":"Image Understanding using Vision and Reasoning through Scene Description Graph","type":"publication"},{"authors":null,"categories":null,"content":"","date":1493251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493251200,"objectID":"58e61e799271aee6ab6f2ed23bbcc7f0","permalink":"https://adityasomak.github.io/project/riddle-project/","publishdate":"2017-04-27T00:00:00Z","relpermalink":"/project/riddle-project/","section":"project","summary":"In this work, we propose image puzzles that require background knowledge to solve. This project is also aimed to advocates logically interpretable systems. `external_link`.","tags":["deep-learning","soft-logic","Vision and Language"],"title":"Solving Image Puzzles","type":"project"},{"authors":null,"categories":null,"content":"","date":1480204800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480204800,"objectID":"553a94c5dfd3b8b099d8a12b2d248093","permalink":"https://adityasomak.github.io/project/example-external-project/","publishdate":"2016-11-27T00:00:00Z","relpermalink":"/project/example-external-project/","section":"project","summary":"In this project, we aim to achieve a deeper understanding of images and videos with the help of background or common-sense knowledge. Our motivation is the way a human interprets images and videos, breaks them down in few important constituent parts, can reason about them, detect complex activities etc. `external_link`.","tags":["common-sense","Vision and Language","Knowledge Acqusition"],"title":"From Images to Sentences","type":"project"},{"authors":["Somak Aditya","Yezhou Yang","Chitta Baral","Cornelia Fermuller","Yiannis Aloimonos"],"categories":null,"content":"Image Understanding is fundamental to systems that need to extract contents and infer concepts from images. In this paper, we develop an architecture for understanding images, through which a system can recognize the content and the underlying concepts of an image and, reason and answer questions about both using a visual module, a reasoning module, and a commonsense knowledge base. In this architecture, visual data combines with background knowledge and; iterates through visual and reasoning modules to answer questions about an image or to generate a textual description of an image. We first provide motivations of such a Deep Image Understanding architecture and then, we describe the necessary components it should include. We also introduce our own preliminary implementation of this architecture and empirically show how this more generic implementation compares with a recent end-to-end Neural approach on specific applications. We address the knowledge-representation challenge in such an architecture by representing an image using a directed labeled graph (called Scene Description Graph). Our implementation uses generic visual recognition techniques and commonsense reasoning to extract such graphs from images. Our experiments show that the extracted graphs capture the syntactic and semantic content of an image with reasonable accuracy.\n","date":1464739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464739200,"objectID":"742033ae75cf97b18a93037d51976d13","permalink":"https://adityasomak.github.io/publication/deepiu/","publishdate":"2016-06-01T00:00:00Z","relpermalink":"/publication/deepiu/","section":"publication","summary":"Image Understanding is fundamental to systems that need to extract contents and infer concepts from images. In this paper, we develop an architecture for understanding images, through which a system can recognize the content and the underlying concepts of an image and, reason and answer questions about both using a visual module, a reasoning module, and a commonsense knowledge base. In this architecture, visual data combines with background knowledge and; iterates through visual and reasoning modules to answer questions about an image or to generate a textual description of an image.","tags":null,"title":"DeepIU: An Architecture for Image Understanding","type":"publication"},{"authors":["Somak Aditya","Yezhou Yang","Chitta Baral","Cornelia Fermuller","Yiannis Aloimonos"],"categories":null,"content":"In this paper we propose the construction of linguistic descriptions of images. This is achieved through the extraction of scene description graphs (SDGs) from visual scenes using an automatically constructed knowledge base. SDGs are constructed using both vision and reasoning. Specifically, commonsense reasoning1 is applied on (a) detections obtained from existing perception methods on given images, (b) a “commonsense” knowledge base constructed using natural language processing of image annotations and \u0026copy; lexical ontological knowledge from resources such as WordNet. Amazon Mechanical Turk(AMT)-based evaluations on Flickr8k, Flickr30k and MS-COCO datasets show that in most cases, sentences auto-constructed from SDGs obtained by our method give a more relevant and thorough description of an image than a recent state-of-the-art image caption based approach. Our Image-Sentence Alignment Evaluation results are also comparable to that of the recent state-of-the art approaches.\n","date":1446336000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1446336000,"objectID":"aa6a548aa04668d349bf10d0c6c97f3e","permalink":"https://adityasomak.github.io/publication/sdg/","publishdate":"2015-11-01T00:00:00Z","relpermalink":"/publication/sdg/","section":"publication","summary":"In this paper we propose the construction of linguistic descriptions of images. This is achieved through the extraction of scene description graphs (SDGs) from visual scenes using an automatically constructed knowledge base. SDGs are constructed using both vision and reasoning. Specifically, commonsense reasoning1 is applied on (a) detections obtained from existing perception methods on given images, (b) a “commonsense” knowledge base constructed using natural language processing of image annotations and \u0026copy; lexical ontological knowledge from resources such as WordNet.","tags":null,"title":"From Images to Sentences through Scene Description Graphs using Commonsense Reasoning and Knowledge","type":"publication"},{"authors":["Arpit Sharma","Nguyen Ha Vo","Somak Aditya","Chitta Baral"],"categories":null,"content":"Concerned about the Turing test’s ability to correctly evaluate if a system exhibits human-like intelligence, the Winograd Schema Challenge (WSC) has been proposed as an alternative. A Winograd Schema consists of a sentence and a question. The answers to the questions are intuitive for humans but are designed to be difficult for machines, as they require various forms of commonsense knowledge about the sentence. In this paper we demonstrate our progress towards addressing the WSC. We present an approach that identifies the knowledge needed to answer a challenge question, hunts down that knowledge from text repositories, and then reasons with them to come up with the answer. In the process we develop a semantic parser (www.kparser.org). We show that our approach works well with respect to a subset of Winograd schemas.\n","date":1437782400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1437782400,"objectID":"cd516e6a02ca02a76a5b8b8e8ba4c618","permalink":"https://adityasomak.github.io/publication/kparser/","publishdate":"2015-07-25T00:00:00Z","relpermalink":"/publication/kparser/","section":"publication","summary":"Concerned about the Turing test’s ability to correctly evaluate if a system exhibits human-like intelligence, the Winograd Schema Challenge (WSC) has been proposed as an alternative. A Winograd Schema consists of a sentence and a question. The answers to the questions are intuitive for humans but are designed to be difficult for machines, as they require various forms of commonsense knowledge about the sentence. In this paper we demonstrate our progress towards addressing the WSC.","tags":null,"title":"Towards Addressing the Winograd Schema Challenge-Building and Using a Semantic Parser and a Knowledge Hunting Module.","type":"publication"},{"authors":null,"categories":null,"content":"","date":1430092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430092800,"objectID":"36377c80bec5f266388a432c2716126a","permalink":"https://adityasomak.github.io/project/kparser/","publishdate":"2015-04-27T00:00:00Z","relpermalink":"/project/kparser/","section":"project","summary":"We build a semantic parser to extract semantic knowledge from natural language sentences. `external_link`.","tags":["common-sense","NLP","Semantic Prasing","Knowledge Acqusition"],"title":"K-Parser: A Knowledge Parser","type":"project"},{"authors":["Somak Aditya","Yezhou Yang","Chitta Baral","Cornelia Fermuller","Yiannis Aloimonos"],"categories":null,"content":"In this paper we explore the use of visual commonsense knowledge and other kinds of knowledge (such as domain knowledge, background knowledge, linguistic knowledge) for scene understanding. In particular, we combine visual processing with techniques from natural language understanding (especially semantic parsing), common-sense reasoning and knowledge representation and reasoning to improve visual perception to reason about finer aspects of activities\n","date":1426118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1426118400,"objectID":"333b12bb8c3c55efbbcc1918d700c7d9","permalink":"https://adityasomak.github.io/publication/common-sense/","publishdate":"2015-03-12T00:00:00Z","relpermalink":"/publication/common-sense/","section":"publication","summary":"In this paper we explore the use of visual commonsense knowledge and other kinds of knowledge (such as domain knowledge, background knowledge, linguistic knowledge) for scene understanding. In particular, we combine visual processing with techniques from natural language understanding (especially semantic parsing), common-sense reasoning and knowledge representation and reasoning to improve visual perception to reason about finer aspects of activities","tags":null,"title":"Visual common-sense for scene understanding using perception, semantic parsing and reasoning.","type":"publication"}]